{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10182333,"sourceType":"datasetVersion","datasetId":6290008},{"sourceId":10210139,"sourceType":"datasetVersion","datasetId":6310355}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/NadaAbodeshish/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:40:05.565629Z","iopub.execute_input":"2024-12-15T23:40:05.566068Z","iopub.status.idle":"2024-12-15T23:40:12.598103Z","shell.execute_reply.started":"2024-12-15T23:40:05.566037Z","shell.execute_reply":"2024-12-15T23:40:12.597295Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion'...\nremote: Enumerating objects: 769, done.\u001b[K\nremote: Counting objects: 100% (132/132), done.\u001b[K\nremote: Compressing objects: 100% (95/95), done.\u001b[K\nremote: Total 769 (delta 67), reused 97 (delta 37), pack-reused 637 (from 1)\u001b[K\nReceiving objects: 100% (769/769), 166.71 MiB | 45.72 MiB/s, done.\nResolving deltas: 100% (419/419), done.\nUpdating files: 100% (213/213), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd /kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:40:12.599841Z","iopub.execute_input":"2024-12-15T23:40:12.600126Z","iopub.status.idle":"2024-12-15T23:40:12.606503Z","shell.execute_reply.started":"2024-12-15T23:40:12.600098Z","shell.execute_reply":"2024-12-15T23:40:12.605667Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nbase_dir = \"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512\"\nos.makedirs(base_dir, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:40:12.607470Z","iopub.execute_input":"2024-12-15T23:40:12.607704Z","iopub.status.idle":"2024-12-15T23:40:12.615787Z","shell.execute_reply.started":"2024-12-15T23:40:12.607680Z","shell.execute_reply":"2024-12-15T23:40:12.614984Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!cp -r /kaggle/input/shrec-processed/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/* /kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:40:12.616866Z","iopub.execute_input":"2024-12-15T23:40:12.617142Z","iopub.status.idle":"2024-12-15T23:40:49.819659Z","shell.execute_reply.started":"2024-12-15T23:40:12.617118Z","shell.execute_reply":"2024-12-15T23:40:49.818521Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:40:49.822839Z","iopub.execute_input":"2024-12-15T23:40:49.823265Z","iopub.status.idle":"2024-12-15T23:40:49.830131Z","shell.execute_reply.started":"2024-12-15T23:40:49.823206Z","shell.execute_reply":"2024-12-15T23:40:49.829085Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%cd /kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:40:49.831565Z","iopub.execute_input":"2024-12-15T23:40:49.831875Z","iopub.status.idle":"2024-12-15T23:40:49.842013Z","shell.execute_reply.started":"2024-12-15T23:40:49.831840Z","shell.execute_reply":"2024-12-15T23:40:49.841050Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install yacs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:40:49.843475Z","iopub.execute_input":"2024-12-15T23:40:49.843862Z","iopub.status.idle":"2024-12-15T23:41:03.518721Z","shell.execute_reply.started":"2024-12-15T23:40:49.843814Z","shell.execute_reply":"2024-12-15T23:41:03.517587Z"}},"outputs":[{"name":"stdout","text":"Collecting yacs\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs) (6.0.2)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nInstalling collected packages: yacs\nSuccessfully installed yacs-0.1.8\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install bps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:41:03.520427Z","iopub.execute_input":"2024-12-15T23:41:03.520834Z","iopub.status.idle":"2024-12-15T23:41:11.695381Z","shell.execute_reply.started":"2024-12-15T23:41:03.520792Z","shell.execute_reply":"2024-12-15T23:41:11.694302Z"}},"outputs":[{"name":"stdout","text":"Collecting bps\n  Downloading BPS-0.1.0-py3-none-any.whl.metadata (61 bytes)\nDownloading BPS-0.1.0-py3-none-any.whl (897 bytes)\nInstalling collected packages: bps\nSuccessfully installed bps-0.1.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install torch-geometric\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:41:11.696997Z","iopub.execute_input":"2024-12-15T23:41:11.697882Z","iopub.status.idle":"2024-12-15T23:41:20.960021Z","shell.execute_reply.started":"2024-12-15T23:41:11.697836Z","shell.execute_reply":"2024-12-15T23:41:20.958944Z"}},"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.6.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.6.2)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python train.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:41:20.961649Z","iopub.execute_input":"2024-12-15T23:41:20.962532Z","iopub.status.idle":"2024-12-16T02:28:50.614043Z","shell.execute_reply.started":"2024-12-15T23:41:20.962487Z","shell.execute_reply":"2024-12-16T02:28:50.613176Z"}},"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\nEpoch: 0, Batch(20/245), Loss: 2.5234\nEpoch: 0, Batch(40/245), Loss: 2.2478\nEpoch: 0, Batch(60/245), Loss: 2.1244\nEpoch: 0, Batch(80/245), Loss: 1.9025\nEpoch: 0, Batch(100/245), Loss: 1.9113\nEpoch: 0, Batch(120/245), Loss: 1.6662\nEpoch: 0, Batch(140/245), Loss: 1.5369\nEpoch: 0, Batch(160/245), Loss: 1.5851\nEpoch: 0, Batch(180/245), Loss: 1.3563\nEpoch: 0, Batch(200/245), Loss: 1.3143\nEpoch: 0, Batch(220/245), Loss: 1.3494\nEpoch: 0, Batch(240/245), Loss: 1.2614\nepoch 0 duration 1.523 train_loss 1.724 val_loss 1.023 val_acc 0.7429\nEpoch: 1, Batch(20/245), Loss: 1.1241\nEpoch: 1, Batch(40/245), Loss: 1.0179\nEpoch: 1, Batch(60/245), Loss: 1.0061\nEpoch: 1, Batch(80/245), Loss: 1.0069\nEpoch: 1, Batch(100/245), Loss: 0.9116\nEpoch: 1, Batch(120/245), Loss: 0.9975\nEpoch: 1, Batch(140/245), Loss: 0.8451\nEpoch: 1, Batch(160/245), Loss: 0.8266\nEpoch: 1, Batch(180/245), Loss: 0.8352\nEpoch: 1, Batch(200/245), Loss: 0.9098\nEpoch: 1, Batch(220/245), Loss: 0.8928\nEpoch: 1, Batch(240/245), Loss: 0.6600\nepoch 1 duration 1.674 train_loss 0.915 val_loss 0.616 val_acc 0.8667\nEpoch: 2, Batch(20/245), Loss: 0.6356\nEpoch: 2, Batch(40/245), Loss: 0.6874\nEpoch: 2, Batch(60/245), Loss: 0.6968\nEpoch: 2, Batch(80/245), Loss: 0.6876\nEpoch: 2, Batch(100/245), Loss: 0.6029\nEpoch: 2, Batch(120/245), Loss: 0.6220\nEpoch: 2, Batch(140/245), Loss: 0.5490\nEpoch: 2, Batch(160/245), Loss: 0.6930\nEpoch: 2, Batch(180/245), Loss: 0.5931\nEpoch: 2, Batch(200/245), Loss: 0.5965\nEpoch: 2, Batch(220/245), Loss: 0.5456\nEpoch: 2, Batch(240/245), Loss: 0.5538\nepoch 2 duration 1.671 train_loss 0.624 val_loss 0.489 val_acc 0.8893\nEpoch: 3, Batch(20/245), Loss: 0.4877\nEpoch: 3, Batch(40/245), Loss: 0.5145\nEpoch: 3, Batch(60/245), Loss: 0.5436\nEpoch: 3, Batch(80/245), Loss: 0.4713\nEpoch: 3, Batch(100/245), Loss: 0.4236\nEpoch: 3, Batch(120/245), Loss: 0.3958\nEpoch: 3, Batch(140/245), Loss: 0.4806\nEpoch: 3, Batch(160/245), Loss: 0.4324\nEpoch: 3, Batch(180/245), Loss: 0.4517\nEpoch: 3, Batch(200/245), Loss: 0.5158\nEpoch: 3, Batch(220/245), Loss: 0.4630\nEpoch: 3, Batch(240/245), Loss: 0.3814\nepoch 3 duration 1.673 train_loss 0.462 val_loss 0.344 val_acc 0.9131\nEpoch: 4, Batch(20/245), Loss: 0.3178\nEpoch: 4, Batch(40/245), Loss: 0.4717\nEpoch: 4, Batch(60/245), Loss: 0.3770\nEpoch: 4, Batch(80/245), Loss: 0.4415\nEpoch: 4, Batch(100/245), Loss: 0.3241\nEpoch: 4, Batch(120/245), Loss: 0.4342\nEpoch: 4, Batch(140/245), Loss: 0.3140\nEpoch: 4, Batch(160/245), Loss: 0.3942\nEpoch: 4, Batch(180/245), Loss: 0.3407\nEpoch: 4, Batch(200/245), Loss: 0.3124\nEpoch: 4, Batch(220/245), Loss: 0.4783\nEpoch: 4, Batch(240/245), Loss: 0.3151\nepoch 4 duration 1.675 train_loss 0.377 val_loss 0.320 val_acc 0.9226\nEpoch: 5, Batch(20/245), Loss: 0.2458\nEpoch: 5, Batch(40/245), Loss: 0.2977\nEpoch: 5, Batch(60/245), Loss: 0.2530\nEpoch: 5, Batch(80/245), Loss: 0.2386\nEpoch: 5, Batch(100/245), Loss: 0.3075\nEpoch: 5, Batch(120/245), Loss: 0.3299\nEpoch: 5, Batch(140/245), Loss: 0.2970\nEpoch: 5, Batch(160/245), Loss: 0.2556\nEpoch: 5, Batch(180/245), Loss: 0.2331\nEpoch: 5, Batch(200/245), Loss: 0.4340\nEpoch: 5, Batch(220/245), Loss: 0.2410\nEpoch: 5, Batch(240/245), Loss: 0.3449\nepoch 5 duration 1.672 train_loss 0.291 val_loss 0.323 val_acc 0.9190\nEpoch: 6, Batch(20/245), Loss: 0.2574\nEpoch: 6, Batch(40/245), Loss: 0.2625\nEpoch: 6, Batch(60/245), Loss: 0.2604\nEpoch: 6, Batch(80/245), Loss: 0.2584\nEpoch: 6, Batch(100/245), Loss: 0.2325\nEpoch: 6, Batch(120/245), Loss: 0.2871\nEpoch: 6, Batch(140/245), Loss: 0.2098\nEpoch: 6, Batch(160/245), Loss: 0.2569\nEpoch: 6, Batch(180/245), Loss: 0.2316\nEpoch: 6, Batch(200/245), Loss: 0.3522\nEpoch: 6, Batch(220/245), Loss: 0.2076\nEpoch: 6, Batch(240/245), Loss: 0.2783\nepoch 6 duration 1.673 train_loss 0.262 val_loss 0.305 val_acc 0.9238\nEpoch: 7, Batch(20/245), Loss: 0.2358\nEpoch: 7, Batch(40/245), Loss: 0.2762\nEpoch: 7, Batch(60/245), Loss: 0.2484\nEpoch: 7, Batch(80/245), Loss: 0.2345\nEpoch: 7, Batch(100/245), Loss: 0.1663\nEpoch: 7, Batch(120/245), Loss: 0.2463\nEpoch: 7, Batch(140/245), Loss: 0.2913\nEpoch: 7, Batch(160/245), Loss: 0.2196\nEpoch: 7, Batch(180/245), Loss: 0.2385\nEpoch: 7, Batch(200/245), Loss: 0.1837\nEpoch: 7, Batch(220/245), Loss: 0.2543\nEpoch: 7, Batch(240/245), Loss: 0.2194\nepoch 7 duration 1.672 train_loss 0.235 val_loss 0.301 val_acc 0.9190\nEpoch: 8, Batch(20/245), Loss: 0.1659\nEpoch: 8, Batch(40/245), Loss: 0.1818\nEpoch: 8, Batch(60/245), Loss: 0.1746\nEpoch: 8, Batch(80/245), Loss: 0.1985\nEpoch: 8, Batch(100/245), Loss: 0.2464\nEpoch: 8, Batch(120/245), Loss: 0.2131\nEpoch: 8, Batch(140/245), Loss: 0.2604\nEpoch: 8, Batch(160/245), Loss: 0.2458\nEpoch: 8, Batch(180/245), Loss: 0.2287\nEpoch: 8, Batch(200/245), Loss: 0.1699\nEpoch: 8, Batch(220/245), Loss: 0.2912\nEpoch: 8, Batch(240/245), Loss: 0.2330\nepoch 8 duration 1.677 train_loss 0.219 val_loss 0.304 val_acc 0.9262\nEpoch: 9, Batch(20/245), Loss: 0.2036\nEpoch: 9, Batch(40/245), Loss: 0.2430\nEpoch: 9, Batch(60/245), Loss: 0.1742\nEpoch: 9, Batch(80/245), Loss: 0.1831\nEpoch: 9, Batch(100/245), Loss: 0.2829\nEpoch: 9, Batch(120/245), Loss: 0.1935\nEpoch: 9, Batch(140/245), Loss: 0.1831\nEpoch: 9, Batch(160/245), Loss: 0.2173\nEpoch: 9, Batch(180/245), Loss: 0.2032\nEpoch: 9, Batch(200/245), Loss: 0.2149\nEpoch: 9, Batch(220/245), Loss: 0.2710\nEpoch: 9, Batch(240/245), Loss: 0.2413\nepoch 9 duration 1.672 train_loss 0.216 val_loss 0.237 val_acc 0.9440\nEpoch: 10, Batch(20/245), Loss: 0.1635\nEpoch: 10, Batch(40/245), Loss: 0.2326\nEpoch: 10, Batch(60/245), Loss: 0.1565\nEpoch: 10, Batch(80/245), Loss: 0.1709\nEpoch: 10, Batch(100/245), Loss: 0.2058\nEpoch: 10, Batch(120/245), Loss: 0.1574\nEpoch: 10, Batch(140/245), Loss: 0.1287\nEpoch: 10, Batch(160/245), Loss: 0.1979\nEpoch: 10, Batch(180/245), Loss: 0.2595\nEpoch: 10, Batch(200/245), Loss: 0.2300\nEpoch: 10, Batch(220/245), Loss: 0.2252\nEpoch: 10, Batch(240/245), Loss: 0.2161\nepoch 10 duration 1.673 train_loss 0.193 val_loss 0.266 val_acc 0.9357\nEpoch: 11, Batch(20/245), Loss: 0.1883\nEpoch: 11, Batch(40/245), Loss: 0.1977\nEpoch: 11, Batch(60/245), Loss: 0.1654\nEpoch: 11, Batch(80/245), Loss: 0.1569\nEpoch: 11, Batch(100/245), Loss: 0.1448\nEpoch: 11, Batch(120/245), Loss: 0.1616\nEpoch: 11, Batch(140/245), Loss: 0.1092\nEpoch: 11, Batch(160/245), Loss: 0.1694\nEpoch: 11, Batch(180/245), Loss: 0.1327\nEpoch: 11, Batch(200/245), Loss: 0.2259\nEpoch: 11, Batch(220/245), Loss: 0.1474\nEpoch: 11, Batch(240/245), Loss: 0.1435\nepoch 11 duration 1.675 train_loss 0.163 val_loss 0.319 val_acc 0.9095\nEpoch: 12, Batch(20/245), Loss: 0.1751\nEpoch: 12, Batch(40/245), Loss: 0.0982\nEpoch: 12, Batch(60/245), Loss: 0.1471\nEpoch: 12, Batch(80/245), Loss: 0.1139\nEpoch: 12, Batch(100/245), Loss: 0.1173\nEpoch: 12, Batch(120/245), Loss: 0.1730\nEpoch: 12, Batch(140/245), Loss: 0.1566\nEpoch: 12, Batch(160/245), Loss: 0.1497\nEpoch: 12, Batch(180/245), Loss: 0.1841\nEpoch: 12, Batch(200/245), Loss: 0.1648\nEpoch: 12, Batch(220/245), Loss: 0.1445\nEpoch: 12, Batch(240/245), Loss: 0.2116\nepoch 12 duration 1.674 train_loss 0.153 val_loss 0.287 val_acc 0.9179\nEpoch: 13, Batch(20/245), Loss: 0.1596\nEpoch: 13, Batch(40/245), Loss: 0.1749\nEpoch: 13, Batch(60/245), Loss: 0.0960\nEpoch: 13, Batch(80/245), Loss: 0.1760\nEpoch: 13, Batch(100/245), Loss: 0.1198\nEpoch: 13, Batch(120/245), Loss: 0.1880\nEpoch: 13, Batch(140/245), Loss: 0.1396\nEpoch: 13, Batch(160/245), Loss: 0.1477\nEpoch: 13, Batch(180/245), Loss: 0.1753\nEpoch: 13, Batch(200/245), Loss: 0.2309\nEpoch: 13, Batch(220/245), Loss: 0.1661\nEpoch: 13, Batch(240/245), Loss: 0.2092\nepoch 13 duration 1.672 train_loss 0.169 val_loss 0.597 val_acc 0.8369\nEpoch: 14, Batch(20/245), Loss: 0.1602\nEpoch: 14, Batch(40/245), Loss: 0.1587\nEpoch: 14, Batch(60/245), Loss: 0.1261\nEpoch: 14, Batch(80/245), Loss: 0.1477\nEpoch: 14, Batch(100/245), Loss: 0.1580\nEpoch: 14, Batch(120/245), Loss: 0.1497\nEpoch: 14, Batch(140/245), Loss: 0.1203\nEpoch: 14, Batch(160/245), Loss: 0.1521\nEpoch: 14, Batch(180/245), Loss: 0.1697\nEpoch: 14, Batch(200/245), Loss: 0.1770\nEpoch: 14, Batch(220/245), Loss: 0.1410\nEpoch: 14, Batch(240/245), Loss: 0.1761\nepoch 14 duration 1.675 train_loss 0.157 val_loss 0.332 val_acc 0.9048\nEpoch: 15, Batch(20/245), Loss: 0.1350\nEpoch: 15, Batch(40/245), Loss: 0.1199\nEpoch: 15, Batch(60/245), Loss: 0.1362\nEpoch: 15, Batch(80/245), Loss: 0.1652\nEpoch: 15, Batch(100/245), Loss: 0.1477\nEpoch: 15, Batch(120/245), Loss: 0.1751\nEpoch: 15, Batch(140/245), Loss: 0.1935\nEpoch: 15, Batch(160/245), Loss: 0.1744\nEpoch: 15, Batch(180/245), Loss: 0.1368\nEpoch: 15, Batch(200/245), Loss: 0.1132\nEpoch: 15, Batch(220/245), Loss: 0.1559\nEpoch: 15, Batch(240/245), Loss: 0.1358\nepoch 15 duration 1.674 train_loss 0.157 val_loss 0.306 val_acc 0.9060\nEpoch: 16, Batch(20/245), Loss: 0.1006\nEpoch: 16, Batch(40/245), Loss: 0.1146\nEpoch: 16, Batch(60/245), Loss: 0.1227\nEpoch: 16, Batch(80/245), Loss: 0.1244\nEpoch: 16, Batch(100/245), Loss: 0.0915\nEpoch: 16, Batch(120/245), Loss: 0.0733\nEpoch: 16, Batch(140/245), Loss: 0.1561\nEpoch: 16, Batch(160/245), Loss: 0.1272\nEpoch: 16, Batch(180/245), Loss: 0.1950\nEpoch: 16, Batch(200/245), Loss: 0.1524\nEpoch: 16, Batch(220/245), Loss: 0.1876\nEpoch: 16, Batch(240/245), Loss: 0.1830\nepoch 16 duration 1.674 train_loss 0.136 val_loss 0.267 val_acc 0.9238\nEpoch: 17, Batch(20/245), Loss: 0.1413\nEpoch: 17, Batch(40/245), Loss: 0.1036\nEpoch: 17, Batch(60/245), Loss: 0.1615\nEpoch: 17, Batch(80/245), Loss: 0.1473\nEpoch: 17, Batch(100/245), Loss: 0.1398\nEpoch: 17, Batch(120/245), Loss: 0.1303\nEpoch: 17, Batch(140/245), Loss: 0.1481\nEpoch: 17, Batch(160/245), Loss: 0.1316\nEpoch: 17, Batch(180/245), Loss: 0.1617\nEpoch: 17, Batch(200/245), Loss: 0.1772\nEpoch: 17, Batch(220/245), Loss: 0.1412\nEpoch: 17, Batch(240/245), Loss: 0.1363\nepoch 17 duration 1.674 train_loss 0.144 val_loss 0.284 val_acc 0.9262\nEpoch: 18, Batch(20/245), Loss: 0.1072\nEpoch: 18, Batch(40/245), Loss: 0.1248\nEpoch: 18, Batch(60/245), Loss: 0.1877\nEpoch: 18, Batch(80/245), Loss: 0.1168\nEpoch: 18, Batch(100/245), Loss: 0.1560\nEpoch: 18, Batch(120/245), Loss: 0.1623\nEpoch: 18, Batch(140/245), Loss: 0.1388\nEpoch: 18, Batch(160/245), Loss: 0.1173\nEpoch: 18, Batch(180/245), Loss: 0.1372\nEpoch: 18, Batch(200/245), Loss: 0.1104\nEpoch: 18, Batch(220/245), Loss: 0.1415\nEpoch: 18, Batch(240/245), Loss: 0.1115\nepoch 18 duration 1.676 train_loss 0.135 val_loss 0.236 val_acc 0.9405\nEpoch: 19, Batch(20/245), Loss: 0.1254\nEpoch: 19, Batch(40/245), Loss: 0.1453\nEpoch: 19, Batch(60/245), Loss: 0.1344\nEpoch: 19, Batch(80/245), Loss: 0.1184\nEpoch: 19, Batch(100/245), Loss: 0.1319\nEpoch: 19, Batch(120/245), Loss: 0.1206\nEpoch: 19, Batch(140/245), Loss: 0.1289\nEpoch: 19, Batch(160/245), Loss: 0.0914\nEpoch: 19, Batch(180/245), Loss: 0.1218\nEpoch: 19, Batch(200/245), Loss: 0.1157\nEpoch: 19, Batch(220/245), Loss: 0.1006\nEpoch: 19, Batch(240/245), Loss: 0.1179\nepoch 19 duration 1.677 train_loss 0.121 val_loss 0.263 val_acc 0.9298\nEpoch: 20, Batch(20/245), Loss: 0.1295\nEpoch: 20, Batch(40/245), Loss: 0.1052\nEpoch: 20, Batch(60/245), Loss: 0.1429\nEpoch: 20, Batch(80/245), Loss: 0.1559\nEpoch: 20, Batch(100/245), Loss: 0.1083\nEpoch: 20, Batch(120/245), Loss: 0.1146\nEpoch: 20, Batch(140/245), Loss: 0.1063\nEpoch: 20, Batch(160/245), Loss: 0.1009\nEpoch: 20, Batch(180/245), Loss: 0.1206\nEpoch: 20, Batch(200/245), Loss: 0.1318\nEpoch: 20, Batch(220/245), Loss: 0.0954\nEpoch: 20, Batch(240/245), Loss: 0.1817\nepoch 20 duration 1.675 train_loss 0.126 val_loss 0.229 val_acc 0.9393\nEpoch: 21, Batch(20/245), Loss: 0.1439\nEpoch: 21, Batch(40/245), Loss: 0.1492\nEpoch: 21, Batch(60/245), Loss: 0.1272\nEpoch: 21, Batch(80/245), Loss: 0.1420\nEpoch: 21, Batch(100/245), Loss: 0.1623\nEpoch: 21, Batch(120/245), Loss: 0.0996\nEpoch: 21, Batch(140/245), Loss: 0.1254\nEpoch: 21, Batch(160/245), Loss: 0.1045\nEpoch: 21, Batch(180/245), Loss: 0.1365\nEpoch: 21, Batch(200/245), Loss: 0.1061\nEpoch: 21, Batch(220/245), Loss: 0.1468\nEpoch: 21, Batch(240/245), Loss: 0.1620\nepoch 21 duration 1.673 train_loss 0.134 val_loss 0.216 val_acc 0.9476\nEpoch: 22, Batch(20/245), Loss: 0.2006\nEpoch: 22, Batch(40/245), Loss: 0.1606\nEpoch: 22, Batch(60/245), Loss: 0.1114\nEpoch: 22, Batch(80/245), Loss: 0.1388\nEpoch: 22, Batch(100/245), Loss: 0.1913\nEpoch: 22, Batch(120/245), Loss: 0.1260\nEpoch: 22, Batch(140/245), Loss: 0.1421\nEpoch: 22, Batch(160/245), Loss: 0.1125\nEpoch: 22, Batch(180/245), Loss: 0.1076\nEpoch: 22, Batch(200/245), Loss: 0.1177\nEpoch: 22, Batch(220/245), Loss: 0.1419\nEpoch: 22, Batch(240/245), Loss: 0.1030\nepoch 22 duration 1.670 train_loss 0.138 val_loss 0.245 val_acc 0.9429\nEpoch: 23, Batch(20/245), Loss: 0.1558\nEpoch: 23, Batch(40/245), Loss: 0.0758\nEpoch: 23, Batch(60/245), Loss: 0.0908\nEpoch: 23, Batch(80/245), Loss: 0.1035\nEpoch: 23, Batch(100/245), Loss: 0.1122\nEpoch: 23, Batch(120/245), Loss: 0.1060\nEpoch: 23, Batch(140/245), Loss: 0.1560\nEpoch: 23, Batch(160/245), Loss: 0.1444\nEpoch: 23, Batch(180/245), Loss: 0.0814\nEpoch: 23, Batch(200/245), Loss: 0.1802\nEpoch: 23, Batch(220/245), Loss: 0.1559\nEpoch: 23, Batch(240/245), Loss: 0.1244\nepoch 23 duration 1.676 train_loss 0.123 val_loss 0.230 val_acc 0.9369\nEpoch: 24, Batch(20/245), Loss: 0.1403\nEpoch: 24, Batch(40/245), Loss: 0.1091\nEpoch: 24, Batch(60/245), Loss: 0.1291\nEpoch: 24, Batch(80/245), Loss: 0.1353\nEpoch: 24, Batch(100/245), Loss: 0.1282\nEpoch: 24, Batch(120/245), Loss: 0.0921\nEpoch: 24, Batch(140/245), Loss: 0.0770\nEpoch: 24, Batch(160/245), Loss: 0.1240\nEpoch: 24, Batch(180/245), Loss: 0.1547\nEpoch: 24, Batch(200/245), Loss: 0.2092\nEpoch: 24, Batch(220/245), Loss: 0.1654\nEpoch: 24, Batch(240/245), Loss: 0.1353\nepoch 24 duration 1.674 train_loss 0.134 val_loss 0.509 val_acc 0.8667\nEpoch: 25, Batch(20/245), Loss: 0.1323\nEpoch: 25, Batch(40/245), Loss: 0.1624\nEpoch: 25, Batch(60/245), Loss: 0.1609\nEpoch: 25, Batch(80/245), Loss: 0.1148\nEpoch: 25, Batch(100/245), Loss: 0.1360\nEpoch: 25, Batch(120/245), Loss: 0.1451\nEpoch: 25, Batch(140/245), Loss: 0.1348\nEpoch: 25, Batch(160/245), Loss: 0.1337\nEpoch: 25, Batch(180/245), Loss: 0.0841\nEpoch: 25, Batch(200/245), Loss: 0.1601\nEpoch: 25, Batch(220/245), Loss: 0.1252\nEpoch: 25, Batch(240/245), Loss: 0.1702\nepoch 25 duration 1.674 train_loss 0.138 val_loss 0.261 val_acc 0.9333\nEpoch: 26, Batch(20/245), Loss: 0.0973\nEpoch: 26, Batch(40/245), Loss: 0.1151\nEpoch: 26, Batch(60/245), Loss: 0.1536\nEpoch: 26, Batch(80/245), Loss: 0.1157\nEpoch: 26, Batch(100/245), Loss: 0.1054\nEpoch: 26, Batch(120/245), Loss: 0.0771\nEpoch: 26, Batch(140/245), Loss: 0.1283\nEpoch: 26, Batch(160/245), Loss: 0.1172\nEpoch: 26, Batch(180/245), Loss: 0.1470\nEpoch: 26, Batch(200/245), Loss: 0.1427\nEpoch: 26, Batch(220/245), Loss: 0.0981\nEpoch: 26, Batch(240/245), Loss: 0.1144\nepoch 26 duration 1.672 train_loss 0.117 val_loss 0.228 val_acc 0.9369\nEpoch: 27, Batch(20/245), Loss: 0.1021\nEpoch: 27, Batch(40/245), Loss: 0.0885\nEpoch: 27, Batch(60/245), Loss: 0.1220\nEpoch: 27, Batch(80/245), Loss: 0.0651\nEpoch: 27, Batch(100/245), Loss: 0.0635\nEpoch: 27, Batch(120/245), Loss: 0.0651\nEpoch: 27, Batch(140/245), Loss: 0.0985\nEpoch: 27, Batch(160/245), Loss: 0.1004\nEpoch: 27, Batch(180/245), Loss: 0.1132\nEpoch: 27, Batch(200/245), Loss: 0.1258\nEpoch: 27, Batch(220/245), Loss: 0.0998\nEpoch: 27, Batch(240/245), Loss: 0.0774\nepoch 27 duration 1.672 train_loss 0.093 val_loss 0.239 val_acc 0.9345\nEpoch: 28, Batch(20/245), Loss: 0.0695\nEpoch: 28, Batch(40/245), Loss: 0.0914\nEpoch: 28, Batch(60/245), Loss: 0.1466\nEpoch: 28, Batch(80/245), Loss: 0.1534\nEpoch: 28, Batch(100/245), Loss: 0.1030\nEpoch: 28, Batch(120/245), Loss: 0.1114\nEpoch: 28, Batch(140/245), Loss: 0.0971\nEpoch: 28, Batch(160/245), Loss: 0.1893\nEpoch: 28, Batch(180/245), Loss: 0.1508\nEpoch: 28, Batch(200/245), Loss: 0.1086\nEpoch: 28, Batch(220/245), Loss: 0.1232\nEpoch: 28, Batch(240/245), Loss: 0.1890\nepoch 28 duration 1.673 train_loss 0.130 val_loss 0.267 val_acc 0.9286\nEpoch: 29, Batch(20/245), Loss: 0.1498\nEpoch: 29, Batch(40/245), Loss: 0.2030\nEpoch: 29, Batch(60/245), Loss: 0.1488\nEpoch: 29, Batch(80/245), Loss: 0.0979\nEpoch: 29, Batch(100/245), Loss: 0.1114\nEpoch: 29, Batch(120/245), Loss: 0.1842\nEpoch: 29, Batch(140/245), Loss: 0.1417\nEpoch: 29, Batch(160/245), Loss: 0.1456\nEpoch: 29, Batch(180/245), Loss: 0.1232\nEpoch: 29, Batch(200/245), Loss: 0.2051\nEpoch: 29, Batch(220/245), Loss: 0.1389\nEpoch: 29, Batch(240/245), Loss: 0.1197\nepoch 29 duration 1.672 train_loss 0.145 val_loss 0.232 val_acc 0.9488\nEpoch: 30, Batch(20/245), Loss: 0.0923\nEpoch: 30, Batch(40/245), Loss: 0.1101\nEpoch: 30, Batch(60/245), Loss: 0.0885\nEpoch: 30, Batch(80/245), Loss: 0.1195\nEpoch: 30, Batch(100/245), Loss: 0.0959\nEpoch: 30, Batch(120/245), Loss: 0.0802\nEpoch: 30, Batch(140/245), Loss: 0.0861\nEpoch: 30, Batch(160/245), Loss: 0.0767\nEpoch: 30, Batch(180/245), Loss: 0.1393\nEpoch: 30, Batch(200/245), Loss: 0.1149\nEpoch: 30, Batch(220/245), Loss: 0.0982\nEpoch: 30, Batch(240/245), Loss: 0.1419\nepoch 30 duration 1.675 train_loss 0.106 val_loss 0.268 val_acc 0.9405\nEpoch: 31, Batch(20/245), Loss: 0.1507\nEpoch: 31, Batch(40/245), Loss: 0.1068\nEpoch: 31, Batch(60/245), Loss: 0.1337\nEpoch: 31, Batch(80/245), Loss: 0.1139\nEpoch: 31, Batch(100/245), Loss: 0.1002\nEpoch: 31, Batch(120/245), Loss: 0.0981\nEpoch: 31, Batch(140/245), Loss: 0.1709\nEpoch: 31, Batch(160/245), Loss: 0.1190\nEpoch: 31, Batch(180/245), Loss: 0.1750\nEpoch: 31, Batch(200/245), Loss: 0.1826\nEpoch: 31, Batch(220/245), Loss: 0.1580\nEpoch: 31, Batch(240/245), Loss: 0.1739\nepoch 31 duration 1.675 train_loss 0.142 val_loss 0.375 val_acc 0.9036\nEpoch: 32, Batch(20/245), Loss: 0.1754\nEpoch: 32, Batch(40/245), Loss: 0.1585\nEpoch: 32, Batch(60/245), Loss: 0.1403\nEpoch: 32, Batch(80/245), Loss: 0.1066\nEpoch: 32, Batch(100/245), Loss: 0.1156\nEpoch: 32, Batch(120/245), Loss: 0.1835\nEpoch: 32, Batch(140/245), Loss: 0.0929\nEpoch: 32, Batch(160/245), Loss: 0.1339\nEpoch: 32, Batch(180/245), Loss: 0.1035\nEpoch: 32, Batch(200/245), Loss: 0.0800\nEpoch: 32, Batch(220/245), Loss: 0.0941\nEpoch: 32, Batch(240/245), Loss: 0.1092\nepoch 32 duration 1.673 train_loss 0.125 val_loss 0.180 val_acc 0.9560\nEpoch: 33, Batch(20/245), Loss: 0.0675\nEpoch: 33, Batch(40/245), Loss: 0.1007\nEpoch: 33, Batch(60/245), Loss: 0.0684\nEpoch: 33, Batch(80/245), Loss: 0.0696\nEpoch: 33, Batch(100/245), Loss: 0.0943\nEpoch: 33, Batch(120/245), Loss: 0.0589\nEpoch: 33, Batch(140/245), Loss: 0.0458\nEpoch: 33, Batch(160/245), Loss: 0.0954\nEpoch: 33, Batch(180/245), Loss: 0.1253\nEpoch: 33, Batch(200/245), Loss: 0.1491\nEpoch: 33, Batch(220/245), Loss: 0.1159\nEpoch: 33, Batch(240/245), Loss: 0.1368\nepoch 33 duration 1.673 train_loss 0.094 val_loss 0.228 val_acc 0.9417\nEpoch: 34, Batch(20/245), Loss: 0.0857\nEpoch: 34, Batch(40/245), Loss: 0.1613\nEpoch: 34, Batch(60/245), Loss: 0.0895\nEpoch: 34, Batch(80/245), Loss: 0.1342\nEpoch: 34, Batch(100/245), Loss: 0.1452\nEpoch: 34, Batch(120/245), Loss: 0.1296\nEpoch: 34, Batch(140/245), Loss: 0.0761\nEpoch: 34, Batch(160/245), Loss: 0.1348\nEpoch: 34, Batch(180/245), Loss: 0.1331\nEpoch: 34, Batch(200/245), Loss: 0.1865\nEpoch: 34, Batch(220/245), Loss: 0.1396\nEpoch: 34, Batch(240/245), Loss: 0.1109\nepoch 34 duration 1.673 train_loss 0.126 val_loss 0.218 val_acc 0.9393\nEpoch: 35, Batch(20/245), Loss: 0.0924\nEpoch: 35, Batch(40/245), Loss: 0.0963\nEpoch: 35, Batch(60/245), Loss: 0.0626\nEpoch: 35, Batch(80/245), Loss: 0.0737\nEpoch: 35, Batch(100/245), Loss: 0.0801\nEpoch: 35, Batch(120/245), Loss: 0.0856\nEpoch: 35, Batch(140/245), Loss: 0.0889\nEpoch: 35, Batch(160/245), Loss: 0.0489\nEpoch: 35, Batch(180/245), Loss: 0.0811\nEpoch: 35, Batch(200/245), Loss: 0.1134\nEpoch: 35, Batch(220/245), Loss: 0.0902\nEpoch: 35, Batch(240/245), Loss: 0.0902\nepoch 35 duration 1.674 train_loss 0.084 val_loss 0.320 val_acc 0.9214\nEpoch: 36, Batch(20/245), Loss: 0.1111\nEpoch: 36, Batch(40/245), Loss: 0.0934\nEpoch: 36, Batch(60/245), Loss: 0.1438\nEpoch: 36, Batch(80/245), Loss: 0.1671\nEpoch: 36, Batch(100/245), Loss: 0.1208\nEpoch: 36, Batch(120/245), Loss: 0.1270\nEpoch: 36, Batch(140/245), Loss: 0.1551\nEpoch: 36, Batch(160/245), Loss: 0.0864\nEpoch: 36, Batch(180/245), Loss: 0.1508\nEpoch: 36, Batch(200/245), Loss: 0.1157\nEpoch: 36, Batch(220/245), Loss: 0.1416\nEpoch: 36, Batch(240/245), Loss: 0.1531\nepoch 36 duration 1.674 train_loss 0.130 val_loss 0.238 val_acc 0.9381\nEpoch: 37, Batch(20/245), Loss: 0.1320\nEpoch: 37, Batch(40/245), Loss: 0.0941\nEpoch: 37, Batch(60/245), Loss: 0.1038\nEpoch: 37, Batch(80/245), Loss: 0.0956\nEpoch: 37, Batch(100/245), Loss: 0.1002\nEpoch: 37, Batch(120/245), Loss: 0.1337\nEpoch: 37, Batch(140/245), Loss: 0.0830\nEpoch: 37, Batch(160/245), Loss: 0.0969\nEpoch: 37, Batch(180/245), Loss: 0.1842\nEpoch: 37, Batch(200/245), Loss: 0.1398\nEpoch: 37, Batch(220/245), Loss: 0.1740\nEpoch: 37, Batch(240/245), Loss: 0.1515\nepoch 37 duration 1.673 train_loss 0.123 val_loss 0.258 val_acc 0.9345\nEpoch: 38, Batch(20/245), Loss: 0.0890\nEpoch: 38, Batch(40/245), Loss: 0.0777\nEpoch: 38, Batch(60/245), Loss: 0.0793\nEpoch: 38, Batch(80/245), Loss: 0.1091\nEpoch: 38, Batch(100/245), Loss: 0.0827\nEpoch: 38, Batch(120/245), Loss: 0.0996\nEpoch: 38, Batch(140/245), Loss: 0.1322\nEpoch: 38, Batch(160/245), Loss: 0.1554\nEpoch: 38, Batch(180/245), Loss: 0.1352\nEpoch: 38, Batch(200/245), Loss: 0.1553\nEpoch: 38, Batch(220/245), Loss: 0.1887\nEpoch: 38, Batch(240/245), Loss: 0.1324\nepoch 38 duration 1.674 train_loss 0.124 val_loss 0.186 val_acc 0.9476\nEpoch: 39, Batch(20/245), Loss: 0.0958\nEpoch: 39, Batch(40/245), Loss: 0.1111\nEpoch: 39, Batch(60/245), Loss: 0.1332\nEpoch: 39, Batch(80/245), Loss: 0.1794\nEpoch: 39, Batch(100/245), Loss: 0.1384\nEpoch: 39, Batch(120/245), Loss: 0.1155\nEpoch: 39, Batch(140/245), Loss: 0.0838\nEpoch: 39, Batch(160/245), Loss: 0.2214\nEpoch: 39, Batch(180/245), Loss: 0.0726\nEpoch: 39, Batch(200/245), Loss: 0.1536\nEpoch: 39, Batch(220/245), Loss: 0.1219\nEpoch: 39, Batch(240/245), Loss: 0.1154\nepoch 39 duration 1.671 train_loss 0.127 val_loss 0.251 val_acc 0.9381\nEpoch: 40, Batch(20/245), Loss: 0.1187\nEpoch: 40, Batch(40/245), Loss: 0.0609\nEpoch: 40, Batch(60/245), Loss: 0.1212\nEpoch: 40, Batch(80/245), Loss: 0.0908\nEpoch: 40, Batch(100/245), Loss: 0.0928\nEpoch: 40, Batch(120/245), Loss: 0.0884\nEpoch: 40, Batch(140/245), Loss: 0.0983\nEpoch: 40, Batch(160/245), Loss: 0.1171\nEpoch: 40, Batch(180/245), Loss: 0.0772\nEpoch: 40, Batch(200/245), Loss: 0.1241\nEpoch: 40, Batch(220/245), Loss: 0.1671\nEpoch: 40, Batch(240/245), Loss: 0.0842\nepoch 40 duration 1.672 train_loss 0.102 val_loss 0.234 val_acc 0.9345\nEpoch: 41, Batch(20/245), Loss: 0.1004\nEpoch: 41, Batch(40/245), Loss: 0.1043\nEpoch: 41, Batch(60/245), Loss: 0.1245\nEpoch: 41, Batch(80/245), Loss: 0.0740\nEpoch: 41, Batch(100/245), Loss: 0.0650\nEpoch: 41, Batch(120/245), Loss: 0.0687\nEpoch: 41, Batch(140/245), Loss: 0.0780\nEpoch: 41, Batch(160/245), Loss: 0.1419\nEpoch: 41, Batch(180/245), Loss: 0.1245\nEpoch: 41, Batch(200/245), Loss: 0.1425\nEpoch: 41, Batch(220/245), Loss: 0.1019\nEpoch: 41, Batch(240/245), Loss: 0.1452\nepoch 41 duration 1.679 train_loss 0.104 val_loss 0.283 val_acc 0.9214\nEpoch: 42, Batch(20/245), Loss: 0.0789\nEpoch: 42, Batch(40/245), Loss: 0.0666\nEpoch: 42, Batch(60/245), Loss: 0.0953\nEpoch: 42, Batch(80/245), Loss: 0.1011\nEpoch: 42, Batch(100/245), Loss: 0.0999\nEpoch: 42, Batch(120/245), Loss: 0.1086\nEpoch: 42, Batch(140/245), Loss: 0.1305\nEpoch: 42, Batch(160/245), Loss: 0.1190\nEpoch: 42, Batch(180/245), Loss: 0.1017\nEpoch: 42, Batch(200/245), Loss: 0.1704\nEpoch: 42, Batch(220/245), Loss: 0.1103\nEpoch: 42, Batch(240/245), Loss: 0.0660\nepoch 42 duration 1.672 train_loss 0.104 val_loss 0.233 val_acc 0.9310\nEpoch: 43, Batch(20/245), Loss: 0.0846\nEpoch: 43, Batch(40/245), Loss: 0.1594\nEpoch: 43, Batch(60/245), Loss: 0.1709\nEpoch: 43, Batch(80/245), Loss: 0.1301\nEpoch: 43, Batch(100/245), Loss: 0.1216\nEpoch: 43, Batch(120/245), Loss: 0.0910\nEpoch: 43, Batch(140/245), Loss: 0.0799\nEpoch: 43, Batch(160/245), Loss: 0.1410\nEpoch: 43, Batch(180/245), Loss: 0.1281\nEpoch: 43, Batch(200/245), Loss: 0.0483\nEpoch: 43, Batch(220/245), Loss: 0.1415\nEpoch: 43, Batch(240/245), Loss: 0.1251\nepoch 43 duration 1.674 train_loss 0.118 val_loss 0.176 val_acc 0.9536\nEpoch: 44, Batch(20/245), Loss: 0.0831\nEpoch: 44, Batch(40/245), Loss: 0.1376\nEpoch: 44, Batch(60/245), Loss: 0.1169\nEpoch: 44, Batch(80/245), Loss: 0.0839\nEpoch: 44, Batch(100/245), Loss: 0.0809\nEpoch: 44, Batch(120/245), Loss: 0.0669\nEpoch: 44, Batch(140/245), Loss: 0.0775\nEpoch: 44, Batch(160/245), Loss: 0.1222\nEpoch: 44, Batch(180/245), Loss: 0.1789\nEpoch: 44, Batch(200/245), Loss: 0.1975\nEpoch: 44, Batch(220/245), Loss: 0.1197\nEpoch: 44, Batch(240/245), Loss: 0.1204\nepoch 44 duration 1.674 train_loss 0.114 val_loss 0.280 val_acc 0.9238\nEpoch: 45, Batch(20/245), Loss: 0.0921\nEpoch: 45, Batch(40/245), Loss: 0.0949\nEpoch: 45, Batch(60/245), Loss: 0.2233\nEpoch: 45, Batch(80/245), Loss: 0.1483\nEpoch: 45, Batch(100/245), Loss: 0.1055\nEpoch: 45, Batch(120/245), Loss: 0.1052\nEpoch: 45, Batch(140/245), Loss: 0.0795\nEpoch: 45, Batch(160/245), Loss: 0.1142\nEpoch: 45, Batch(180/245), Loss: 0.0745\nEpoch: 45, Batch(200/245), Loss: 0.0809\nEpoch: 45, Batch(220/245), Loss: 0.0857\nEpoch: 45, Batch(240/245), Loss: 0.1537\nepoch 45 duration 1.674 train_loss 0.120 val_loss 0.400 val_acc 0.9012\nEpoch: 46, Batch(20/245), Loss: 0.0926\nEpoch: 46, Batch(40/245), Loss: 0.1184\nEpoch: 46, Batch(60/245), Loss: 0.1203\nEpoch: 46, Batch(80/245), Loss: 0.0715\nEpoch: 46, Batch(100/245), Loss: 0.0936\nEpoch: 46, Batch(120/245), Loss: 0.0982\nEpoch: 46, Batch(140/245), Loss: 0.0773\nEpoch: 46, Batch(160/245), Loss: 0.1055\nEpoch: 46, Batch(180/245), Loss: 0.1061\nEpoch: 46, Batch(200/245), Loss: 0.0872\nEpoch: 46, Batch(220/245), Loss: 0.0975\nEpoch: 46, Batch(240/245), Loss: 0.1128\nepoch 46 duration 1.674 train_loss 0.099 val_loss 0.291 val_acc 0.9250\nEpoch: 47, Batch(20/245), Loss: 0.0576\nEpoch: 47, Batch(40/245), Loss: 0.0691\nEpoch: 47, Batch(60/245), Loss: 0.0921\nEpoch: 47, Batch(80/245), Loss: 0.0967\nEpoch: 47, Batch(100/245), Loss: 0.0969\nEpoch: 47, Batch(120/245), Loss: 0.0809\nEpoch: 47, Batch(140/245), Loss: 0.1670\nEpoch: 47, Batch(160/245), Loss: 0.1099\nEpoch: 47, Batch(180/245), Loss: 0.1419\nEpoch: 47, Batch(200/245), Loss: 0.0832\nEpoch: 47, Batch(220/245), Loss: 0.1262\nEpoch: 47, Batch(240/245), Loss: 0.0936\nepoch 47 duration 1.674 train_loss 0.101 val_loss 0.198 val_acc 0.9405\nEpoch: 48, Batch(20/245), Loss: 0.0884\nEpoch: 48, Batch(40/245), Loss: 0.1301\nEpoch: 48, Batch(60/245), Loss: 0.1660\nEpoch: 48, Batch(80/245), Loss: 0.1644\nEpoch: 48, Batch(100/245), Loss: 0.0984\nEpoch: 48, Batch(120/245), Loss: 0.0908\nEpoch: 48, Batch(140/245), Loss: 0.0436\nEpoch: 48, Batch(160/245), Loss: 0.1013\nEpoch: 48, Batch(180/245), Loss: 0.0693\nEpoch: 48, Batch(200/245), Loss: 0.0690\nEpoch: 48, Batch(220/245), Loss: 0.1223\nEpoch: 48, Batch(240/245), Loss: 0.0716\nepoch 48 duration 1.673 train_loss 0.101 val_loss 0.170 val_acc 0.9488\nEpoch: 49, Batch(20/245), Loss: 0.0864\nEpoch: 49, Batch(40/245), Loss: 0.0677\nEpoch: 49, Batch(60/245), Loss: 0.0743\nEpoch: 49, Batch(80/245), Loss: 0.0814\nEpoch: 49, Batch(100/245), Loss: 0.0494\nEpoch: 49, Batch(120/245), Loss: 0.0661\nEpoch: 49, Batch(140/245), Loss: 0.0870\nEpoch: 49, Batch(160/245), Loss: 0.1312\nEpoch: 49, Batch(180/245), Loss: 0.0976\nEpoch: 49, Batch(200/245), Loss: 0.0812\nEpoch: 49, Batch(220/245), Loss: 0.1116\nEpoch: 49, Batch(240/245), Loss: 0.1740\nepoch 49 duration 1.675 train_loss 0.094 val_loss 0.195 val_acc 0.9393\nEpoch: 50, Batch(20/245), Loss: 0.1033\nEpoch: 50, Batch(40/245), Loss: 0.1195\nEpoch: 50, Batch(60/245), Loss: 0.0834\nEpoch: 50, Batch(80/245), Loss: 0.0738\nEpoch: 50, Batch(100/245), Loss: 0.1022\nEpoch: 50, Batch(120/245), Loss: 0.0835\nEpoch: 50, Batch(140/245), Loss: 0.0562\nEpoch: 50, Batch(160/245), Loss: 0.0741\nEpoch: 50, Batch(180/245), Loss: 0.0520\nEpoch: 50, Batch(200/245), Loss: 0.0554\nEpoch: 50, Batch(220/245), Loss: 0.1109\nEpoch: 50, Batch(240/245), Loss: 0.0476\nepoch 50 duration 1.673 train_loss 0.080 val_loss 0.176 val_acc 0.9536\nEpoch: 51, Batch(20/245), Loss: 0.0608\nEpoch: 51, Batch(40/245), Loss: 0.0720\nEpoch: 51, Batch(60/245), Loss: 0.0601\nEpoch: 51, Batch(80/245), Loss: 0.0801\nEpoch: 51, Batch(100/245), Loss: 0.0926\nEpoch: 51, Batch(120/245), Loss: 0.0274\nEpoch: 51, Batch(140/245), Loss: 0.0377\nEpoch: 51, Batch(160/245), Loss: 0.0290\nEpoch: 51, Batch(180/245), Loss: 0.0780\nEpoch: 51, Batch(200/245), Loss: 0.0545\nEpoch: 51, Batch(220/245), Loss: 0.0709\nEpoch: 51, Batch(240/245), Loss: 0.0475\nepoch 51 duration 1.675 train_loss 0.060 val_loss 0.177 val_acc 0.9536\nEpoch: 52, Batch(20/245), Loss: 0.0399\nEpoch: 52, Batch(40/245), Loss: 0.0517\nEpoch: 52, Batch(60/245), Loss: 0.0478\nEpoch: 52, Batch(80/245), Loss: 0.0591\nEpoch: 52, Batch(100/245), Loss: 0.0802\nEpoch: 52, Batch(120/245), Loss: 0.0315\nEpoch: 52, Batch(140/245), Loss: 0.0488\nEpoch: 52, Batch(160/245), Loss: 0.0478\nEpoch: 52, Batch(180/245), Loss: 0.0321\nEpoch: 52, Batch(200/245), Loss: 0.0701\nEpoch: 52, Batch(220/245), Loss: 0.0352\nEpoch: 52, Batch(240/245), Loss: 0.0339\nepoch 52 duration 1.676 train_loss 0.048 val_loss 0.170 val_acc 0.9560\nEpoch: 53, Batch(20/245), Loss: 0.0252\nEpoch: 53, Batch(40/245), Loss: 0.0753\nEpoch: 53, Batch(60/245), Loss: 0.0583\nEpoch: 53, Batch(80/245), Loss: 0.0379\nEpoch: 53, Batch(100/245), Loss: 0.0442\nEpoch: 53, Batch(120/245), Loss: 0.0576\nEpoch: 53, Batch(140/245), Loss: 0.0380\nEpoch: 53, Batch(160/245), Loss: 0.0839\nEpoch: 53, Batch(180/245), Loss: 0.0607\nEpoch: 53, Batch(200/245), Loss: 0.0331\nEpoch: 53, Batch(220/245), Loss: 0.0401\nEpoch: 53, Batch(240/245), Loss: 0.0512\nepoch 53 duration 1.679 train_loss 0.050 val_loss 0.171 val_acc 0.9524\nEpoch: 54, Batch(20/245), Loss: 0.0374\nEpoch: 54, Batch(40/245), Loss: 0.0672\nEpoch: 54, Batch(60/245), Loss: 0.0350\nEpoch: 54, Batch(80/245), Loss: 0.0322\nEpoch: 54, Batch(100/245), Loss: 0.0445\nEpoch: 54, Batch(120/245), Loss: 0.0711\nEpoch: 54, Batch(140/245), Loss: 0.0417\nEpoch: 54, Batch(160/245), Loss: 0.0393\nEpoch: 54, Batch(180/245), Loss: 0.0445\nEpoch: 54, Batch(200/245), Loss: 0.0390\nEpoch: 54, Batch(220/245), Loss: 0.0632\nEpoch: 54, Batch(240/245), Loss: 0.0430\nepoch 54 duration 1.679 train_loss 0.046 val_loss 0.161 val_acc 0.9524\nEpoch: 55, Batch(20/245), Loss: 0.0348\nEpoch: 55, Batch(40/245), Loss: 0.0376\nEpoch: 55, Batch(60/245), Loss: 0.0250\nEpoch: 55, Batch(80/245), Loss: 0.0413\nEpoch: 55, Batch(100/245), Loss: 0.0247\nEpoch: 55, Batch(120/245), Loss: 0.0409\nEpoch: 55, Batch(140/245), Loss: 0.0590\nEpoch: 55, Batch(160/245), Loss: 0.0424\nEpoch: 55, Batch(180/245), Loss: 0.0403\nEpoch: 55, Batch(200/245), Loss: 0.0477\nEpoch: 55, Batch(220/245), Loss: 0.0422\nEpoch: 55, Batch(240/245), Loss: 0.0336\nepoch 55 duration 1.677 train_loss 0.039 val_loss 0.172 val_acc 0.9548\nEpoch: 56, Batch(20/245), Loss: 0.0336\nEpoch: 56, Batch(40/245), Loss: 0.0266\nEpoch: 56, Batch(60/245), Loss: 0.0406\nEpoch: 56, Batch(80/245), Loss: 0.0567\nEpoch: 56, Batch(100/245), Loss: 0.0383\nEpoch: 56, Batch(120/245), Loss: 0.0486\nEpoch: 56, Batch(140/245), Loss: 0.0378\nEpoch: 56, Batch(160/245), Loss: 0.0353\nEpoch: 56, Batch(180/245), Loss: 0.0417\nEpoch: 56, Batch(200/245), Loss: 0.0264\nEpoch: 56, Batch(220/245), Loss: 0.0376\nEpoch: 56, Batch(240/245), Loss: 0.0341\nepoch 56 duration 1.672 train_loss 0.038 val_loss 0.184 val_acc 0.9536\nEpoch: 57, Batch(20/245), Loss: 0.0346\nEpoch: 57, Batch(40/245), Loss: 0.0496\nEpoch: 57, Batch(60/245), Loss: 0.0546\nEpoch: 57, Batch(80/245), Loss: 0.0531\nEpoch: 57, Batch(100/245), Loss: 0.0321\nEpoch: 57, Batch(120/245), Loss: 0.0423\nEpoch: 57, Batch(140/245), Loss: 0.0426\nEpoch: 57, Batch(160/245), Loss: 0.0267\nEpoch: 57, Batch(180/245), Loss: 0.0404\nEpoch: 57, Batch(200/245), Loss: 0.0380\nEpoch: 57, Batch(220/245), Loss: 0.0239\nEpoch: 57, Batch(240/245), Loss: 0.0353\nepoch 57 duration 1.678 train_loss 0.040 val_loss 0.181 val_acc 0.9512\nEpoch: 58, Batch(20/245), Loss: 0.0594\nEpoch: 58, Batch(40/245), Loss: 0.0454\nEpoch: 58, Batch(60/245), Loss: 0.0398\nEpoch: 58, Batch(80/245), Loss: 0.0373\nEpoch: 58, Batch(100/245), Loss: 0.0331\nEpoch: 58, Batch(120/245), Loss: 0.0305\nEpoch: 58, Batch(140/245), Loss: 0.0284\nEpoch: 58, Batch(160/245), Loss: 0.0656\nEpoch: 58, Batch(180/245), Loss: 0.0549\nEpoch: 58, Batch(200/245), Loss: 0.0527\nEpoch: 58, Batch(220/245), Loss: 0.0512\nEpoch: 58, Batch(240/245), Loss: 0.0444\nepoch 58 duration 1.677 train_loss 0.046 val_loss 0.181 val_acc 0.9548\nEpoch: 59, Batch(20/245), Loss: 0.0279\nEpoch: 59, Batch(40/245), Loss: 0.0206\nEpoch: 59, Batch(60/245), Loss: 0.0564\nEpoch: 59, Batch(80/245), Loss: 0.0198\nEpoch: 59, Batch(100/245), Loss: 0.0617\nEpoch: 59, Batch(120/245), Loss: 0.0300\nEpoch: 59, Batch(140/245), Loss: 0.0597\nEpoch: 59, Batch(160/245), Loss: 0.0298\nEpoch: 59, Batch(180/245), Loss: 0.0434\nEpoch: 59, Batch(200/245), Loss: 0.0443\nEpoch: 59, Batch(220/245), Loss: 0.0765\nEpoch: 59, Batch(240/245), Loss: 0.0355\nepoch 59 duration 1.674 train_loss 0.043 val_loss 0.191 val_acc 0.9476\nEpoch: 60, Batch(20/245), Loss: 0.0311\nEpoch: 60, Batch(40/245), Loss: 0.0292\nEpoch: 60, Batch(60/245), Loss: 0.0381\nEpoch: 60, Batch(80/245), Loss: 0.0237\nEpoch: 60, Batch(100/245), Loss: 0.0304\nEpoch: 60, Batch(120/245), Loss: 0.0358\nEpoch: 60, Batch(140/245), Loss: 0.0293\nEpoch: 60, Batch(160/245), Loss: 0.0606\nEpoch: 60, Batch(180/245), Loss: 0.0396\nEpoch: 60, Batch(200/245), Loss: 0.0720\nEpoch: 60, Batch(220/245), Loss: 0.0374\nEpoch: 60, Batch(240/245), Loss: 0.0587\nepoch 60 duration 1.681 train_loss 0.041 val_loss 0.186 val_acc 0.9476\nEpoch: 61, Batch(20/245), Loss: 0.0320\nEpoch: 61, Batch(40/245), Loss: 0.0423\nEpoch: 61, Batch(60/245), Loss: 0.0415\nEpoch: 61, Batch(80/245), Loss: 0.0323\nEpoch: 61, Batch(100/245), Loss: 0.0222\nEpoch: 61, Batch(120/245), Loss: 0.0322\nEpoch: 61, Batch(140/245), Loss: 0.0282\nEpoch: 61, Batch(160/245), Loss: 0.0372\nEpoch: 61, Batch(180/245), Loss: 0.0515\nEpoch: 61, Batch(200/245), Loss: 0.0394\nEpoch: 61, Batch(220/245), Loss: 0.0403\nEpoch: 61, Batch(240/245), Loss: 0.0317\nepoch 61 duration 1.672 train_loss 0.036 val_loss 0.186 val_acc 0.9548\nEpoch: 62, Batch(20/245), Loss: 0.0275\nEpoch: 62, Batch(40/245), Loss: 0.0478\nEpoch: 62, Batch(60/245), Loss: 0.0280\nEpoch: 62, Batch(80/245), Loss: 0.0221\nEpoch: 62, Batch(100/245), Loss: 0.0379\nEpoch: 62, Batch(120/245), Loss: 0.0462\nEpoch: 62, Batch(140/245), Loss: 0.0474\nEpoch: 62, Batch(160/245), Loss: 0.0315\nEpoch: 62, Batch(180/245), Loss: 0.0364\nEpoch: 62, Batch(200/245), Loss: 0.0361\nEpoch: 62, Batch(220/245), Loss: 0.0319\nEpoch: 62, Batch(240/245), Loss: 0.0313\nepoch 62 duration 1.676 train_loss 0.036 val_loss 0.183 val_acc 0.9524\nEpoch: 63, Batch(20/245), Loss: 0.0368\nEpoch: 63, Batch(40/245), Loss: 0.0481\nEpoch: 63, Batch(60/245), Loss: 0.0302\nEpoch: 63, Batch(80/245), Loss: 0.0336\nEpoch: 63, Batch(100/245), Loss: 0.0413\nEpoch: 63, Batch(120/245), Loss: 0.0552\nEpoch: 63, Batch(140/245), Loss: 0.0333\nEpoch: 63, Batch(160/245), Loss: 0.0367\nEpoch: 63, Batch(180/245), Loss: 0.0548\nEpoch: 63, Batch(200/245), Loss: 0.0351\nEpoch: 63, Batch(220/245), Loss: 0.0248\nEpoch: 63, Batch(240/245), Loss: 0.0255\nepoch 63 duration 1.684 train_loss 0.038 val_loss 0.187 val_acc 0.9524\nEpoch: 64, Batch(20/245), Loss: 0.0296\nEpoch: 64, Batch(40/245), Loss: 0.0292\nEpoch: 64, Batch(60/245), Loss: 0.0298\nEpoch: 64, Batch(80/245), Loss: 0.0307\nEpoch: 64, Batch(100/245), Loss: 0.0466\nEpoch: 64, Batch(120/245), Loss: 0.0373\nEpoch: 64, Batch(140/245), Loss: 0.0683\nEpoch: 64, Batch(160/245), Loss: 0.0524\nEpoch: 64, Batch(180/245), Loss: 0.0535\nEpoch: 64, Batch(200/245), Loss: 0.0376\nEpoch: 64, Batch(220/245), Loss: 0.0281\nEpoch: 64, Batch(240/245), Loss: 0.0352\nepoch 64 duration 1.673 train_loss 0.040 val_loss 0.204 val_acc 0.9536\nEpoch: 65, Batch(20/245), Loss: 0.0294\nEpoch: 65, Batch(40/245), Loss: 0.0303\nEpoch: 65, Batch(60/245), Loss: 0.0304\nEpoch: 65, Batch(80/245), Loss: 0.0649\nEpoch: 65, Batch(100/245), Loss: 0.0487\nEpoch: 65, Batch(120/245), Loss: 0.0402\nEpoch: 65, Batch(140/245), Loss: 0.0394\nEpoch: 65, Batch(160/245), Loss: 0.0258\nEpoch: 65, Batch(180/245), Loss: 0.0362\nEpoch: 65, Batch(200/245), Loss: 0.0305\nEpoch: 65, Batch(220/245), Loss: 0.0424\nEpoch: 65, Batch(240/245), Loss: 0.0283\nepoch 65 duration 1.672 train_loss 0.037 val_loss 0.196 val_acc 0.9512\nEpoch: 66, Batch(20/245), Loss: 0.0427\nEpoch: 66, Batch(40/245), Loss: 0.0374\nEpoch: 66, Batch(60/245), Loss: 0.0278\nEpoch: 66, Batch(80/245), Loss: 0.0325\nEpoch: 66, Batch(100/245), Loss: 0.0311\nEpoch: 66, Batch(120/245), Loss: 0.0245\nEpoch: 66, Batch(140/245), Loss: 0.0216\nEpoch: 66, Batch(160/245), Loss: 0.0375\nEpoch: 66, Batch(180/245), Loss: 0.0241\nEpoch: 66, Batch(200/245), Loss: 0.0273\nEpoch: 66, Batch(220/245), Loss: 0.0584\nEpoch: 66, Batch(240/245), Loss: 0.0431\nepoch 66 duration 1.679 train_loss 0.036 val_loss 0.202 val_acc 0.9476\nEpoch: 67, Batch(20/245), Loss: 0.0427\nEpoch: 67, Batch(40/245), Loss: 0.0322\nEpoch: 67, Batch(60/245), Loss: 0.0385\nEpoch: 67, Batch(80/245), Loss: 0.0529\nEpoch: 67, Batch(100/245), Loss: 0.0269\nEpoch: 67, Batch(120/245), Loss: 0.0243\nEpoch: 67, Batch(140/245), Loss: 0.0389\nEpoch: 67, Batch(160/245), Loss: 0.0397\nEpoch: 67, Batch(180/245), Loss: 0.0927\nEpoch: 67, Batch(200/245), Loss: 0.0329\nEpoch: 67, Batch(220/245), Loss: 0.0360\nEpoch: 67, Batch(240/245), Loss: 0.0288\nepoch 67 duration 1.673 train_loss 0.040 val_loss 0.196 val_acc 0.9571\nEpoch: 68, Batch(20/245), Loss: 0.0320\nEpoch: 68, Batch(40/245), Loss: 0.0410\nEpoch: 68, Batch(60/245), Loss: 0.0317\nEpoch: 68, Batch(80/245), Loss: 0.0272\nEpoch: 68, Batch(100/245), Loss: 0.0424\nEpoch: 68, Batch(120/245), Loss: 0.0396\nEpoch: 68, Batch(140/245), Loss: 0.0261\nEpoch: 68, Batch(160/245), Loss: 0.0517\nEpoch: 68, Batch(180/245), Loss: 0.0313\nEpoch: 68, Batch(200/245), Loss: 0.0388\nEpoch: 68, Batch(220/245), Loss: 0.0519\nEpoch: 68, Batch(240/245), Loss: 0.0322\nepoch 68 duration 1.677 train_loss 0.037 val_loss 0.202 val_acc 0.9536\nEpoch: 69, Batch(20/245), Loss: 0.0185\nEpoch: 69, Batch(40/245), Loss: 0.0358\nEpoch: 69, Batch(60/245), Loss: 0.0319\nEpoch: 69, Batch(80/245), Loss: 0.0362\nEpoch: 69, Batch(100/245), Loss: 0.0290\nEpoch: 69, Batch(120/245), Loss: 0.0535\nEpoch: 69, Batch(140/245), Loss: 0.0237\nEpoch: 69, Batch(160/245), Loss: 0.0487\nEpoch: 69, Batch(180/245), Loss: 0.0343\nEpoch: 69, Batch(200/245), Loss: 0.0447\nEpoch: 69, Batch(220/245), Loss: 0.0425\nEpoch: 69, Batch(240/245), Loss: 0.0362\nepoch 69 duration 1.672 train_loss 0.036 val_loss 0.198 val_acc 0.9571\nEpoch: 70, Batch(20/245), Loss: 0.0289\nEpoch: 70, Batch(40/245), Loss: 0.0408\nEpoch: 70, Batch(60/245), Loss: 0.0747\nEpoch: 70, Batch(80/245), Loss: 0.0237\nEpoch: 70, Batch(100/245), Loss: 0.0505\nEpoch: 70, Batch(120/245), Loss: 0.0350\nEpoch: 70, Batch(140/245), Loss: 0.0432\nEpoch: 70, Batch(160/245), Loss: 0.0585\nEpoch: 70, Batch(180/245), Loss: 0.0489\nEpoch: 70, Batch(200/245), Loss: 0.0228\nEpoch: 70, Batch(220/245), Loss: 0.0390\nEpoch: 70, Batch(240/245), Loss: 0.0380\nepoch 70 duration 1.673 train_loss 0.042 val_loss 0.201 val_acc 0.9548\nEpoch: 71, Batch(20/245), Loss: 0.0504\nEpoch: 71, Batch(40/245), Loss: 0.0400\nEpoch: 71, Batch(60/245), Loss: 0.0444\nEpoch: 71, Batch(80/245), Loss: 0.0555\nEpoch: 71, Batch(100/245), Loss: 0.0506\nEpoch: 71, Batch(120/245), Loss: 0.0213\nEpoch: 71, Batch(140/245), Loss: 0.0741\nEpoch: 71, Batch(160/245), Loss: 0.0444\nEpoch: 71, Batch(180/245), Loss: 0.0343\nEpoch: 71, Batch(200/245), Loss: 0.0386\nEpoch: 71, Batch(220/245), Loss: 0.0666\nEpoch: 71, Batch(240/245), Loss: 0.0396\nepoch 71 duration 1.677 train_loss 0.047 val_loss 0.187 val_acc 0.9548\nEpoch: 72, Batch(20/245), Loss: 0.0196\nEpoch: 72, Batch(40/245), Loss: 0.0539\nEpoch: 72, Batch(60/245), Loss: 0.0356\nEpoch: 72, Batch(80/245), Loss: 0.0683\nEpoch: 72, Batch(100/245), Loss: 0.0313\nEpoch: 72, Batch(120/245), Loss: 0.0376\nEpoch: 72, Batch(140/245), Loss: 0.0278\nEpoch: 72, Batch(160/245), Loss: 0.0267\nEpoch: 72, Batch(180/245), Loss: 0.0347\nEpoch: 72, Batch(200/245), Loss: 0.0236\nEpoch: 72, Batch(220/245), Loss: 0.0515\nEpoch: 72, Batch(240/245), Loss: 0.0314\nepoch 72 duration 1.671 train_loss 0.037 val_loss 0.186 val_acc 0.9512\nEpoch: 73, Batch(20/245), Loss: 0.0300\nEpoch: 73, Batch(40/245), Loss: 0.0232\nEpoch: 73, Batch(60/245), Loss: 0.0400\nEpoch: 73, Batch(80/245), Loss: 0.0573\nEpoch: 73, Batch(100/245), Loss: 0.0351\nEpoch: 73, Batch(120/245), Loss: 0.0300\nEpoch: 73, Batch(140/245), Loss: 0.0220\nEpoch: 73, Batch(160/245), Loss: 0.0597\nEpoch: 73, Batch(180/245), Loss: 0.0518\nEpoch: 73, Batch(200/245), Loss: 0.0450\nEpoch: 73, Batch(220/245), Loss: 0.0373\nEpoch: 73, Batch(240/245), Loss: 0.0312\nepoch 73 duration 1.674 train_loss 0.038 val_loss 0.167 val_acc 0.9571\nEpoch: 74, Batch(20/245), Loss: 0.0253\nEpoch: 74, Batch(40/245), Loss: 0.0351\nEpoch: 74, Batch(60/245), Loss: 0.0408\nEpoch: 74, Batch(80/245), Loss: 0.0389\nEpoch: 74, Batch(100/245), Loss: 0.0238\nEpoch: 74, Batch(120/245), Loss: 0.0435\nEpoch: 74, Batch(140/245), Loss: 0.0276\nEpoch: 74, Batch(160/245), Loss: 0.0386\nEpoch: 74, Batch(180/245), Loss: 0.0305\nEpoch: 74, Batch(200/245), Loss: 0.0394\nEpoch: 74, Batch(220/245), Loss: 0.0228\nEpoch: 74, Batch(240/245), Loss: 0.0285\nepoch 74 duration 1.675 train_loss 0.033 val_loss 0.191 val_acc 0.9560\nEpoch: 75, Batch(20/245), Loss: 0.0261\nEpoch: 75, Batch(40/245), Loss: 0.0280\nEpoch: 75, Batch(60/245), Loss: 0.0221\nEpoch: 75, Batch(80/245), Loss: 0.0344\nEpoch: 75, Batch(100/245), Loss: 0.0343\nEpoch: 75, Batch(120/245), Loss: 0.0354\nEpoch: 75, Batch(140/245), Loss: 0.0316\nEpoch: 75, Batch(160/245), Loss: 0.0228\nEpoch: 75, Batch(180/245), Loss: 0.0284\nEpoch: 75, Batch(200/245), Loss: 0.0437\nEpoch: 75, Batch(220/245), Loss: 0.0375\nEpoch: 75, Batch(240/245), Loss: 0.0662\nepoch 75 duration 1.672 train_loss 0.034 val_loss 0.184 val_acc 0.9595\nEpoch: 76, Batch(20/245), Loss: 0.0263\nEpoch: 76, Batch(40/245), Loss: 0.0321\nEpoch: 76, Batch(60/245), Loss: 0.0198\nEpoch: 76, Batch(80/245), Loss: 0.0209\nEpoch: 76, Batch(100/245), Loss: 0.0367\nEpoch: 76, Batch(120/245), Loss: 0.0282\nEpoch: 76, Batch(140/245), Loss: 0.0512\nEpoch: 76, Batch(160/245), Loss: 0.0287\nEpoch: 76, Batch(180/245), Loss: 0.0398\nEpoch: 76, Batch(200/245), Loss: 0.0268\nEpoch: 76, Batch(220/245), Loss: 0.0347\nEpoch: 76, Batch(240/245), Loss: 0.0305\nepoch 76 duration 1.672 train_loss 0.031 val_loss 0.202 val_acc 0.9536\nEpoch: 77, Batch(20/245), Loss: 0.0319\nEpoch: 77, Batch(40/245), Loss: 0.0279\nEpoch: 77, Batch(60/245), Loss: 0.0274\nEpoch: 77, Batch(80/245), Loss: 0.0215\nEpoch: 77, Batch(100/245), Loss: 0.0382\nEpoch: 77, Batch(120/245), Loss: 0.0275\nEpoch: 77, Batch(140/245), Loss: 0.0287\nEpoch: 77, Batch(160/245), Loss: 0.0324\nEpoch: 77, Batch(180/245), Loss: 0.0426\nEpoch: 77, Batch(200/245), Loss: 0.0190\nEpoch: 77, Batch(220/245), Loss: 0.0281\nEpoch: 77, Batch(240/245), Loss: 0.0300\nepoch 77 duration 1.672 train_loss 0.030 val_loss 0.192 val_acc 0.9536\nEpoch: 78, Batch(20/245), Loss: 0.0377\nEpoch: 78, Batch(40/245), Loss: 0.0333\nEpoch: 78, Batch(60/245), Loss: 0.0300\nEpoch: 78, Batch(80/245), Loss: 0.0556\nEpoch: 78, Batch(100/245), Loss: 0.0353\nEpoch: 78, Batch(120/245), Loss: 0.0282\nEpoch: 78, Batch(140/245), Loss: 0.0374\nEpoch: 78, Batch(160/245), Loss: 0.0323\nEpoch: 78, Batch(180/245), Loss: 0.0283\nEpoch: 78, Batch(200/245), Loss: 0.0315\nEpoch: 78, Batch(220/245), Loss: 0.0356\nEpoch: 78, Batch(240/245), Loss: 0.0227\nepoch 78 duration 1.673 train_loss 0.034 val_loss 0.211 val_acc 0.9488\nEpoch: 79, Batch(20/245), Loss: 0.0370\nEpoch: 79, Batch(40/245), Loss: 0.0444\nEpoch: 79, Batch(60/245), Loss: 0.0330\nEpoch: 79, Batch(80/245), Loss: 0.0611\nEpoch: 79, Batch(100/245), Loss: 0.0321\nEpoch: 79, Batch(120/245), Loss: 0.0218\nEpoch: 79, Batch(140/245), Loss: 0.0353\nEpoch: 79, Batch(160/245), Loss: 0.0399\nEpoch: 79, Batch(180/245), Loss: 0.0288\nEpoch: 79, Batch(200/245), Loss: 0.0354\nEpoch: 79, Batch(220/245), Loss: 0.0298\nEpoch: 79, Batch(240/245), Loss: 0.0274\nepoch 79 duration 1.674 train_loss 0.036 val_loss 0.206 val_acc 0.9536\nEpoch: 80, Batch(20/245), Loss: 0.0256\nEpoch: 80, Batch(40/245), Loss: 0.0357\nEpoch: 80, Batch(60/245), Loss: 0.0410\nEpoch: 80, Batch(80/245), Loss: 0.0258\nEpoch: 80, Batch(100/245), Loss: 0.0561\nEpoch: 80, Batch(120/245), Loss: 0.0358\nEpoch: 80, Batch(140/245), Loss: 0.0287\nEpoch: 80, Batch(160/245), Loss: 0.0342\nEpoch: 80, Batch(180/245), Loss: 0.0242\nEpoch: 80, Batch(200/245), Loss: 0.0391\nEpoch: 80, Batch(220/245), Loss: 0.0427\nEpoch: 80, Batch(240/245), Loss: 0.0242\nepoch 80 duration 1.673 train_loss 0.034 val_loss 0.208 val_acc 0.9512\nEpoch: 81, Batch(20/245), Loss: 0.0305\nEpoch: 81, Batch(40/245), Loss: 0.0347\nEpoch: 81, Batch(60/245), Loss: 0.0317\nEpoch: 81, Batch(80/245), Loss: 0.0234\nEpoch: 81, Batch(100/245), Loss: 0.0291\nEpoch: 81, Batch(120/245), Loss: 0.0442\nEpoch: 81, Batch(140/245), Loss: 0.0271\nEpoch: 81, Batch(160/245), Loss: 0.0343\nEpoch: 81, Batch(180/245), Loss: 0.0253\nEpoch: 81, Batch(200/245), Loss: 0.0265\nEpoch: 81, Batch(220/245), Loss: 0.0245\nEpoch: 81, Batch(240/245), Loss: 0.0421\nepoch 81 duration 1.674 train_loss 0.032 val_loss 0.199 val_acc 0.9536\nEpoch: 82, Batch(20/245), Loss: 0.0242\nEpoch: 82, Batch(40/245), Loss: 0.0253\nEpoch: 82, Batch(60/245), Loss: 0.0256\nEpoch: 82, Batch(80/245), Loss: 0.0402\nEpoch: 82, Batch(100/245), Loss: 0.0301\nEpoch: 82, Batch(120/245), Loss: 0.0216\nEpoch: 82, Batch(140/245), Loss: 0.0302\nEpoch: 82, Batch(160/245), Loss: 0.0290\nEpoch: 82, Batch(180/245), Loss: 0.0280\nEpoch: 82, Batch(200/245), Loss: 0.0362\nEpoch: 82, Batch(220/245), Loss: 0.0316\nEpoch: 82, Batch(240/245), Loss: 0.0300\nepoch 82 duration 1.673 train_loss 0.029 val_loss 0.213 val_acc 0.9536\nEpoch: 83, Batch(20/245), Loss: 0.0497\nEpoch: 83, Batch(40/245), Loss: 0.0495\nEpoch: 83, Batch(60/245), Loss: 0.0325\nEpoch: 83, Batch(80/245), Loss: 0.0458\nEpoch: 83, Batch(100/245), Loss: 0.0247\nEpoch: 83, Batch(120/245), Loss: 0.0426\nEpoch: 83, Batch(140/245), Loss: 0.0407\nEpoch: 83, Batch(160/245), Loss: 0.0287\nEpoch: 83, Batch(180/245), Loss: 0.0295\nEpoch: 83, Batch(200/245), Loss: 0.0223\nEpoch: 83, Batch(220/245), Loss: 0.0355\nEpoch: 83, Batch(240/245), Loss: 0.0219\nepoch 83 duration 1.678 train_loss 0.035 val_loss 0.208 val_acc 0.9536\nEpoch: 84, Batch(20/245), Loss: 0.0250\nEpoch: 84, Batch(40/245), Loss: 0.0345\nEpoch: 84, Batch(60/245), Loss: 0.0537\nEpoch: 84, Batch(80/245), Loss: 0.0289\nEpoch: 84, Batch(100/245), Loss: 0.0249\nEpoch: 84, Batch(120/245), Loss: 0.0239\nEpoch: 84, Batch(140/245), Loss: 0.0257\nEpoch: 84, Batch(160/245), Loss: 0.0518\nEpoch: 84, Batch(180/245), Loss: 0.0436\nEpoch: 84, Batch(200/245), Loss: 0.0271\nEpoch: 84, Batch(220/245), Loss: 0.0255\nEpoch: 84, Batch(240/245), Loss: 0.0189\nepoch 84 duration 1.678 train_loss 0.032 val_loss 0.204 val_acc 0.9548\nEpoch: 85, Batch(20/245), Loss: 0.0265\nEpoch: 85, Batch(40/245), Loss: 0.0265\nEpoch: 85, Batch(60/245), Loss: 0.0283\nEpoch: 85, Batch(80/245), Loss: 0.0359\nEpoch: 85, Batch(100/245), Loss: 0.0381\nEpoch: 85, Batch(120/245), Loss: 0.0351\nEpoch: 85, Batch(140/245), Loss: 0.0204\nEpoch: 85, Batch(160/245), Loss: 0.0312\nEpoch: 85, Batch(180/245), Loss: 0.0425\nEpoch: 85, Batch(200/245), Loss: 0.0261\nEpoch: 85, Batch(220/245), Loss: 0.0224\nEpoch: 85, Batch(240/245), Loss: 0.0356\nepoch 85 duration 1.672 train_loss 0.031 val_loss 0.202 val_acc 0.9548\nEpoch: 86, Batch(20/245), Loss: 0.0232\nEpoch: 86, Batch(40/245), Loss: 0.0329\nEpoch: 86, Batch(60/245), Loss: 0.0230\nEpoch: 86, Batch(80/245), Loss: 0.0437\nEpoch: 86, Batch(100/245), Loss: 0.0332\nEpoch: 86, Batch(120/245), Loss: 0.0211\nEpoch: 86, Batch(140/245), Loss: 0.0317\nEpoch: 86, Batch(160/245), Loss: 0.0335\nEpoch: 86, Batch(180/245), Loss: 0.0312\nEpoch: 86, Batch(200/245), Loss: 0.0267\nEpoch: 86, Batch(220/245), Loss: 0.0223\nEpoch: 86, Batch(240/245), Loss: 0.0253\nepoch 86 duration 1.673 train_loss 0.029 val_loss 0.201 val_acc 0.9536\nEpoch: 87, Batch(20/245), Loss: 0.0341\nEpoch: 87, Batch(40/245), Loss: 0.0402\nEpoch: 87, Batch(60/245), Loss: 0.0631\nEpoch: 87, Batch(80/245), Loss: 0.0243\nEpoch: 87, Batch(100/245), Loss: 0.0320\nEpoch: 87, Batch(120/245), Loss: 0.0302\nEpoch: 87, Batch(140/245), Loss: 0.0302\nEpoch: 87, Batch(160/245), Loss: 0.0370\nEpoch: 87, Batch(180/245), Loss: 0.0213\nEpoch: 87, Batch(200/245), Loss: 0.0526\nEpoch: 87, Batch(220/245), Loss: 0.0349\nEpoch: 87, Batch(240/245), Loss: 0.0334\nepoch 87 duration 1.673 train_loss 0.036 val_loss 0.207 val_acc 0.9524\nEpoch: 88, Batch(20/245), Loss: 0.0275\nEpoch: 88, Batch(40/245), Loss: 0.0258\nEpoch: 88, Batch(60/245), Loss: 0.0405\nEpoch: 88, Batch(80/245), Loss: 0.0533\nEpoch: 88, Batch(100/245), Loss: 0.0235\nEpoch: 88, Batch(120/245), Loss: 0.0450\nEpoch: 88, Batch(140/245), Loss: 0.0448\nEpoch: 88, Batch(160/245), Loss: 0.0325\nEpoch: 88, Batch(180/245), Loss: 0.0226\nEpoch: 88, Batch(200/245), Loss: 0.0261\nEpoch: 88, Batch(220/245), Loss: 0.0342\nEpoch: 88, Batch(240/245), Loss: 0.0349\nepoch 88 duration 1.673 train_loss 0.034 val_loss 0.197 val_acc 0.9524\nEpoch: 89, Batch(20/245), Loss: 0.0300\nEpoch: 89, Batch(40/245), Loss: 0.0283\nEpoch: 89, Batch(60/245), Loss: 0.0186\nEpoch: 89, Batch(80/245), Loss: 0.0436\nEpoch: 89, Batch(100/245), Loss: 0.0260\nEpoch: 89, Batch(120/245), Loss: 0.0367\nEpoch: 89, Batch(140/245), Loss: 0.0342\nEpoch: 89, Batch(160/245), Loss: 0.0228\nEpoch: 89, Batch(180/245), Loss: 0.0438\nEpoch: 89, Batch(200/245), Loss: 0.0275\nEpoch: 89, Batch(220/245), Loss: 0.0252\nEpoch: 89, Batch(240/245), Loss: 0.0223\nepoch 89 duration 1.672 train_loss 0.030 val_loss 0.200 val_acc 0.9560\nEpoch: 90, Batch(20/245), Loss: 0.0279\nEpoch: 90, Batch(40/245), Loss: 0.0269\nEpoch: 90, Batch(60/245), Loss: 0.0348\nEpoch: 90, Batch(80/245), Loss: 0.0308\nEpoch: 90, Batch(100/245), Loss: 0.0455\nEpoch: 90, Batch(120/245), Loss: 0.0403\nEpoch: 90, Batch(140/245), Loss: 0.0309\nEpoch: 90, Batch(160/245), Loss: 0.0214\nEpoch: 90, Batch(180/245), Loss: 0.0329\nEpoch: 90, Batch(200/245), Loss: 0.0242\nEpoch: 90, Batch(220/245), Loss: 0.0256\nEpoch: 90, Batch(240/245), Loss: 0.0206\nepoch 90 duration 1.672 train_loss 0.030 val_loss 0.215 val_acc 0.9524\nEpoch: 91, Batch(20/245), Loss: 0.0236\nEpoch: 91, Batch(40/245), Loss: 0.0398\nEpoch: 91, Batch(60/245), Loss: 0.0489\nEpoch: 91, Batch(80/245), Loss: 0.0509\nEpoch: 91, Batch(100/245), Loss: 0.0282\nEpoch: 91, Batch(120/245), Loss: 0.0223\nEpoch: 91, Batch(140/245), Loss: 0.0319\nEpoch: 91, Batch(160/245), Loss: 0.0243\nEpoch: 91, Batch(180/245), Loss: 0.0281\nEpoch: 91, Batch(200/245), Loss: 0.0396\nEpoch: 91, Batch(220/245), Loss: 0.0262\nEpoch: 91, Batch(240/245), Loss: 0.0245\nepoch 91 duration 1.674 train_loss 0.032 val_loss 0.202 val_acc 0.9560\nEpoch: 92, Batch(20/245), Loss: 0.0564\nEpoch: 92, Batch(40/245), Loss: 0.0240\nEpoch: 92, Batch(60/245), Loss: 0.0286\nEpoch: 92, Batch(80/245), Loss: 0.0264\nEpoch: 92, Batch(100/245), Loss: 0.0207\nEpoch: 92, Batch(120/245), Loss: 0.0612\nEpoch: 92, Batch(140/245), Loss: 0.0284\nEpoch: 92, Batch(160/245), Loss: 0.0394\nEpoch: 92, Batch(180/245), Loss: 0.0376\nEpoch: 92, Batch(200/245), Loss: 0.0327\nEpoch: 92, Batch(220/245), Loss: 0.0400\nEpoch: 92, Batch(240/245), Loss: 0.0319\nepoch 92 duration 1.673 train_loss 0.035 val_loss 0.204 val_acc 0.9548\nEpoch: 93, Batch(20/245), Loss: 0.0367\nEpoch: 93, Batch(40/245), Loss: 0.0216\nEpoch: 93, Batch(60/245), Loss: 0.0512\nEpoch: 93, Batch(80/245), Loss: 0.0335\nEpoch: 93, Batch(100/245), Loss: 0.0305\nEpoch: 93, Batch(120/245), Loss: 0.0324\nEpoch: 93, Batch(140/245), Loss: 0.0234\nEpoch: 93, Batch(160/245), Loss: 0.0264\nEpoch: 93, Batch(180/245), Loss: 0.0255\nEpoch: 93, Batch(200/245), Loss: 0.0224\nEpoch: 93, Batch(220/245), Loss: 0.0329\nEpoch: 93, Batch(240/245), Loss: 0.0290\nepoch 93 duration 1.673 train_loss 0.030 val_loss 0.199 val_acc 0.9536\nEpoch: 94, Batch(20/245), Loss: 0.0230\nEpoch: 94, Batch(40/245), Loss: 0.0242\nEpoch: 94, Batch(60/245), Loss: 0.0252\nEpoch: 94, Batch(80/245), Loss: 0.0353\nEpoch: 94, Batch(100/245), Loss: 0.0320\nEpoch: 94, Batch(120/245), Loss: 0.0403\nEpoch: 94, Batch(140/245), Loss: 0.0468\nEpoch: 94, Batch(160/245), Loss: 0.0232\nEpoch: 94, Batch(180/245), Loss: 0.0315\nEpoch: 94, Batch(200/245), Loss: 0.0195\nEpoch: 94, Batch(220/245), Loss: 0.0266\nEpoch: 94, Batch(240/245), Loss: 0.0244\nepoch 94 duration 1.673 train_loss 0.029 val_loss 0.211 val_acc 0.9536\nEpoch: 95, Batch(20/245), Loss: 0.0240\nEpoch: 95, Batch(40/245), Loss: 0.0316\nEpoch: 95, Batch(60/245), Loss: 0.0302\nEpoch: 95, Batch(80/245), Loss: 0.0262\nEpoch: 95, Batch(100/245), Loss: 0.0298\nEpoch: 95, Batch(120/245), Loss: 0.0345\nEpoch: 95, Batch(140/245), Loss: 0.0244\nEpoch: 95, Batch(160/245), Loss: 0.0298\nEpoch: 95, Batch(180/245), Loss: 0.0348\nEpoch: 95, Batch(200/245), Loss: 0.0314\nEpoch: 95, Batch(220/245), Loss: 0.0336\nEpoch: 95, Batch(240/245), Loss: 0.0219\nepoch 95 duration 1.677 train_loss 0.030 val_loss 0.204 val_acc 0.9500\nEpoch: 96, Batch(20/245), Loss: 0.0183\nEpoch: 96, Batch(40/245), Loss: 0.0308\nEpoch: 96, Batch(60/245), Loss: 0.0209\nEpoch: 96, Batch(80/245), Loss: 0.0433\nEpoch: 96, Batch(100/245), Loss: 0.0306\nEpoch: 96, Batch(120/245), Loss: 0.0265\nEpoch: 96, Batch(140/245), Loss: 0.0252\nEpoch: 96, Batch(160/245), Loss: 0.0334\nEpoch: 96, Batch(180/245), Loss: 0.0483\nEpoch: 96, Batch(200/245), Loss: 0.0346\nEpoch: 96, Batch(220/245), Loss: 0.0235\nEpoch: 96, Batch(240/245), Loss: 0.0282\nepoch 96 duration 1.681 train_loss 0.030 val_loss 0.196 val_acc 0.9536\nEpoch: 97, Batch(20/245), Loss: 0.0182\nEpoch: 97, Batch(40/245), Loss: 0.0339\nEpoch: 97, Batch(60/245), Loss: 0.0204\nEpoch: 97, Batch(80/245), Loss: 0.0292\nEpoch: 97, Batch(100/245), Loss: 0.0468\nEpoch: 97, Batch(120/245), Loss: 0.0176\nEpoch: 97, Batch(140/245), Loss: 0.0252\nEpoch: 97, Batch(160/245), Loss: 0.0317\nEpoch: 97, Batch(180/245), Loss: 0.0209\nEpoch: 97, Batch(200/245), Loss: 0.0449\nEpoch: 97, Batch(220/245), Loss: 0.0205\nEpoch: 97, Batch(240/245), Loss: 0.0297\nepoch 97 duration 1.677 train_loss 0.029 val_loss 0.202 val_acc 0.9548\nEpoch: 98, Batch(20/245), Loss: 0.0271\nEpoch: 98, Batch(40/245), Loss: 0.0223\nEpoch: 98, Batch(60/245), Loss: 0.0393\nEpoch: 98, Batch(80/245), Loss: 0.0283\nEpoch: 98, Batch(100/245), Loss: 0.0208\nEpoch: 98, Batch(120/245), Loss: 0.0314\nEpoch: 98, Batch(140/245), Loss: 0.0240\nEpoch: 98, Batch(160/245), Loss: 0.0401\nEpoch: 98, Batch(180/245), Loss: 0.0216\nEpoch: 98, Batch(200/245), Loss: 0.0259\nEpoch: 98, Batch(220/245), Loss: 0.0292\nEpoch: 98, Batch(240/245), Loss: 0.0330\nepoch 98 duration 1.675 train_loss 0.028 val_loss 0.196 val_acc 0.9571\nEpoch: 99, Batch(20/245), Loss: 0.0271\nEpoch: 99, Batch(40/245), Loss: 0.0282\nEpoch: 99, Batch(60/245), Loss: 0.0281\nEpoch: 99, Batch(80/245), Loss: 0.0393\nEpoch: 99, Batch(100/245), Loss: 0.0343\nEpoch: 99, Batch(120/245), Loss: 0.0339\nEpoch: 99, Batch(140/245), Loss: 0.0258\nEpoch: 99, Batch(160/245), Loss: 0.0268\nEpoch: 99, Batch(180/245), Loss: 0.0380\nEpoch: 99, Batch(200/245), Loss: 0.0238\nEpoch: 99, Batch(220/245), Loss: 0.0153\nEpoch: 99, Batch(240/245), Loss: 0.0217\nepoch 99 duration 1.678 train_loss 0.028 val_loss 0.205 val_acc 0.9560\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!python test.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T02:28:50.617133Z","iopub.execute_input":"2024-12-16T02:28:50.617887Z","iopub.status.idle":"2024-12-16T02:29:13.181343Z","shell.execute_reply.started":"2024-12-16T02:28:50.617854Z","shell.execute_reply":"2024-12-16T02:29:13.180498Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main/test.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n#model parameters:  4628878\naverage inference time:  18.372788617724463\naccuracy:  0.9595238095238096\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}