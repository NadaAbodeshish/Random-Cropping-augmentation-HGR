{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T23:40:05.566068Z",
     "iopub.status.busy": "2024-12-15T23:40:05.565629Z",
     "iopub.status.idle": "2024-12-15T23:40:12.598103Z",
     "shell.execute_reply": "2024-12-15T23:40:12.597295Z",
     "shell.execute_reply.started": "2024-12-15T23:40:05.566037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion'...\n",
      "remote: Enumerating objects: 769, done.\u001b[K\n",
      "remote: Counting objects: 100% (132/132), done.\u001b[K\n",
      "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
      "remote: Total 769 (delta 67), reused 97 (delta 37), pack-reused 637 (from 1)\u001b[K\n",
      "Receiving objects: 100% (769/769), 166.71 MiB | 45.72 MiB/s, done.\n",
      "Resolving deltas: 100% (419/419), done.\n",
      "Updating files: 100% (213/213), done.\n"
     ]
    }
   ],
   "source": [
    "!git clonehttps://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:40:12.600126Z",
     "iopub.status.busy": "2024-12-15T23:40:12.599841Z",
     "iopub.status.idle": "2024-12-15T23:40:12.606503Z",
     "shell.execute_reply": "2024-12-15T23:40:12.605667Z",
     "shell.execute_reply.started": "2024-12-15T23:40:12.600098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/workingRandom-Cropping-augmentation-HGR/FPPRC/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:40:12.607704Z",
     "iopub.status.busy": "2024-12-15T23:40:12.607470Z",
     "iopub.status.idle": "2024-12-15T23:40:12.615787Z",
     "shell.execute_reply": "2024-12-15T23:40:12.614984Z",
     "shell.execute_reply.started": "2024-12-15T23:40:12.607680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = \"/kaggle/working/Random-Cropping-augmentation-HGR/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:40:12.617142Z",
     "iopub.status.busy": "2024-12-15T23:40:12.616866Z",
     "iopub.status.idle": "2024-12-15T23:40:49.819659Z",
     "shell.execute_reply": "2024-12-15T23:40:49.818521Z",
     "shell.execute_reply.started": "2024-12-15T23:40:12.617118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/shrec-processed/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/* /kaggle/working/Random-Cropping-augmentation-HGR/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:40:49.823265Z",
     "iopub.status.busy": "2024-12-15T23:40:49.822839Z",
     "iopub.status.idle": "2024-12-15T23:40:49.830131Z",
     "shell.execute_reply": "2024-12-15T23:40:49.829085Z",
     "shell.execute_reply.started": "2024-12-15T23:40:49.823206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:40:49.831875Z",
     "iopub.status.busy": "2024-12-15T23:40:49.831565Z",
     "iopub.status.idle": "2024-12-15T23:40:49.842013Z",
     "shell.execute_reply": "2024-12-15T23:40:49.841050Z",
     "shell.execute_reply.started": "2024-12-15T23:40:49.831840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/aRandom-Cropping-augmentation-HGR/FPPRC/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:40:49.843862Z",
     "iopub.status.busy": "2024-12-15T23:40:49.843475Z",
     "iopub.status.idle": "2024-12-15T23:41:03.518721Z",
     "shell.execute_reply": "2024-12-15T23:41:03.517587Z",
     "shell.execute_reply.started": "2024-12-15T23:40:49.843814Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs) (6.0.2)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: yacs\n",
      "Successfully installed yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:41:03.520834Z",
     "iopub.status.busy": "2024-12-15T23:41:03.520427Z",
     "iopub.status.idle": "2024-12-15T23:41:11.695381Z",
     "shell.execute_reply": "2024-12-15T23:41:11.694302Z",
     "shell.execute_reply.started": "2024-12-15T23:41:03.520792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bps\n",
      "  Downloading BPS-0.1.0-py3-none-any.whl.metadata (61 bytes)\n",
      "Downloading BPS-0.1.0-py3-none-any.whl (897 bytes)\n",
      "Installing collected packages: bps\n",
      "Successfully installed bps-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:41:11.697882Z",
     "iopub.status.busy": "2024-12-15T23:41:11.696997Z",
     "iopub.status.idle": "2024-12-15T23:41:20.960021Z",
     "shell.execute_reply": "2024-12-15T23:41:20.958944Z",
     "shell.execute_reply.started": "2024-12-15T23:41:11.697836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.6.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.6.2)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:41:20.962532Z",
     "iopub.status.busy": "2024-12-15T23:41:20.961649Z",
     "iopub.status.idle": "2024-12-16T02:28:50.614043Z",
     "shell.execute_reply": "2024-12-16T02:28:50.613176Z",
     "shell.execute_reply.started": "2024-12-15T23:41:20.962487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Epoch: 0, Batch(20/245), Loss: 2.5234\n",
      "Epoch: 0, Batch(40/245), Loss: 2.2478\n",
      "Epoch: 0, Batch(60/245), Loss: 2.1244\n",
      "Epoch: 0, Batch(80/245), Loss: 1.9025\n",
      "Epoch: 0, Batch(100/245), Loss: 1.9113\n",
      "Epoch: 0, Batch(120/245), Loss: 1.6662\n",
      "Epoch: 0, Batch(140/245), Loss: 1.5369\n",
      "Epoch: 0, Batch(160/245), Loss: 1.5851\n",
      "Epoch: 0, Batch(180/245), Loss: 1.3563\n",
      "Epoch: 0, Batch(200/245), Loss: 1.3143\n",
      "Epoch: 0, Batch(220/245), Loss: 1.3494\n",
      "Epoch: 0, Batch(240/245), Loss: 1.2614\n",
      "epoch 0 duration 1.523 train_loss 1.724 val_loss 1.023 val_acc 0.7429\n",
      "Epoch: 1, Batch(20/245), Loss: 1.1241\n",
      "Epoch: 1, Batch(40/245), Loss: 1.0179\n",
      "Epoch: 1, Batch(60/245), Loss: 1.0061\n",
      "Epoch: 1, Batch(80/245), Loss: 1.0069\n",
      "Epoch: 1, Batch(100/245), Loss: 0.9116\n",
      "Epoch: 1, Batch(120/245), Loss: 0.9975\n",
      "Epoch: 1, Batch(140/245), Loss: 0.8451\n",
      "Epoch: 1, Batch(160/245), Loss: 0.8266\n",
      "Epoch: 1, Batch(180/245), Loss: 0.8352\n",
      "Epoch: 1, Batch(200/245), Loss: 0.9098\n",
      "Epoch: 1, Batch(220/245), Loss: 0.8928\n",
      "Epoch: 1, Batch(240/245), Loss: 0.6600\n",
      "epoch 1 duration 1.674 train_loss 0.915 val_loss 0.616 val_acc 0.8667\n",
      "Epoch: 2, Batch(20/245), Loss: 0.6356\n",
      "Epoch: 2, Batch(40/245), Loss: 0.6874\n",
      "Epoch: 2, Batch(60/245), Loss: 0.6968\n",
      "Epoch: 2, Batch(80/245), Loss: 0.6876\n",
      "Epoch: 2, Batch(100/245), Loss: 0.6029\n",
      "Epoch: 2, Batch(120/245), Loss: 0.6220\n",
      "Epoch: 2, Batch(140/245), Loss: 0.5490\n",
      "Epoch: 2, Batch(160/245), Loss: 0.6930\n",
      "Epoch: 2, Batch(180/245), Loss: 0.5931\n",
      "Epoch: 2, Batch(200/245), Loss: 0.5965\n",
      "Epoch: 2, Batch(220/245), Loss: 0.5456\n",
      "Epoch: 2, Batch(240/245), Loss: 0.5538\n",
      "epoch 2 duration 1.671 train_loss 0.624 val_loss 0.489 val_acc 0.8893\n",
      "Epoch: 3, Batch(20/245), Loss: 0.4877\n",
      "Epoch: 3, Batch(40/245), Loss: 0.5145\n",
      "Epoch: 3, Batch(60/245), Loss: 0.5436\n",
      "Epoch: 3, Batch(80/245), Loss: 0.4713\n",
      "Epoch: 3, Batch(100/245), Loss: 0.4236\n",
      "Epoch: 3, Batch(120/245), Loss: 0.3958\n",
      "Epoch: 3, Batch(140/245), Loss: 0.4806\n",
      "Epoch: 3, Batch(160/245), Loss: 0.4324\n",
      "Epoch: 3, Batch(180/245), Loss: 0.4517\n",
      "Epoch: 3, Batch(200/245), Loss: 0.5158\n",
      "Epoch: 3, Batch(220/245), Loss: 0.4630\n",
      "Epoch: 3, Batch(240/245), Loss: 0.3814\n",
      "epoch 3 duration 1.673 train_loss 0.462 val_loss 0.344 val_acc 0.9131\n",
      "Epoch: 4, Batch(20/245), Loss: 0.3178\n",
      "Epoch: 4, Batch(40/245), Loss: 0.4717\n",
      "Epoch: 4, Batch(60/245), Loss: 0.3770\n",
      "Epoch: 4, Batch(80/245), Loss: 0.4415\n",
      "Epoch: 4, Batch(100/245), Loss: 0.3241\n",
      "Epoch: 4, Batch(120/245), Loss: 0.4342\n",
      "Epoch: 4, Batch(140/245), Loss: 0.3140\n",
      "Epoch: 4, Batch(160/245), Loss: 0.3942\n",
      "Epoch: 4, Batch(180/245), Loss: 0.3407\n",
      "Epoch: 4, Batch(200/245), Loss: 0.3124\n",
      "Epoch: 4, Batch(220/245), Loss: 0.4783\n",
      "Epoch: 4, Batch(240/245), Loss: 0.3151\n",
      "epoch 4 duration 1.675 train_loss 0.377 val_loss 0.320 val_acc 0.9226\n",
      "Epoch: 5, Batch(20/245), Loss: 0.2458\n",
      "Epoch: 5, Batch(40/245), Loss: 0.2977\n",
      "Epoch: 5, Batch(60/245), Loss: 0.2530\n",
      "Epoch: 5, Batch(80/245), Loss: 0.2386\n",
      "Epoch: 5, Batch(100/245), Loss: 0.3075\n",
      "Epoch: 5, Batch(120/245), Loss: 0.3299\n",
      "Epoch: 5, Batch(140/245), Loss: 0.2970\n",
      "Epoch: 5, Batch(160/245), Loss: 0.2556\n",
      "Epoch: 5, Batch(180/245), Loss: 0.2331\n",
      "Epoch: 5, Batch(200/245), Loss: 0.4340\n",
      "Epoch: 5, Batch(220/245), Loss: 0.2410\n",
      "Epoch: 5, Batch(240/245), Loss: 0.3449\n",
      "epoch 5 duration 1.672 train_loss 0.291 val_loss 0.323 val_acc 0.9190\n",
      "Epoch: 6, Batch(20/245), Loss: 0.2574\n",
      "Epoch: 6, Batch(40/245), Loss: 0.2625\n",
      "Epoch: 6, Batch(60/245), Loss: 0.2604\n",
      "Epoch: 6, Batch(80/245), Loss: 0.2584\n",
      "Epoch: 6, Batch(100/245), Loss: 0.2325\n",
      "Epoch: 6, Batch(120/245), Loss: 0.2871\n",
      "Epoch: 6, Batch(140/245), Loss: 0.2098\n",
      "Epoch: 6, Batch(160/245), Loss: 0.2569\n",
      "Epoch: 6, Batch(180/245), Loss: 0.2316\n",
      "Epoch: 6, Batch(200/245), Loss: 0.3522\n",
      "Epoch: 6, Batch(220/245), Loss: 0.2076\n",
      "Epoch: 6, Batch(240/245), Loss: 0.2783\n",
      "epoch 6 duration 1.673 train_loss 0.262 val_loss 0.305 val_acc 0.9238\n",
      "Epoch: 7, Batch(20/245), Loss: 0.2358\n",
      "Epoch: 7, Batch(40/245), Loss: 0.2762\n",
      "Epoch: 7, Batch(60/245), Loss: 0.2484\n",
      "Epoch: 7, Batch(80/245), Loss: 0.2345\n",
      "Epoch: 7, Batch(100/245), Loss: 0.1663\n",
      "Epoch: 7, Batch(120/245), Loss: 0.2463\n",
      "Epoch: 7, Batch(140/245), Loss: 0.2913\n",
      "Epoch: 7, Batch(160/245), Loss: 0.2196\n",
      "Epoch: 7, Batch(180/245), Loss: 0.2385\n",
      "Epoch: 7, Batch(200/245), Loss: 0.1837\n",
      "Epoch: 7, Batch(220/245), Loss: 0.2543\n",
      "Epoch: 7, Batch(240/245), Loss: 0.2194\n",
      "epoch 7 duration 1.672 train_loss 0.235 val_loss 0.301 val_acc 0.9190\n",
      "Epoch: 8, Batch(20/245), Loss: 0.1659\n",
      "Epoch: 8, Batch(40/245), Loss: 0.1818\n",
      "Epoch: 8, Batch(60/245), Loss: 0.1746\n",
      "Epoch: 8, Batch(80/245), Loss: 0.1985\n",
      "Epoch: 8, Batch(100/245), Loss: 0.2464\n",
      "Epoch: 8, Batch(120/245), Loss: 0.2131\n",
      "Epoch: 8, Batch(140/245), Loss: 0.2604\n",
      "Epoch: 8, Batch(160/245), Loss: 0.2458\n",
      "Epoch: 8, Batch(180/245), Loss: 0.2287\n",
      "Epoch: 8, Batch(200/245), Loss: 0.1699\n",
      "Epoch: 8, Batch(220/245), Loss: 0.2912\n",
      "Epoch: 8, Batch(240/245), Loss: 0.2330\n",
      "epoch 8 duration 1.677 train_loss 0.219 val_loss 0.304 val_acc 0.9262\n",
      "Epoch: 9, Batch(20/245), Loss: 0.2036\n",
      "Epoch: 9, Batch(40/245), Loss: 0.2430\n",
      "Epoch: 9, Batch(60/245), Loss: 0.1742\n",
      "Epoch: 9, Batch(80/245), Loss: 0.1831\n",
      "Epoch: 9, Batch(100/245), Loss: 0.2829\n",
      "Epoch: 9, Batch(120/245), Loss: 0.1935\n",
      "Epoch: 9, Batch(140/245), Loss: 0.1831\n",
      "Epoch: 9, Batch(160/245), Loss: 0.2173\n",
      "Epoch: 9, Batch(180/245), Loss: 0.2032\n",
      "Epoch: 9, Batch(200/245), Loss: 0.2149\n",
      "Epoch: 9, Batch(220/245), Loss: 0.2710\n",
      "Epoch: 9, Batch(240/245), Loss: 0.2413\n",
      "epoch 9 duration 1.672 train_loss 0.216 val_loss 0.237 val_acc 0.9440\n",
      "Epoch: 10, Batch(20/245), Loss: 0.1635\n",
      "Epoch: 10, Batch(40/245), Loss: 0.2326\n",
      "Epoch: 10, Batch(60/245), Loss: 0.1565\n",
      "Epoch: 10, Batch(80/245), Loss: 0.1709\n",
      "Epoch: 10, Batch(100/245), Loss: 0.2058\n",
      "Epoch: 10, Batch(120/245), Loss: 0.1574\n",
      "Epoch: 10, Batch(140/245), Loss: 0.1287\n",
      "Epoch: 10, Batch(160/245), Loss: 0.1979\n",
      "Epoch: 10, Batch(180/245), Loss: 0.2595\n",
      "Epoch: 10, Batch(200/245), Loss: 0.2300\n",
      "Epoch: 10, Batch(220/245), Loss: 0.2252\n",
      "Epoch: 10, Batch(240/245), Loss: 0.2161\n",
      "epoch 10 duration 1.673 train_loss 0.193 val_loss 0.266 val_acc 0.9357\n",
      "Epoch: 11, Batch(20/245), Loss: 0.1883\n",
      "Epoch: 11, Batch(40/245), Loss: 0.1977\n",
      "Epoch: 11, Batch(60/245), Loss: 0.1654\n",
      "Epoch: 11, Batch(80/245), Loss: 0.1569\n",
      "Epoch: 11, Batch(100/245), Loss: 0.1448\n",
      "Epoch: 11, Batch(120/245), Loss: 0.1616\n",
      "Epoch: 11, Batch(140/245), Loss: 0.1092\n",
      "Epoch: 11, Batch(160/245), Loss: 0.1694\n",
      "Epoch: 11, Batch(180/245), Loss: 0.1327\n",
      "Epoch: 11, Batch(200/245), Loss: 0.2259\n",
      "Epoch: 11, Batch(220/245), Loss: 0.1474\n",
      "Epoch: 11, Batch(240/245), Loss: 0.1435\n",
      "epoch 11 duration 1.675 train_loss 0.163 val_loss 0.319 val_acc 0.9095\n",
      "Epoch: 12, Batch(20/245), Loss: 0.1751\n",
      "Epoch: 12, Batch(40/245), Loss: 0.0982\n",
      "Epoch: 12, Batch(60/245), Loss: 0.1471\n",
      "Epoch: 12, Batch(80/245), Loss: 0.1139\n",
      "Epoch: 12, Batch(100/245), Loss: 0.1173\n",
      "Epoch: 12, Batch(120/245), Loss: 0.1730\n",
      "Epoch: 12, Batch(140/245), Loss: 0.1566\n",
      "Epoch: 12, Batch(160/245), Loss: 0.1497\n",
      "Epoch: 12, Batch(180/245), Loss: 0.1841\n",
      "Epoch: 12, Batch(200/245), Loss: 0.1648\n",
      "Epoch: 12, Batch(220/245), Loss: 0.1445\n",
      "Epoch: 12, Batch(240/245), Loss: 0.2116\n",
      "epoch 12 duration 1.674 train_loss 0.153 val_loss 0.287 val_acc 0.9179\n",
      "Epoch: 13, Batch(20/245), Loss: 0.1596\n",
      "Epoch: 13, Batch(40/245), Loss: 0.1749\n",
      "Epoch: 13, Batch(60/245), Loss: 0.0960\n",
      "Epoch: 13, Batch(80/245), Loss: 0.1760\n",
      "Epoch: 13, Batch(100/245), Loss: 0.1198\n",
      "Epoch: 13, Batch(120/245), Loss: 0.1880\n",
      "Epoch: 13, Batch(140/245), Loss: 0.1396\n",
      "Epoch: 13, Batch(160/245), Loss: 0.1477\n",
      "Epoch: 13, Batch(180/245), Loss: 0.1753\n",
      "Epoch: 13, Batch(200/245), Loss: 0.2309\n",
      "Epoch: 13, Batch(220/245), Loss: 0.1661\n",
      "Epoch: 13, Batch(240/245), Loss: 0.2092\n",
      "epoch 13 duration 1.672 train_loss 0.169 val_loss 0.597 val_acc 0.8369\n",
      "Epoch: 14, Batch(20/245), Loss: 0.1602\n",
      "Epoch: 14, Batch(40/245), Loss: 0.1587\n",
      "Epoch: 14, Batch(60/245), Loss: 0.1261\n",
      "Epoch: 14, Batch(80/245), Loss: 0.1477\n",
      "Epoch: 14, Batch(100/245), Loss: 0.1580\n",
      "Epoch: 14, Batch(120/245), Loss: 0.1497\n",
      "Epoch: 14, Batch(140/245), Loss: 0.1203\n",
      "Epoch: 14, Batch(160/245), Loss: 0.1521\n",
      "Epoch: 14, Batch(180/245), Loss: 0.1697\n",
      "Epoch: 14, Batch(200/245), Loss: 0.1770\n",
      "Epoch: 14, Batch(220/245), Loss: 0.1410\n",
      "Epoch: 14, Batch(240/245), Loss: 0.1761\n",
      "epoch 14 duration 1.675 train_loss 0.157 val_loss 0.332 val_acc 0.9048\n",
      "Epoch: 15, Batch(20/245), Loss: 0.1350\n",
      "Epoch: 15, Batch(40/245), Loss: 0.1199\n",
      "Epoch: 15, Batch(60/245), Loss: 0.1362\n",
      "Epoch: 15, Batch(80/245), Loss: 0.1652\n",
      "Epoch: 15, Batch(100/245), Loss: 0.1477\n",
      "Epoch: 15, Batch(120/245), Loss: 0.1751\n",
      "Epoch: 15, Batch(140/245), Loss: 0.1935\n",
      "Epoch: 15, Batch(160/245), Loss: 0.1744\n",
      "Epoch: 15, Batch(180/245), Loss: 0.1368\n",
      "Epoch: 15, Batch(200/245), Loss: 0.1132\n",
      "Epoch: 15, Batch(220/245), Loss: 0.1559\n",
      "Epoch: 15, Batch(240/245), Loss: 0.1358\n",
      "epoch 15 duration 1.674 train_loss 0.157 val_loss 0.306 val_acc 0.9060\n",
      "Epoch: 16, Batch(20/245), Loss: 0.1006\n",
      "Epoch: 16, Batch(40/245), Loss: 0.1146\n",
      "Epoch: 16, Batch(60/245), Loss: 0.1227\n",
      "Epoch: 16, Batch(80/245), Loss: 0.1244\n",
      "Epoch: 16, Batch(100/245), Loss: 0.0915\n",
      "Epoch: 16, Batch(120/245), Loss: 0.0733\n",
      "Epoch: 16, Batch(140/245), Loss: 0.1561\n",
      "Epoch: 16, Batch(160/245), Loss: 0.1272\n",
      "Epoch: 16, Batch(180/245), Loss: 0.1950\n",
      "Epoch: 16, Batch(200/245), Loss: 0.1524\n",
      "Epoch: 16, Batch(220/245), Loss: 0.1876\n",
      "Epoch: 16, Batch(240/245), Loss: 0.1830\n",
      "epoch 16 duration 1.674 train_loss 0.136 val_loss 0.267 val_acc 0.9238\n",
      "Epoch: 17, Batch(20/245), Loss: 0.1413\n",
      "Epoch: 17, Batch(40/245), Loss: 0.1036\n",
      "Epoch: 17, Batch(60/245), Loss: 0.1615\n",
      "Epoch: 17, Batch(80/245), Loss: 0.1473\n",
      "Epoch: 17, Batch(100/245), Loss: 0.1398\n",
      "Epoch: 17, Batch(120/245), Loss: 0.1303\n",
      "Epoch: 17, Batch(140/245), Loss: 0.1481\n",
      "Epoch: 17, Batch(160/245), Loss: 0.1316\n",
      "Epoch: 17, Batch(180/245), Loss: 0.1617\n",
      "Epoch: 17, Batch(200/245), Loss: 0.1772\n",
      "Epoch: 17, Batch(220/245), Loss: 0.1412\n",
      "Epoch: 17, Batch(240/245), Loss: 0.1363\n",
      "epoch 17 duration 1.674 train_loss 0.144 val_loss 0.284 val_acc 0.9262\n",
      "Epoch: 18, Batch(20/245), Loss: 0.1072\n",
      "Epoch: 18, Batch(40/245), Loss: 0.1248\n",
      "Epoch: 18, Batch(60/245), Loss: 0.1877\n",
      "Epoch: 18, Batch(80/245), Loss: 0.1168\n",
      "Epoch: 18, Batch(100/245), Loss: 0.1560\n",
      "Epoch: 18, Batch(120/245), Loss: 0.1623\n",
      "Epoch: 18, Batch(140/245), Loss: 0.1388\n",
      "Epoch: 18, Batch(160/245), Loss: 0.1173\n",
      "Epoch: 18, Batch(180/245), Loss: 0.1372\n",
      "Epoch: 18, Batch(200/245), Loss: 0.1104\n",
      "Epoch: 18, Batch(220/245), Loss: 0.1415\n",
      "Epoch: 18, Batch(240/245), Loss: 0.1115\n",
      "epoch 18 duration 1.676 train_loss 0.135 val_loss 0.236 val_acc 0.9405\n",
      "Epoch: 19, Batch(20/245), Loss: 0.1254\n",
      "Epoch: 19, Batch(40/245), Loss: 0.1453\n",
      "Epoch: 19, Batch(60/245), Loss: 0.1344\n",
      "Epoch: 19, Batch(80/245), Loss: 0.1184\n",
      "Epoch: 19, Batch(100/245), Loss: 0.1319\n",
      "Epoch: 19, Batch(120/245), Loss: 0.1206\n",
      "Epoch: 19, Batch(140/245), Loss: 0.1289\n",
      "Epoch: 19, Batch(160/245), Loss: 0.0914\n",
      "Epoch: 19, Batch(180/245), Loss: 0.1218\n",
      "Epoch: 19, Batch(200/245), Loss: 0.1157\n",
      "Epoch: 19, Batch(220/245), Loss: 0.1006\n",
      "Epoch: 19, Batch(240/245), Loss: 0.1179\n",
      "epoch 19 duration 1.677 train_loss 0.121 val_loss 0.263 val_acc 0.9298\n",
      "Epoch: 20, Batch(20/245), Loss: 0.1295\n",
      "Epoch: 20, Batch(40/245), Loss: 0.1052\n",
      "Epoch: 20, Batch(60/245), Loss: 0.1429\n",
      "Epoch: 20, Batch(80/245), Loss: 0.1559\n",
      "Epoch: 20, Batch(100/245), Loss: 0.1083\n",
      "Epoch: 20, Batch(120/245), Loss: 0.1146\n",
      "Epoch: 20, Batch(140/245), Loss: 0.1063\n",
      "Epoch: 20, Batch(160/245), Loss: 0.1009\n",
      "Epoch: 20, Batch(180/245), Loss: 0.1206\n",
      "Epoch: 20, Batch(200/245), Loss: 0.1318\n",
      "Epoch: 20, Batch(220/245), Loss: 0.0954\n",
      "Epoch: 20, Batch(240/245), Loss: 0.1817\n",
      "epoch 20 duration 1.675 train_loss 0.126 val_loss 0.229 val_acc 0.9393\n",
      "Epoch: 21, Batch(20/245), Loss: 0.1439\n",
      "Epoch: 21, Batch(40/245), Loss: 0.1492\n",
      "Epoch: 21, Batch(60/245), Loss: 0.1272\n",
      "Epoch: 21, Batch(80/245), Loss: 0.1420\n",
      "Epoch: 21, Batch(100/245), Loss: 0.1623\n",
      "Epoch: 21, Batch(120/245), Loss: 0.0996\n",
      "Epoch: 21, Batch(140/245), Loss: 0.1254\n",
      "Epoch: 21, Batch(160/245), Loss: 0.1045\n",
      "Epoch: 21, Batch(180/245), Loss: 0.1365\n",
      "Epoch: 21, Batch(200/245), Loss: 0.1061\n",
      "Epoch: 21, Batch(220/245), Loss: 0.1468\n",
      "Epoch: 21, Batch(240/245), Loss: 0.1620\n",
      "epoch 21 duration 1.673 train_loss 0.134 val_loss 0.216 val_acc 0.9476\n",
      "Epoch: 22, Batch(20/245), Loss: 0.2006\n",
      "Epoch: 22, Batch(40/245), Loss: 0.1606\n",
      "Epoch: 22, Batch(60/245), Loss: 0.1114\n",
      "Epoch: 22, Batch(80/245), Loss: 0.1388\n",
      "Epoch: 22, Batch(100/245), Loss: 0.1913\n",
      "Epoch: 22, Batch(120/245), Loss: 0.1260\n",
      "Epoch: 22, Batch(140/245), Loss: 0.1421\n",
      "Epoch: 22, Batch(160/245), Loss: 0.1125\n",
      "Epoch: 22, Batch(180/245), Loss: 0.1076\n",
      "Epoch: 22, Batch(200/245), Loss: 0.1177\n",
      "Epoch: 22, Batch(220/245), Loss: 0.1419\n",
      "Epoch: 22, Batch(240/245), Loss: 0.1030\n",
      "epoch 22 duration 1.670 train_loss 0.138 val_loss 0.245 val_acc 0.9429\n",
      "Epoch: 23, Batch(20/245), Loss: 0.1558\n",
      "Epoch: 23, Batch(40/245), Loss: 0.0758\n",
      "Epoch: 23, Batch(60/245), Loss: 0.0908\n",
      "Epoch: 23, Batch(80/245), Loss: 0.1035\n",
      "Epoch: 23, Batch(100/245), Loss: 0.1122\n",
      "Epoch: 23, Batch(120/245), Loss: 0.1060\n",
      "Epoch: 23, Batch(140/245), Loss: 0.1560\n",
      "Epoch: 23, Batch(160/245), Loss: 0.1444\n",
      "Epoch: 23, Batch(180/245), Loss: 0.0814\n",
      "Epoch: 23, Batch(200/245), Loss: 0.1802\n",
      "Epoch: 23, Batch(220/245), Loss: 0.1559\n",
      "Epoch: 23, Batch(240/245), Loss: 0.1244\n",
      "epoch 23 duration 1.676 train_loss 0.123 val_loss 0.230 val_acc 0.9369\n",
      "Epoch: 24, Batch(20/245), Loss: 0.1403\n",
      "Epoch: 24, Batch(40/245), Loss: 0.1091\n",
      "Epoch: 24, Batch(60/245), Loss: 0.1291\n",
      "Epoch: 24, Batch(80/245), Loss: 0.1353\n",
      "Epoch: 24, Batch(100/245), Loss: 0.1282\n",
      "Epoch: 24, Batch(120/245), Loss: 0.0921\n",
      "Epoch: 24, Batch(140/245), Loss: 0.0770\n",
      "Epoch: 24, Batch(160/245), Loss: 0.1240\n",
      "Epoch: 24, Batch(180/245), Loss: 0.1547\n",
      "Epoch: 24, Batch(200/245), Loss: 0.2092\n",
      "Epoch: 24, Batch(220/245), Loss: 0.1654\n",
      "Epoch: 24, Batch(240/245), Loss: 0.1353\n",
      "epoch 24 duration 1.674 train_loss 0.134 val_loss 0.509 val_acc 0.8667\n",
      "Epoch: 25, Batch(20/245), Loss: 0.1323\n",
      "Epoch: 25, Batch(40/245), Loss: 0.1624\n",
      "Epoch: 25, Batch(60/245), Loss: 0.1609\n",
      "Epoch: 25, Batch(80/245), Loss: 0.1148\n",
      "Epoch: 25, Batch(100/245), Loss: 0.1360\n",
      "Epoch: 25, Batch(120/245), Loss: 0.1451\n",
      "Epoch: 25, Batch(140/245), Loss: 0.1348\n",
      "Epoch: 25, Batch(160/245), Loss: 0.1337\n",
      "Epoch: 25, Batch(180/245), Loss: 0.0841\n",
      "Epoch: 25, Batch(200/245), Loss: 0.1601\n",
      "Epoch: 25, Batch(220/245), Loss: 0.1252\n",
      "Epoch: 25, Batch(240/245), Loss: 0.1702\n",
      "epoch 25 duration 1.674 train_loss 0.138 val_loss 0.261 val_acc 0.9333\n",
      "Epoch: 26, Batch(20/245), Loss: 0.0973\n",
      "Epoch: 26, Batch(40/245), Loss: 0.1151\n",
      "Epoch: 26, Batch(60/245), Loss: 0.1536\n",
      "Epoch: 26, Batch(80/245), Loss: 0.1157\n",
      "Epoch: 26, Batch(100/245), Loss: 0.1054\n",
      "Epoch: 26, Batch(120/245), Loss: 0.0771\n",
      "Epoch: 26, Batch(140/245), Loss: 0.1283\n",
      "Epoch: 26, Batch(160/245), Loss: 0.1172\n",
      "Epoch: 26, Batch(180/245), Loss: 0.1470\n",
      "Epoch: 26, Batch(200/245), Loss: 0.1427\n",
      "Epoch: 26, Batch(220/245), Loss: 0.0981\n",
      "Epoch: 26, Batch(240/245), Loss: 0.1144\n",
      "epoch 26 duration 1.672 train_loss 0.117 val_loss 0.228 val_acc 0.9369\n",
      "Epoch: 27, Batch(20/245), Loss: 0.1021\n",
      "Epoch: 27, Batch(40/245), Loss: 0.0885\n",
      "Epoch: 27, Batch(60/245), Loss: 0.1220\n",
      "Epoch: 27, Batch(80/245), Loss: 0.0651\n",
      "Epoch: 27, Batch(100/245), Loss: 0.0635\n",
      "Epoch: 27, Batch(120/245), Loss: 0.0651\n",
      "Epoch: 27, Batch(140/245), Loss: 0.0985\n",
      "Epoch: 27, Batch(160/245), Loss: 0.1004\n",
      "Epoch: 27, Batch(180/245), Loss: 0.1132\n",
      "Epoch: 27, Batch(200/245), Loss: 0.1258\n",
      "Epoch: 27, Batch(220/245), Loss: 0.0998\n",
      "Epoch: 27, Batch(240/245), Loss: 0.0774\n",
      "epoch 27 duration 1.672 train_loss 0.093 val_loss 0.239 val_acc 0.9345\n",
      "Epoch: 28, Batch(20/245), Loss: 0.0695\n",
      "Epoch: 28, Batch(40/245), Loss: 0.0914\n",
      "Epoch: 28, Batch(60/245), Loss: 0.1466\n",
      "Epoch: 28, Batch(80/245), Loss: 0.1534\n",
      "Epoch: 28, Batch(100/245), Loss: 0.1030\n",
      "Epoch: 28, Batch(120/245), Loss: 0.1114\n",
      "Epoch: 28, Batch(140/245), Loss: 0.0971\n",
      "Epoch: 28, Batch(160/245), Loss: 0.1893\n",
      "Epoch: 28, Batch(180/245), Loss: 0.1508\n",
      "Epoch: 28, Batch(200/245), Loss: 0.1086\n",
      "Epoch: 28, Batch(220/245), Loss: 0.1232\n",
      "Epoch: 28, Batch(240/245), Loss: 0.1890\n",
      "epoch 28 duration 1.673 train_loss 0.130 val_loss 0.267 val_acc 0.9286\n",
      "Epoch: 29, Batch(20/245), Loss: 0.1498\n",
      "Epoch: 29, Batch(40/245), Loss: 0.2030\n",
      "Epoch: 29, Batch(60/245), Loss: 0.1488\n",
      "Epoch: 29, Batch(80/245), Loss: 0.0979\n",
      "Epoch: 29, Batch(100/245), Loss: 0.1114\n",
      "Epoch: 29, Batch(120/245), Loss: 0.1842\n",
      "Epoch: 29, Batch(140/245), Loss: 0.1417\n",
      "Epoch: 29, Batch(160/245), Loss: 0.1456\n",
      "Epoch: 29, Batch(180/245), Loss: 0.1232\n",
      "Epoch: 29, Batch(200/245), Loss: 0.2051\n",
      "Epoch: 29, Batch(220/245), Loss: 0.1389\n",
      "Epoch: 29, Batch(240/245), Loss: 0.1197\n",
      "epoch 29 duration 1.672 train_loss 0.145 val_loss 0.232 val_acc 0.9488\n",
      "Epoch: 30, Batch(20/245), Loss: 0.0923\n",
      "Epoch: 30, Batch(40/245), Loss: 0.1101\n",
      "Epoch: 30, Batch(60/245), Loss: 0.0885\n",
      "Epoch: 30, Batch(80/245), Loss: 0.1195\n",
      "Epoch: 30, Batch(100/245), Loss: 0.0959\n",
      "Epoch: 30, Batch(120/245), Loss: 0.0802\n",
      "Epoch: 30, Batch(140/245), Loss: 0.0861\n",
      "Epoch: 30, Batch(160/245), Loss: 0.0767\n",
      "Epoch: 30, Batch(180/245), Loss: 0.1393\n",
      "Epoch: 30, Batch(200/245), Loss: 0.1149\n",
      "Epoch: 30, Batch(220/245), Loss: 0.0982\n",
      "Epoch: 30, Batch(240/245), Loss: 0.1419\n",
      "epoch 30 duration 1.675 train_loss 0.106 val_loss 0.268 val_acc 0.9405\n",
      "Epoch: 31, Batch(20/245), Loss: 0.1507\n",
      "Epoch: 31, Batch(40/245), Loss: 0.1068\n",
      "Epoch: 31, Batch(60/245), Loss: 0.1337\n",
      "Epoch: 31, Batch(80/245), Loss: 0.1139\n",
      "Epoch: 31, Batch(100/245), Loss: 0.1002\n",
      "Epoch: 31, Batch(120/245), Loss: 0.0981\n",
      "Epoch: 31, Batch(140/245), Loss: 0.1709\n",
      "Epoch: 31, Batch(160/245), Loss: 0.1190\n",
      "Epoch: 31, Batch(180/245), Loss: 0.1750\n",
      "Epoch: 31, Batch(200/245), Loss: 0.1826\n",
      "Epoch: 31, Batch(220/245), Loss: 0.1580\n",
      "Epoch: 31, Batch(240/245), Loss: 0.1739\n",
      "epoch 31 duration 1.675 train_loss 0.142 val_loss 0.375 val_acc 0.9036\n",
      "Epoch: 32, Batch(20/245), Loss: 0.1754\n",
      "Epoch: 32, Batch(40/245), Loss: 0.1585\n",
      "Epoch: 32, Batch(60/245), Loss: 0.1403\n",
      "Epoch: 32, Batch(80/245), Loss: 0.1066\n",
      "Epoch: 32, Batch(100/245), Loss: 0.1156\n",
      "Epoch: 32, Batch(120/245), Loss: 0.1835\n",
      "Epoch: 32, Batch(140/245), Loss: 0.0929\n",
      "Epoch: 32, Batch(160/245), Loss: 0.1339\n",
      "Epoch: 32, Batch(180/245), Loss: 0.1035\n",
      "Epoch: 32, Batch(200/245), Loss: 0.0800\n",
      "Epoch: 32, Batch(220/245), Loss: 0.0941\n",
      "Epoch: 32, Batch(240/245), Loss: 0.1092\n",
      "epoch 32 duration 1.673 train_loss 0.125 val_loss 0.180 val_acc 0.9560\n",
      "Epoch: 33, Batch(20/245), Loss: 0.0675\n",
      "Epoch: 33, Batch(40/245), Loss: 0.1007\n",
      "Epoch: 33, Batch(60/245), Loss: 0.0684\n",
      "Epoch: 33, Batch(80/245), Loss: 0.0696\n",
      "Epoch: 33, Batch(100/245), Loss: 0.0943\n",
      "Epoch: 33, Batch(120/245), Loss: 0.0589\n",
      "Epoch: 33, Batch(140/245), Loss: 0.0458\n",
      "Epoch: 33, Batch(160/245), Loss: 0.0954\n",
      "Epoch: 33, Batch(180/245), Loss: 0.1253\n",
      "Epoch: 33, Batch(200/245), Loss: 0.1491\n",
      "Epoch: 33, Batch(220/245), Loss: 0.1159\n",
      "Epoch: 33, Batch(240/245), Loss: 0.1368\n",
      "epoch 33 duration 1.673 train_loss 0.094 val_loss 0.228 val_acc 0.9417\n",
      "Epoch: 34, Batch(20/245), Loss: 0.0857\n",
      "Epoch: 34, Batch(40/245), Loss: 0.1613\n",
      "Epoch: 34, Batch(60/245), Loss: 0.0895\n",
      "Epoch: 34, Batch(80/245), Loss: 0.1342\n",
      "Epoch: 34, Batch(100/245), Loss: 0.1452\n",
      "Epoch: 34, Batch(120/245), Loss: 0.1296\n",
      "Epoch: 34, Batch(140/245), Loss: 0.0761\n",
      "Epoch: 34, Batch(160/245), Loss: 0.1348\n",
      "Epoch: 34, Batch(180/245), Loss: 0.1331\n",
      "Epoch: 34, Batch(200/245), Loss: 0.1865\n",
      "Epoch: 34, Batch(220/245), Loss: 0.1396\n",
      "Epoch: 34, Batch(240/245), Loss: 0.1109\n",
      "epoch 34 duration 1.673 train_loss 0.126 val_loss 0.218 val_acc 0.9393\n",
      "Epoch: 35, Batch(20/245), Loss: 0.0924\n",
      "Epoch: 35, Batch(40/245), Loss: 0.0963\n",
      "Epoch: 35, Batch(60/245), Loss: 0.0626\n",
      "Epoch: 35, Batch(80/245), Loss: 0.0737\n",
      "Epoch: 35, Batch(100/245), Loss: 0.0801\n",
      "Epoch: 35, Batch(120/245), Loss: 0.0856\n",
      "Epoch: 35, Batch(140/245), Loss: 0.0889\n",
      "Epoch: 35, Batch(160/245), Loss: 0.0489\n",
      "Epoch: 35, Batch(180/245), Loss: 0.0811\n",
      "Epoch: 35, Batch(200/245), Loss: 0.1134\n",
      "Epoch: 35, Batch(220/245), Loss: 0.0902\n",
      "Epoch: 35, Batch(240/245), Loss: 0.0902\n",
      "epoch 35 duration 1.674 train_loss 0.084 val_loss 0.320 val_acc 0.9214\n",
      "Epoch: 36, Batch(20/245), Loss: 0.1111\n",
      "Epoch: 36, Batch(40/245), Loss: 0.0934\n",
      "Epoch: 36, Batch(60/245), Loss: 0.1438\n",
      "Epoch: 36, Batch(80/245), Loss: 0.1671\n",
      "Epoch: 36, Batch(100/245), Loss: 0.1208\n",
      "Epoch: 36, Batch(120/245), Loss: 0.1270\n",
      "Epoch: 36, Batch(140/245), Loss: 0.1551\n",
      "Epoch: 36, Batch(160/245), Loss: 0.0864\n",
      "Epoch: 36, Batch(180/245), Loss: 0.1508\n",
      "Epoch: 36, Batch(200/245), Loss: 0.1157\n",
      "Epoch: 36, Batch(220/245), Loss: 0.1416\n",
      "Epoch: 36, Batch(240/245), Loss: 0.1531\n",
      "epoch 36 duration 1.674 train_loss 0.130 val_loss 0.238 val_acc 0.9381\n",
      "Epoch: 37, Batch(20/245), Loss: 0.1320\n",
      "Epoch: 37, Batch(40/245), Loss: 0.0941\n",
      "Epoch: 37, Batch(60/245), Loss: 0.1038\n",
      "Epoch: 37, Batch(80/245), Loss: 0.0956\n",
      "Epoch: 37, Batch(100/245), Loss: 0.1002\n",
      "Epoch: 37, Batch(120/245), Loss: 0.1337\n",
      "Epoch: 37, Batch(140/245), Loss: 0.0830\n",
      "Epoch: 37, Batch(160/245), Loss: 0.0969\n",
      "Epoch: 37, Batch(180/245), Loss: 0.1842\n",
      "Epoch: 37, Batch(200/245), Loss: 0.1398\n",
      "Epoch: 37, Batch(220/245), Loss: 0.1740\n",
      "Epoch: 37, Batch(240/245), Loss: 0.1515\n",
      "epoch 37 duration 1.673 train_loss 0.123 val_loss 0.258 val_acc 0.9345\n",
      "Epoch: 38, Batch(20/245), Loss: 0.0890\n",
      "Epoch: 38, Batch(40/245), Loss: 0.0777\n",
      "Epoch: 38, Batch(60/245), Loss: 0.0793\n",
      "Epoch: 38, Batch(80/245), Loss: 0.1091\n",
      "Epoch: 38, Batch(100/245), Loss: 0.0827\n",
      "Epoch: 38, Batch(120/245), Loss: 0.0996\n",
      "Epoch: 38, Batch(140/245), Loss: 0.1322\n",
      "Epoch: 38, Batch(160/245), Loss: 0.1554\n",
      "Epoch: 38, Batch(180/245), Loss: 0.1352\n",
      "Epoch: 38, Batch(200/245), Loss: 0.1553\n",
      "Epoch: 38, Batch(220/245), Loss: 0.1887\n",
      "Epoch: 38, Batch(240/245), Loss: 0.1324\n",
      "epoch 38 duration 1.674 train_loss 0.124 val_loss 0.186 val_acc 0.9476\n",
      "Epoch: 39, Batch(20/245), Loss: 0.0958\n",
      "Epoch: 39, Batch(40/245), Loss: 0.1111\n",
      "Epoch: 39, Batch(60/245), Loss: 0.1332\n",
      "Epoch: 39, Batch(80/245), Loss: 0.1794\n",
      "Epoch: 39, Batch(100/245), Loss: 0.1384\n",
      "Epoch: 39, Batch(120/245), Loss: 0.1155\n",
      "Epoch: 39, Batch(140/245), Loss: 0.0838\n",
      "Epoch: 39, Batch(160/245), Loss: 0.2214\n",
      "Epoch: 39, Batch(180/245), Loss: 0.0726\n",
      "Epoch: 39, Batch(200/245), Loss: 0.1536\n",
      "Epoch: 39, Batch(220/245), Loss: 0.1219\n",
      "Epoch: 39, Batch(240/245), Loss: 0.1154\n",
      "epoch 39 duration 1.671 train_loss 0.127 val_loss 0.251 val_acc 0.9381\n",
      "Epoch: 40, Batch(20/245), Loss: 0.1187\n",
      "Epoch: 40, Batch(40/245), Loss: 0.0609\n",
      "Epoch: 40, Batch(60/245), Loss: 0.1212\n",
      "Epoch: 40, Batch(80/245), Loss: 0.0908\n",
      "Epoch: 40, Batch(100/245), Loss: 0.0928\n",
      "Epoch: 40, Batch(120/245), Loss: 0.0884\n",
      "Epoch: 40, Batch(140/245), Loss: 0.0983\n",
      "Epoch: 40, Batch(160/245), Loss: 0.1171\n",
      "Epoch: 40, Batch(180/245), Loss: 0.0772\n",
      "Epoch: 40, Batch(200/245), Loss: 0.1241\n",
      "Epoch: 40, Batch(220/245), Loss: 0.1671\n",
      "Epoch: 40, Batch(240/245), Loss: 0.0842\n",
      "epoch 40 duration 1.672 train_loss 0.102 val_loss 0.234 val_acc 0.9345\n",
      "Epoch: 41, Batch(20/245), Loss: 0.1004\n",
      "Epoch: 41, Batch(40/245), Loss: 0.1043\n",
      "Epoch: 41, Batch(60/245), Loss: 0.1245\n",
      "Epoch: 41, Batch(80/245), Loss: 0.0740\n",
      "Epoch: 41, Batch(100/245), Loss: 0.0650\n",
      "Epoch: 41, Batch(120/245), Loss: 0.0687\n",
      "Epoch: 41, Batch(140/245), Loss: 0.0780\n",
      "Epoch: 41, Batch(160/245), Loss: 0.1419\n",
      "Epoch: 41, Batch(180/245), Loss: 0.1245\n",
      "Epoch: 41, Batch(200/245), Loss: 0.1425\n",
      "Epoch: 41, Batch(220/245), Loss: 0.1019\n",
      "Epoch: 41, Batch(240/245), Loss: 0.1452\n",
      "epoch 41 duration 1.679 train_loss 0.104 val_loss 0.283 val_acc 0.9214\n",
      "Epoch: 42, Batch(20/245), Loss: 0.0789\n",
      "Epoch: 42, Batch(40/245), Loss: 0.0666\n",
      "Epoch: 42, Batch(60/245), Loss: 0.0953\n",
      "Epoch: 42, Batch(80/245), Loss: 0.1011\n",
      "Epoch: 42, Batch(100/245), Loss: 0.0999\n",
      "Epoch: 42, Batch(120/245), Loss: 0.1086\n",
      "Epoch: 42, Batch(140/245), Loss: 0.1305\n",
      "Epoch: 42, Batch(160/245), Loss: 0.1190\n",
      "Epoch: 42, Batch(180/245), Loss: 0.1017\n",
      "Epoch: 42, Batch(200/245), Loss: 0.1704\n",
      "Epoch: 42, Batch(220/245), Loss: 0.1103\n",
      "Epoch: 42, Batch(240/245), Loss: 0.0660\n",
      "epoch 42 duration 1.672 train_loss 0.104 val_loss 0.233 val_acc 0.9310\n",
      "Epoch: 43, Batch(20/245), Loss: 0.0846\n",
      "Epoch: 43, Batch(40/245), Loss: 0.1594\n",
      "Epoch: 43, Batch(60/245), Loss: 0.1709\n",
      "Epoch: 43, Batch(80/245), Loss: 0.1301\n",
      "Epoch: 43, Batch(100/245), Loss: 0.1216\n",
      "Epoch: 43, Batch(120/245), Loss: 0.0910\n",
      "Epoch: 43, Batch(140/245), Loss: 0.0799\n",
      "Epoch: 43, Batch(160/245), Loss: 0.1410\n",
      "Epoch: 43, Batch(180/245), Loss: 0.1281\n",
      "Epoch: 43, Batch(200/245), Loss: 0.0483\n",
      "Epoch: 43, Batch(220/245), Loss: 0.1415\n",
      "Epoch: 43, Batch(240/245), Loss: 0.1251\n",
      "epoch 43 duration 1.674 train_loss 0.118 val_loss 0.176 val_acc 0.9536\n",
      "Epoch: 44, Batch(20/245), Loss: 0.0831\n",
      "Epoch: 44, Batch(40/245), Loss: 0.1376\n",
      "Epoch: 44, Batch(60/245), Loss: 0.1169\n",
      "Epoch: 44, Batch(80/245), Loss: 0.0839\n",
      "Epoch: 44, Batch(100/245), Loss: 0.0809\n",
      "Epoch: 44, Batch(120/245), Loss: 0.0669\n",
      "Epoch: 44, Batch(140/245), Loss: 0.0775\n",
      "Epoch: 44, Batch(160/245), Loss: 0.1222\n",
      "Epoch: 44, Batch(180/245), Loss: 0.1789\n",
      "Epoch: 44, Batch(200/245), Loss: 0.1975\n",
      "Epoch: 44, Batch(220/245), Loss: 0.1197\n",
      "Epoch: 44, Batch(240/245), Loss: 0.1204\n",
      "epoch 44 duration 1.674 train_loss 0.114 val_loss 0.280 val_acc 0.9238\n",
      "Epoch: 45, Batch(20/245), Loss: 0.0921\n",
      "Epoch: 45, Batch(40/245), Loss: 0.0949\n",
      "Epoch: 45, Batch(60/245), Loss: 0.2233\n",
      "Epoch: 45, Batch(80/245), Loss: 0.1483\n",
      "Epoch: 45, Batch(100/245), Loss: 0.1055\n",
      "Epoch: 45, Batch(120/245), Loss: 0.1052\n",
      "Epoch: 45, Batch(140/245), Loss: 0.0795\n",
      "Epoch: 45, Batch(160/245), Loss: 0.1142\n",
      "Epoch: 45, Batch(180/245), Loss: 0.0745\n",
      "Epoch: 45, Batch(200/245), Loss: 0.0809\n",
      "Epoch: 45, Batch(220/245), Loss: 0.0857\n",
      "Epoch: 45, Batch(240/245), Loss: 0.1537\n",
      "epoch 45 duration 1.674 train_loss 0.120 val_loss 0.400 val_acc 0.9012\n",
      "Epoch: 46, Batch(20/245), Loss: 0.0926\n",
      "Epoch: 46, Batch(40/245), Loss: 0.1184\n",
      "Epoch: 46, Batch(60/245), Loss: 0.1203\n",
      "Epoch: 46, Batch(80/245), Loss: 0.0715\n",
      "Epoch: 46, Batch(100/245), Loss: 0.0936\n",
      "Epoch: 46, Batch(120/245), Loss: 0.0982\n",
      "Epoch: 46, Batch(140/245), Loss: 0.0773\n",
      "Epoch: 46, Batch(160/245), Loss: 0.1055\n",
      "Epoch: 46, Batch(180/245), Loss: 0.1061\n",
      "Epoch: 46, Batch(200/245), Loss: 0.0872\n",
      "Epoch: 46, Batch(220/245), Loss: 0.0975\n",
      "Epoch: 46, Batch(240/245), Loss: 0.1128\n",
      "epoch 46 duration 1.674 train_loss 0.099 val_loss 0.291 val_acc 0.9250\n",
      "Epoch: 47, Batch(20/245), Loss: 0.0576\n",
      "Epoch: 47, Batch(40/245), Loss: 0.0691\n",
      "Epoch: 47, Batch(60/245), Loss: 0.0921\n",
      "Epoch: 47, Batch(80/245), Loss: 0.0967\n",
      "Epoch: 47, Batch(100/245), Loss: 0.0969\n",
      "Epoch: 47, Batch(120/245), Loss: 0.0809\n",
      "Epoch: 47, Batch(140/245), Loss: 0.1670\n",
      "Epoch: 47, Batch(160/245), Loss: 0.1099\n",
      "Epoch: 47, Batch(180/245), Loss: 0.1419\n",
      "Epoch: 47, Batch(200/245), Loss: 0.0832\n",
      "Epoch: 47, Batch(220/245), Loss: 0.1262\n",
      "Epoch: 47, Batch(240/245), Loss: 0.0936\n",
      "epoch 47 duration 1.674 train_loss 0.101 val_loss 0.198 val_acc 0.9405\n",
      "Epoch: 48, Batch(20/245), Loss: 0.0884\n",
      "Epoch: 48, Batch(40/245), Loss: 0.1301\n",
      "Epoch: 48, Batch(60/245), Loss: 0.1660\n",
      "Epoch: 48, Batch(80/245), Loss: 0.1644\n",
      "Epoch: 48, Batch(100/245), Loss: 0.0984\n",
      "Epoch: 48, Batch(120/245), Loss: 0.0908\n",
      "Epoch: 48, Batch(140/245), Loss: 0.0436\n",
      "Epoch: 48, Batch(160/245), Loss: 0.1013\n",
      "Epoch: 48, Batch(180/245), Loss: 0.0693\n",
      "Epoch: 48, Batch(200/245), Loss: 0.0690\n",
      "Epoch: 48, Batch(220/245), Loss: 0.1223\n",
      "Epoch: 48, Batch(240/245), Loss: 0.0716\n",
      "epoch 48 duration 1.673 train_loss 0.101 val_loss 0.170 val_acc 0.9488\n",
      "Epoch: 49, Batch(20/245), Loss: 0.0864\n",
      "Epoch: 49, Batch(40/245), Loss: 0.0677\n",
      "Epoch: 49, Batch(60/245), Loss: 0.0743\n",
      "Epoch: 49, Batch(80/245), Loss: 0.0814\n",
      "Epoch: 49, Batch(100/245), Loss: 0.0494\n",
      "Epoch: 49, Batch(120/245), Loss: 0.0661\n",
      "Epoch: 49, Batch(140/245), Loss: 0.0870\n",
      "Epoch: 49, Batch(160/245), Loss: 0.1312\n",
      "Epoch: 49, Batch(180/245), Loss: 0.0976\n",
      "Epoch: 49, Batch(200/245), Loss: 0.0812\n",
      "Epoch: 49, Batch(220/245), Loss: 0.1116\n",
      "Epoch: 49, Batch(240/245), Loss: 0.1740\n",
      "epoch 49 duration 1.675 train_loss 0.094 val_loss 0.195 val_acc 0.9393\n",
      "Epoch: 50, Batch(20/245), Loss: 0.1033\n",
      "Epoch: 50, Batch(40/245), Loss: 0.1195\n",
      "Epoch: 50, Batch(60/245), Loss: 0.0834\n",
      "Epoch: 50, Batch(80/245), Loss: 0.0738\n",
      "Epoch: 50, Batch(100/245), Loss: 0.1022\n",
      "Epoch: 50, Batch(120/245), Loss: 0.0835\n",
      "Epoch: 50, Batch(140/245), Loss: 0.0562\n",
      "Epoch: 50, Batch(160/245), Loss: 0.0741\n",
      "Epoch: 50, Batch(180/245), Loss: 0.0520\n",
      "Epoch: 50, Batch(200/245), Loss: 0.0554\n",
      "Epoch: 50, Batch(220/245), Loss: 0.1109\n",
      "Epoch: 50, Batch(240/245), Loss: 0.0476\n",
      "epoch 50 duration 1.673 train_loss 0.080 val_loss 0.176 val_acc 0.9536\n",
      "Epoch: 51, Batch(20/245), Loss: 0.0608\n",
      "Epoch: 51, Batch(40/245), Loss: 0.0720\n",
      "Epoch: 51, Batch(60/245), Loss: 0.0601\n",
      "Epoch: 51, Batch(80/245), Loss: 0.0801\n",
      "Epoch: 51, Batch(100/245), Loss: 0.0926\n",
      "Epoch: 51, Batch(120/245), Loss: 0.0274\n",
      "Epoch: 51, Batch(140/245), Loss: 0.0377\n",
      "Epoch: 51, Batch(160/245), Loss: 0.0290\n",
      "Epoch: 51, Batch(180/245), Loss: 0.0780\n",
      "Epoch: 51, Batch(200/245), Loss: 0.0545\n",
      "Epoch: 51, Batch(220/245), Loss: 0.0709\n",
      "Epoch: 51, Batch(240/245), Loss: 0.0475\n",
      "epoch 51 duration 1.675 train_loss 0.060 val_loss 0.177 val_acc 0.9536\n",
      "Epoch: 52, Batch(20/245), Loss: 0.0399\n",
      "Epoch: 52, Batch(40/245), Loss: 0.0517\n",
      "Epoch: 52, Batch(60/245), Loss: 0.0478\n",
      "Epoch: 52, Batch(80/245), Loss: 0.0591\n",
      "Epoch: 52, Batch(100/245), Loss: 0.0802\n",
      "Epoch: 52, Batch(120/245), Loss: 0.0315\n",
      "Epoch: 52, Batch(140/245), Loss: 0.0488\n",
      "Epoch: 52, Batch(160/245), Loss: 0.0478\n",
      "Epoch: 52, Batch(180/245), Loss: 0.0321\n",
      "Epoch: 52, Batch(200/245), Loss: 0.0701\n",
      "Epoch: 52, Batch(220/245), Loss: 0.0352\n",
      "Epoch: 52, Batch(240/245), Loss: 0.0339\n",
      "epoch 52 duration 1.676 train_loss 0.048 val_loss 0.170 val_acc 0.9560\n",
      "Epoch: 53, Batch(20/245), Loss: 0.0252\n",
      "Epoch: 53, Batch(40/245), Loss: 0.0753\n",
      "Epoch: 53, Batch(60/245), Loss: 0.0583\n",
      "Epoch: 53, Batch(80/245), Loss: 0.0379\n",
      "Epoch: 53, Batch(100/245), Loss: 0.0442\n",
      "Epoch: 53, Batch(120/245), Loss: 0.0576\n",
      "Epoch: 53, Batch(140/245), Loss: 0.0380\n",
      "Epoch: 53, Batch(160/245), Loss: 0.0839\n",
      "Epoch: 53, Batch(180/245), Loss: 0.0607\n",
      "Epoch: 53, Batch(200/245), Loss: 0.0331\n",
      "Epoch: 53, Batch(220/245), Loss: 0.0401\n",
      "Epoch: 53, Batch(240/245), Loss: 0.0512\n",
      "epoch 53 duration 1.679 train_loss 0.050 val_loss 0.171 val_acc 0.9524\n",
      "Epoch: 54, Batch(20/245), Loss: 0.0374\n",
      "Epoch: 54, Batch(40/245), Loss: 0.0672\n",
      "Epoch: 54, Batch(60/245), Loss: 0.0350\n",
      "Epoch: 54, Batch(80/245), Loss: 0.0322\n",
      "Epoch: 54, Batch(100/245), Loss: 0.0445\n",
      "Epoch: 54, Batch(120/245), Loss: 0.0711\n",
      "Epoch: 54, Batch(140/245), Loss: 0.0417\n",
      "Epoch: 54, Batch(160/245), Loss: 0.0393\n",
      "Epoch: 54, Batch(180/245), Loss: 0.0445\n",
      "Epoch: 54, Batch(200/245), Loss: 0.0390\n",
      "Epoch: 54, Batch(220/245), Loss: 0.0632\n",
      "Epoch: 54, Batch(240/245), Loss: 0.0430\n",
      "epoch 54 duration 1.679 train_loss 0.046 val_loss 0.161 val_acc 0.9524\n",
      "Epoch: 55, Batch(20/245), Loss: 0.0348\n",
      "Epoch: 55, Batch(40/245), Loss: 0.0376\n",
      "Epoch: 55, Batch(60/245), Loss: 0.0250\n",
      "Epoch: 55, Batch(80/245), Loss: 0.0413\n",
      "Epoch: 55, Batch(100/245), Loss: 0.0247\n",
      "Epoch: 55, Batch(120/245), Loss: 0.0409\n",
      "Epoch: 55, Batch(140/245), Loss: 0.0590\n",
      "Epoch: 55, Batch(160/245), Loss: 0.0424\n",
      "Epoch: 55, Batch(180/245), Loss: 0.0403\n",
      "Epoch: 55, Batch(200/245), Loss: 0.0477\n",
      "Epoch: 55, Batch(220/245), Loss: 0.0422\n",
      "Epoch: 55, Batch(240/245), Loss: 0.0336\n",
      "epoch 55 duration 1.677 train_loss 0.039 val_loss 0.172 val_acc 0.9548\n",
      "Epoch: 56, Batch(20/245), Loss: 0.0336\n",
      "Epoch: 56, Batch(40/245), Loss: 0.0266\n",
      "Epoch: 56, Batch(60/245), Loss: 0.0406\n",
      "Epoch: 56, Batch(80/245), Loss: 0.0567\n",
      "Epoch: 56, Batch(100/245), Loss: 0.0383\n",
      "Epoch: 56, Batch(120/245), Loss: 0.0486\n",
      "Epoch: 56, Batch(140/245), Loss: 0.0378\n",
      "Epoch: 56, Batch(160/245), Loss: 0.0353\n",
      "Epoch: 56, Batch(180/245), Loss: 0.0417\n",
      "Epoch: 56, Batch(200/245), Loss: 0.0264\n",
      "Epoch: 56, Batch(220/245), Loss: 0.0376\n",
      "Epoch: 56, Batch(240/245), Loss: 0.0341\n",
      "epoch 56 duration 1.672 train_loss 0.038 val_loss 0.184 val_acc 0.9536\n",
      "Epoch: 57, Batch(20/245), Loss: 0.0346\n",
      "Epoch: 57, Batch(40/245), Loss: 0.0496\n",
      "Epoch: 57, Batch(60/245), Loss: 0.0546\n",
      "Epoch: 57, Batch(80/245), Loss: 0.0531\n",
      "Epoch: 57, Batch(100/245), Loss: 0.0321\n",
      "Epoch: 57, Batch(120/245), Loss: 0.0423\n",
      "Epoch: 57, Batch(140/245), Loss: 0.0426\n",
      "Epoch: 57, Batch(160/245), Loss: 0.0267\n",
      "Epoch: 57, Batch(180/245), Loss: 0.0404\n",
      "Epoch: 57, Batch(200/245), Loss: 0.0380\n",
      "Epoch: 57, Batch(220/245), Loss: 0.0239\n",
      "Epoch: 57, Batch(240/245), Loss: 0.0353\n",
      "epoch 57 duration 1.678 train_loss 0.040 val_loss 0.181 val_acc 0.9512\n",
      "Epoch: 58, Batch(20/245), Loss: 0.0594\n",
      "Epoch: 58, Batch(40/245), Loss: 0.0454\n",
      "Epoch: 58, Batch(60/245), Loss: 0.0398\n",
      "Epoch: 58, Batch(80/245), Loss: 0.0373\n",
      "Epoch: 58, Batch(100/245), Loss: 0.0331\n",
      "Epoch: 58, Batch(120/245), Loss: 0.0305\n",
      "Epoch: 58, Batch(140/245), Loss: 0.0284\n",
      "Epoch: 58, Batch(160/245), Loss: 0.0656\n",
      "Epoch: 58, Batch(180/245), Loss: 0.0549\n",
      "Epoch: 58, Batch(200/245), Loss: 0.0527\n",
      "Epoch: 58, Batch(220/245), Loss: 0.0512\n",
      "Epoch: 58, Batch(240/245), Loss: 0.0444\n",
      "epoch 58 duration 1.677 train_loss 0.046 val_loss 0.181 val_acc 0.9548\n",
      "Epoch: 59, Batch(20/245), Loss: 0.0279\n",
      "Epoch: 59, Batch(40/245), Loss: 0.0206\n",
      "Epoch: 59, Batch(60/245), Loss: 0.0564\n",
      "Epoch: 59, Batch(80/245), Loss: 0.0198\n",
      "Epoch: 59, Batch(100/245), Loss: 0.0617\n",
      "Epoch: 59, Batch(120/245), Loss: 0.0300\n",
      "Epoch: 59, Batch(140/245), Loss: 0.0597\n",
      "Epoch: 59, Batch(160/245), Loss: 0.0298\n",
      "Epoch: 59, Batch(180/245), Loss: 0.0434\n",
      "Epoch: 59, Batch(200/245), Loss: 0.0443\n",
      "Epoch: 59, Batch(220/245), Loss: 0.0765\n",
      "Epoch: 59, Batch(240/245), Loss: 0.0355\n",
      "epoch 59 duration 1.674 train_loss 0.043 val_loss 0.191 val_acc 0.9476\n",
      "Epoch: 60, Batch(20/245), Loss: 0.0311\n",
      "Epoch: 60, Batch(40/245), Loss: 0.0292\n",
      "Epoch: 60, Batch(60/245), Loss: 0.0381\n",
      "Epoch: 60, Batch(80/245), Loss: 0.0237\n",
      "Epoch: 60, Batch(100/245), Loss: 0.0304\n",
      "Epoch: 60, Batch(120/245), Loss: 0.0358\n",
      "Epoch: 60, Batch(140/245), Loss: 0.0293\n",
      "Epoch: 60, Batch(160/245), Loss: 0.0606\n",
      "Epoch: 60, Batch(180/245), Loss: 0.0396\n",
      "Epoch: 60, Batch(200/245), Loss: 0.0720\n",
      "Epoch: 60, Batch(220/245), Loss: 0.0374\n",
      "Epoch: 60, Batch(240/245), Loss: 0.0587\n",
      "epoch 60 duration 1.681 train_loss 0.041 val_loss 0.186 val_acc 0.9476\n",
      "Epoch: 61, Batch(20/245), Loss: 0.0320\n",
      "Epoch: 61, Batch(40/245), Loss: 0.0423\n",
      "Epoch: 61, Batch(60/245), Loss: 0.0415\n",
      "Epoch: 61, Batch(80/245), Loss: 0.0323\n",
      "Epoch: 61, Batch(100/245), Loss: 0.0222\n",
      "Epoch: 61, Batch(120/245), Loss: 0.0322\n",
      "Epoch: 61, Batch(140/245), Loss: 0.0282\n",
      "Epoch: 61, Batch(160/245), Loss: 0.0372\n",
      "Epoch: 61, Batch(180/245), Loss: 0.0515\n",
      "Epoch: 61, Batch(200/245), Loss: 0.0394\n",
      "Epoch: 61, Batch(220/245), Loss: 0.0403\n",
      "Epoch: 61, Batch(240/245), Loss: 0.0317\n",
      "epoch 61 duration 1.672 train_loss 0.036 val_loss 0.186 val_acc 0.9548\n",
      "Epoch: 62, Batch(20/245), Loss: 0.0275\n",
      "Epoch: 62, Batch(40/245), Loss: 0.0478\n",
      "Epoch: 62, Batch(60/245), Loss: 0.0280\n",
      "Epoch: 62, Batch(80/245), Loss: 0.0221\n",
      "Epoch: 62, Batch(100/245), Loss: 0.0379\n",
      "Epoch: 62, Batch(120/245), Loss: 0.0462\n",
      "Epoch: 62, Batch(140/245), Loss: 0.0474\n",
      "Epoch: 62, Batch(160/245), Loss: 0.0315\n",
      "Epoch: 62, Batch(180/245), Loss: 0.0364\n",
      "Epoch: 62, Batch(200/245), Loss: 0.0361\n",
      "Epoch: 62, Batch(220/245), Loss: 0.0319\n",
      "Epoch: 62, Batch(240/245), Loss: 0.0313\n",
      "epoch 62 duration 1.676 train_loss 0.036 val_loss 0.183 val_acc 0.9524\n",
      "Epoch: 63, Batch(20/245), Loss: 0.0368\n",
      "Epoch: 63, Batch(40/245), Loss: 0.0481\n",
      "Epoch: 63, Batch(60/245), Loss: 0.0302\n",
      "Epoch: 63, Batch(80/245), Loss: 0.0336\n",
      "Epoch: 63, Batch(100/245), Loss: 0.0413\n",
      "Epoch: 63, Batch(120/245), Loss: 0.0552\n",
      "Epoch: 63, Batch(140/245), Loss: 0.0333\n",
      "Epoch: 63, Batch(160/245), Loss: 0.0367\n",
      "Epoch: 63, Batch(180/245), Loss: 0.0548\n",
      "Epoch: 63, Batch(200/245), Loss: 0.0351\n",
      "Epoch: 63, Batch(220/245), Loss: 0.0248\n",
      "Epoch: 63, Batch(240/245), Loss: 0.0255\n",
      "epoch 63 duration 1.684 train_loss 0.038 val_loss 0.187 val_acc 0.9524\n",
      "Epoch: 64, Batch(20/245), Loss: 0.0296\n",
      "Epoch: 64, Batch(40/245), Loss: 0.0292\n",
      "Epoch: 64, Batch(60/245), Loss: 0.0298\n",
      "Epoch: 64, Batch(80/245), Loss: 0.0307\n",
      "Epoch: 64, Batch(100/245), Loss: 0.0466\n",
      "Epoch: 64, Batch(120/245), Loss: 0.0373\n",
      "Epoch: 64, Batch(140/245), Loss: 0.0683\n",
      "Epoch: 64, Batch(160/245), Loss: 0.0524\n",
      "Epoch: 64, Batch(180/245), Loss: 0.0535\n",
      "Epoch: 64, Batch(200/245), Loss: 0.0376\n",
      "Epoch: 64, Batch(220/245), Loss: 0.0281\n",
      "Epoch: 64, Batch(240/245), Loss: 0.0352\n",
      "epoch 64 duration 1.673 train_loss 0.040 val_loss 0.204 val_acc 0.9536\n",
      "Epoch: 65, Batch(20/245), Loss: 0.0294\n",
      "Epoch: 65, Batch(40/245), Loss: 0.0303\n",
      "Epoch: 65, Batch(60/245), Loss: 0.0304\n",
      "Epoch: 65, Batch(80/245), Loss: 0.0649\n",
      "Epoch: 65, Batch(100/245), Loss: 0.0487\n",
      "Epoch: 65, Batch(120/245), Loss: 0.0402\n",
      "Epoch: 65, Batch(140/245), Loss: 0.0394\n",
      "Epoch: 65, Batch(160/245), Loss: 0.0258\n",
      "Epoch: 65, Batch(180/245), Loss: 0.0362\n",
      "Epoch: 65, Batch(200/245), Loss: 0.0305\n",
      "Epoch: 65, Batch(220/245), Loss: 0.0424\n",
      "Epoch: 65, Batch(240/245), Loss: 0.0283\n",
      "epoch 65 duration 1.672 train_loss 0.037 val_loss 0.196 val_acc 0.9512\n",
      "Epoch: 66, Batch(20/245), Loss: 0.0427\n",
      "Epoch: 66, Batch(40/245), Loss: 0.0374\n",
      "Epoch: 66, Batch(60/245), Loss: 0.0278\n",
      "Epoch: 66, Batch(80/245), Loss: 0.0325\n",
      "Epoch: 66, Batch(100/245), Loss: 0.0311\n",
      "Epoch: 66, Batch(120/245), Loss: 0.0245\n",
      "Epoch: 66, Batch(140/245), Loss: 0.0216\n",
      "Epoch: 66, Batch(160/245), Loss: 0.0375\n",
      "Epoch: 66, Batch(180/245), Loss: 0.0241\n",
      "Epoch: 66, Batch(200/245), Loss: 0.0273\n",
      "Epoch: 66, Batch(220/245), Loss: 0.0584\n",
      "Epoch: 66, Batch(240/245), Loss: 0.0431\n",
      "epoch 66 duration 1.679 train_loss 0.036 val_loss 0.202 val_acc 0.9476\n",
      "Epoch: 67, Batch(20/245), Loss: 0.0427\n",
      "Epoch: 67, Batch(40/245), Loss: 0.0322\n",
      "Epoch: 67, Batch(60/245), Loss: 0.0385\n",
      "Epoch: 67, Batch(80/245), Loss: 0.0529\n",
      "Epoch: 67, Batch(100/245), Loss: 0.0269\n",
      "Epoch: 67, Batch(120/245), Loss: 0.0243\n",
      "Epoch: 67, Batch(140/245), Loss: 0.0389\n",
      "Epoch: 67, Batch(160/245), Loss: 0.0397\n",
      "Epoch: 67, Batch(180/245), Loss: 0.0927\n",
      "Epoch: 67, Batch(200/245), Loss: 0.0329\n",
      "Epoch: 67, Batch(220/245), Loss: 0.0360\n",
      "Epoch: 67, Batch(240/245), Loss: 0.0288\n",
      "epoch 67 duration 1.673 train_loss 0.040 val_loss 0.196 val_acc 0.9571\n",
      "Epoch: 68, Batch(20/245), Loss: 0.0320\n",
      "Epoch: 68, Batch(40/245), Loss: 0.0410\n",
      "Epoch: 68, Batch(60/245), Loss: 0.0317\n",
      "Epoch: 68, Batch(80/245), Loss: 0.0272\n",
      "Epoch: 68, Batch(100/245), Loss: 0.0424\n",
      "Epoch: 68, Batch(120/245), Loss: 0.0396\n",
      "Epoch: 68, Batch(140/245), Loss: 0.0261\n",
      "Epoch: 68, Batch(160/245), Loss: 0.0517\n",
      "Epoch: 68, Batch(180/245), Loss: 0.0313\n",
      "Epoch: 68, Batch(200/245), Loss: 0.0388\n",
      "Epoch: 68, Batch(220/245), Loss: 0.0519\n",
      "Epoch: 68, Batch(240/245), Loss: 0.0322\n",
      "epoch 68 duration 1.677 train_loss 0.037 val_loss 0.202 val_acc 0.9536\n",
      "Epoch: 69, Batch(20/245), Loss: 0.0185\n",
      "Epoch: 69, Batch(40/245), Loss: 0.0358\n",
      "Epoch: 69, Batch(60/245), Loss: 0.0319\n",
      "Epoch: 69, Batch(80/245), Loss: 0.0362\n",
      "Epoch: 69, Batch(100/245), Loss: 0.0290\n",
      "Epoch: 69, Batch(120/245), Loss: 0.0535\n",
      "Epoch: 69, Batch(140/245), Loss: 0.0237\n",
      "Epoch: 69, Batch(160/245), Loss: 0.0487\n",
      "Epoch: 69, Batch(180/245), Loss: 0.0343\n",
      "Epoch: 69, Batch(200/245), Loss: 0.0447\n",
      "Epoch: 69, Batch(220/245), Loss: 0.0425\n",
      "Epoch: 69, Batch(240/245), Loss: 0.0362\n",
      "epoch 69 duration 1.672 train_loss 0.036 val_loss 0.198 val_acc 0.9571\n",
      "Epoch: 70, Batch(20/245), Loss: 0.0289\n",
      "Epoch: 70, Batch(40/245), Loss: 0.0408\n",
      "Epoch: 70, Batch(60/245), Loss: 0.0747\n",
      "Epoch: 70, Batch(80/245), Loss: 0.0237\n",
      "Epoch: 70, Batch(100/245), Loss: 0.0505\n",
      "Epoch: 70, Batch(120/245), Loss: 0.0350\n",
      "Epoch: 70, Batch(140/245), Loss: 0.0432\n",
      "Epoch: 70, Batch(160/245), Loss: 0.0585\n",
      "Epoch: 70, Batch(180/245), Loss: 0.0489\n",
      "Epoch: 70, Batch(200/245), Loss: 0.0228\n",
      "Epoch: 70, Batch(220/245), Loss: 0.0390\n",
      "Epoch: 70, Batch(240/245), Loss: 0.0380\n",
      "epoch 70 duration 1.673 train_loss 0.042 val_loss 0.201 val_acc 0.9548\n",
      "Epoch: 71, Batch(20/245), Loss: 0.0504\n",
      "Epoch: 71, Batch(40/245), Loss: 0.0400\n",
      "Epoch: 71, Batch(60/245), Loss: 0.0444\n",
      "Epoch: 71, Batch(80/245), Loss: 0.0555\n",
      "Epoch: 71, Batch(100/245), Loss: 0.0506\n",
      "Epoch: 71, Batch(120/245), Loss: 0.0213\n",
      "Epoch: 71, Batch(140/245), Loss: 0.0741\n",
      "Epoch: 71, Batch(160/245), Loss: 0.0444\n",
      "Epoch: 71, Batch(180/245), Loss: 0.0343\n",
      "Epoch: 71, Batch(200/245), Loss: 0.0386\n",
      "Epoch: 71, Batch(220/245), Loss: 0.0666\n",
      "Epoch: 71, Batch(240/245), Loss: 0.0396\n",
      "epoch 71 duration 1.677 train_loss 0.047 val_loss 0.187 val_acc 0.9548\n",
      "Epoch: 72, Batch(20/245), Loss: 0.0196\n",
      "Epoch: 72, Batch(40/245), Loss: 0.0539\n",
      "Epoch: 72, Batch(60/245), Loss: 0.0356\n",
      "Epoch: 72, Batch(80/245), Loss: 0.0683\n",
      "Epoch: 72, Batch(100/245), Loss: 0.0313\n",
      "Epoch: 72, Batch(120/245), Loss: 0.0376\n",
      "Epoch: 72, Batch(140/245), Loss: 0.0278\n",
      "Epoch: 72, Batch(160/245), Loss: 0.0267\n",
      "Epoch: 72, Batch(180/245), Loss: 0.0347\n",
      "Epoch: 72, Batch(200/245), Loss: 0.0236\n",
      "Epoch: 72, Batch(220/245), Loss: 0.0515\n",
      "Epoch: 72, Batch(240/245), Loss: 0.0314\n",
      "epoch 72 duration 1.671 train_loss 0.037 val_loss 0.186 val_acc 0.9512\n",
      "Epoch: 73, Batch(20/245), Loss: 0.0300\n",
      "Epoch: 73, Batch(40/245), Loss: 0.0232\n",
      "Epoch: 73, Batch(60/245), Loss: 0.0400\n",
      "Epoch: 73, Batch(80/245), Loss: 0.0573\n",
      "Epoch: 73, Batch(100/245), Loss: 0.0351\n",
      "Epoch: 73, Batch(120/245), Loss: 0.0300\n",
      "Epoch: 73, Batch(140/245), Loss: 0.0220\n",
      "Epoch: 73, Batch(160/245), Loss: 0.0597\n",
      "Epoch: 73, Batch(180/245), Loss: 0.0518\n",
      "Epoch: 73, Batch(200/245), Loss: 0.0450\n",
      "Epoch: 73, Batch(220/245), Loss: 0.0373\n",
      "Epoch: 73, Batch(240/245), Loss: 0.0312\n",
      "epoch 73 duration 1.674 train_loss 0.038 val_loss 0.167 val_acc 0.9571\n",
      "Epoch: 74, Batch(20/245), Loss: 0.0253\n",
      "Epoch: 74, Batch(40/245), Loss: 0.0351\n",
      "Epoch: 74, Batch(60/245), Loss: 0.0408\n",
      "Epoch: 74, Batch(80/245), Loss: 0.0389\n",
      "Epoch: 74, Batch(100/245), Loss: 0.0238\n",
      "Epoch: 74, Batch(120/245), Loss: 0.0435\n",
      "Epoch: 74, Batch(140/245), Loss: 0.0276\n",
      "Epoch: 74, Batch(160/245), Loss: 0.0386\n",
      "Epoch: 74, Batch(180/245), Loss: 0.0305\n",
      "Epoch: 74, Batch(200/245), Loss: 0.0394\n",
      "Epoch: 74, Batch(220/245), Loss: 0.0228\n",
      "Epoch: 74, Batch(240/245), Loss: 0.0285\n",
      "epoch 74 duration 1.675 train_loss 0.033 val_loss 0.191 val_acc 0.9560\n",
      "Epoch: 75, Batch(20/245), Loss: 0.0261\n",
      "Epoch: 75, Batch(40/245), Loss: 0.0280\n",
      "Epoch: 75, Batch(60/245), Loss: 0.0221\n",
      "Epoch: 75, Batch(80/245), Loss: 0.0344\n",
      "Epoch: 75, Batch(100/245), Loss: 0.0343\n",
      "Epoch: 75, Batch(120/245), Loss: 0.0354\n",
      "Epoch: 75, Batch(140/245), Loss: 0.0316\n",
      "Epoch: 75, Batch(160/245), Loss: 0.0228\n",
      "Epoch: 75, Batch(180/245), Loss: 0.0284\n",
      "Epoch: 75, Batch(200/245), Loss: 0.0437\n",
      "Epoch: 75, Batch(220/245), Loss: 0.0375\n",
      "Epoch: 75, Batch(240/245), Loss: 0.0662\n",
      "epoch 75 duration 1.672 train_loss 0.034 val_loss 0.184 val_acc 0.9595\n",
      "Epoch: 76, Batch(20/245), Loss: 0.0263\n",
      "Epoch: 76, Batch(40/245), Loss: 0.0321\n",
      "Epoch: 76, Batch(60/245), Loss: 0.0198\n",
      "Epoch: 76, Batch(80/245), Loss: 0.0209\n",
      "Epoch: 76, Batch(100/245), Loss: 0.0367\n",
      "Epoch: 76, Batch(120/245), Loss: 0.0282\n",
      "Epoch: 76, Batch(140/245), Loss: 0.0512\n",
      "Epoch: 76, Batch(160/245), Loss: 0.0287\n",
      "Epoch: 76, Batch(180/245), Loss: 0.0398\n",
      "Epoch: 76, Batch(200/245), Loss: 0.0268\n",
      "Epoch: 76, Batch(220/245), Loss: 0.0347\n",
      "Epoch: 76, Batch(240/245), Loss: 0.0305\n",
      "epoch 76 duration 1.672 train_loss 0.031 val_loss 0.202 val_acc 0.9536\n",
      "Epoch: 77, Batch(20/245), Loss: 0.0319\n",
      "Epoch: 77, Batch(40/245), Loss: 0.0279\n",
      "Epoch: 77, Batch(60/245), Loss: 0.0274\n",
      "Epoch: 77, Batch(80/245), Loss: 0.0215\n",
      "Epoch: 77, Batch(100/245), Loss: 0.0382\n",
      "Epoch: 77, Batch(120/245), Loss: 0.0275\n",
      "Epoch: 77, Batch(140/245), Loss: 0.0287\n",
      "Epoch: 77, Batch(160/245), Loss: 0.0324\n",
      "Epoch: 77, Batch(180/245), Loss: 0.0426\n",
      "Epoch: 77, Batch(200/245), Loss: 0.0190\n",
      "Epoch: 77, Batch(220/245), Loss: 0.0281\n",
      "Epoch: 77, Batch(240/245), Loss: 0.0300\n",
      "epoch 77 duration 1.672 train_loss 0.030 val_loss 0.192 val_acc 0.9536\n",
      "Epoch: 78, Batch(20/245), Loss: 0.0377\n",
      "Epoch: 78, Batch(40/245), Loss: 0.0333\n",
      "Epoch: 78, Batch(60/245), Loss: 0.0300\n",
      "Epoch: 78, Batch(80/245), Loss: 0.0556\n",
      "Epoch: 78, Batch(100/245), Loss: 0.0353\n",
      "Epoch: 78, Batch(120/245), Loss: 0.0282\n",
      "Epoch: 78, Batch(140/245), Loss: 0.0374\n",
      "Epoch: 78, Batch(160/245), Loss: 0.0323\n",
      "Epoch: 78, Batch(180/245), Loss: 0.0283\n",
      "Epoch: 78, Batch(200/245), Loss: 0.0315\n",
      "Epoch: 78, Batch(220/245), Loss: 0.0356\n",
      "Epoch: 78, Batch(240/245), Loss: 0.0227\n",
      "epoch 78 duration 1.673 train_loss 0.034 val_loss 0.211 val_acc 0.9488\n",
      "Epoch: 79, Batch(20/245), Loss: 0.0370\n",
      "Epoch: 79, Batch(40/245), Loss: 0.0444\n",
      "Epoch: 79, Batch(60/245), Loss: 0.0330\n",
      "Epoch: 79, Batch(80/245), Loss: 0.0611\n",
      "Epoch: 79, Batch(100/245), Loss: 0.0321\n",
      "Epoch: 79, Batch(120/245), Loss: 0.0218\n",
      "Epoch: 79, Batch(140/245), Loss: 0.0353\n",
      "Epoch: 79, Batch(160/245), Loss: 0.0399\n",
      "Epoch: 79, Batch(180/245), Loss: 0.0288\n",
      "Epoch: 79, Batch(200/245), Loss: 0.0354\n",
      "Epoch: 79, Batch(220/245), Loss: 0.0298\n",
      "Epoch: 79, Batch(240/245), Loss: 0.0274\n",
      "epoch 79 duration 1.674 train_loss 0.036 val_loss 0.206 val_acc 0.9536\n",
      "Epoch: 80, Batch(20/245), Loss: 0.0256\n",
      "Epoch: 80, Batch(40/245), Loss: 0.0357\n",
      "Epoch: 80, Batch(60/245), Loss: 0.0410\n",
      "Epoch: 80, Batch(80/245), Loss: 0.0258\n",
      "Epoch: 80, Batch(100/245), Loss: 0.0561\n",
      "Epoch: 80, Batch(120/245), Loss: 0.0358\n",
      "Epoch: 80, Batch(140/245), Loss: 0.0287\n",
      "Epoch: 80, Batch(160/245), Loss: 0.0342\n",
      "Epoch: 80, Batch(180/245), Loss: 0.0242\n",
      "Epoch: 80, Batch(200/245), Loss: 0.0391\n",
      "Epoch: 80, Batch(220/245), Loss: 0.0427\n",
      "Epoch: 80, Batch(240/245), Loss: 0.0242\n",
      "epoch 80 duration 1.673 train_loss 0.034 val_loss 0.208 val_acc 0.9512\n",
      "Epoch: 81, Batch(20/245), Loss: 0.0305\n",
      "Epoch: 81, Batch(40/245), Loss: 0.0347\n",
      "Epoch: 81, Batch(60/245), Loss: 0.0317\n",
      "Epoch: 81, Batch(80/245), Loss: 0.0234\n",
      "Epoch: 81, Batch(100/245), Loss: 0.0291\n",
      "Epoch: 81, Batch(120/245), Loss: 0.0442\n",
      "Epoch: 81, Batch(140/245), Loss: 0.0271\n",
      "Epoch: 81, Batch(160/245), Loss: 0.0343\n",
      "Epoch: 81, Batch(180/245), Loss: 0.0253\n",
      "Epoch: 81, Batch(200/245), Loss: 0.0265\n",
      "Epoch: 81, Batch(220/245), Loss: 0.0245\n",
      "Epoch: 81, Batch(240/245), Loss: 0.0421\n",
      "epoch 81 duration 1.674 train_loss 0.032 val_loss 0.199 val_acc 0.9536\n",
      "Epoch: 82, Batch(20/245), Loss: 0.0242\n",
      "Epoch: 82, Batch(40/245), Loss: 0.0253\n",
      "Epoch: 82, Batch(60/245), Loss: 0.0256\n",
      "Epoch: 82, Batch(80/245), Loss: 0.0402\n",
      "Epoch: 82, Batch(100/245), Loss: 0.0301\n",
      "Epoch: 82, Batch(120/245), Loss: 0.0216\n",
      "Epoch: 82, Batch(140/245), Loss: 0.0302\n",
      "Epoch: 82, Batch(160/245), Loss: 0.0290\n",
      "Epoch: 82, Batch(180/245), Loss: 0.0280\n",
      "Epoch: 82, Batch(200/245), Loss: 0.0362\n",
      "Epoch: 82, Batch(220/245), Loss: 0.0316\n",
      "Epoch: 82, Batch(240/245), Loss: 0.0300\n",
      "epoch 82 duration 1.673 train_loss 0.029 val_loss 0.213 val_acc 0.9536\n",
      "Epoch: 83, Batch(20/245), Loss: 0.0497\n",
      "Epoch: 83, Batch(40/245), Loss: 0.0495\n",
      "Epoch: 83, Batch(60/245), Loss: 0.0325\n",
      "Epoch: 83, Batch(80/245), Loss: 0.0458\n",
      "Epoch: 83, Batch(100/245), Loss: 0.0247\n",
      "Epoch: 83, Batch(120/245), Loss: 0.0426\n",
      "Epoch: 83, Batch(140/245), Loss: 0.0407\n",
      "Epoch: 83, Batch(160/245), Loss: 0.0287\n",
      "Epoch: 83, Batch(180/245), Loss: 0.0295\n",
      "Epoch: 83, Batch(200/245), Loss: 0.0223\n",
      "Epoch: 83, Batch(220/245), Loss: 0.0355\n",
      "Epoch: 83, Batch(240/245), Loss: 0.0219\n",
      "epoch 83 duration 1.678 train_loss 0.035 val_loss 0.208 val_acc 0.9536\n",
      "Epoch: 84, Batch(20/245), Loss: 0.0250\n",
      "Epoch: 84, Batch(40/245), Loss: 0.0345\n",
      "Epoch: 84, Batch(60/245), Loss: 0.0537\n",
      "Epoch: 84, Batch(80/245), Loss: 0.0289\n",
      "Epoch: 84, Batch(100/245), Loss: 0.0249\n",
      "Epoch: 84, Batch(120/245), Loss: 0.0239\n",
      "Epoch: 84, Batch(140/245), Loss: 0.0257\n",
      "Epoch: 84, Batch(160/245), Loss: 0.0518\n",
      "Epoch: 84, Batch(180/245), Loss: 0.0436\n",
      "Epoch: 84, Batch(200/245), Loss: 0.0271\n",
      "Epoch: 84, Batch(220/245), Loss: 0.0255\n",
      "Epoch: 84, Batch(240/245), Loss: 0.0189\n",
      "epoch 84 duration 1.678 train_loss 0.032 val_loss 0.204 val_acc 0.9548\n",
      "Epoch: 85, Batch(20/245), Loss: 0.0265\n",
      "Epoch: 85, Batch(40/245), Loss: 0.0265\n",
      "Epoch: 85, Batch(60/245), Loss: 0.0283\n",
      "Epoch: 85, Batch(80/245), Loss: 0.0359\n",
      "Epoch: 85, Batch(100/245), Loss: 0.0381\n",
      "Epoch: 85, Batch(120/245), Loss: 0.0351\n",
      "Epoch: 85, Batch(140/245), Loss: 0.0204\n",
      "Epoch: 85, Batch(160/245), Loss: 0.0312\n",
      "Epoch: 85, Batch(180/245), Loss: 0.0425\n",
      "Epoch: 85, Batch(200/245), Loss: 0.0261\n",
      "Epoch: 85, Batch(220/245), Loss: 0.0224\n",
      "Epoch: 85, Batch(240/245), Loss: 0.0356\n",
      "epoch 85 duration 1.672 train_loss 0.031 val_loss 0.202 val_acc 0.9548\n",
      "Epoch: 86, Batch(20/245), Loss: 0.0232\n",
      "Epoch: 86, Batch(40/245), Loss: 0.0329\n",
      "Epoch: 86, Batch(60/245), Loss: 0.0230\n",
      "Epoch: 86, Batch(80/245), Loss: 0.0437\n",
      "Epoch: 86, Batch(100/245), Loss: 0.0332\n",
      "Epoch: 86, Batch(120/245), Loss: 0.0211\n",
      "Epoch: 86, Batch(140/245), Loss: 0.0317\n",
      "Epoch: 86, Batch(160/245), Loss: 0.0335\n",
      "Epoch: 86, Batch(180/245), Loss: 0.0312\n",
      "Epoch: 86, Batch(200/245), Loss: 0.0267\n",
      "Epoch: 86, Batch(220/245), Loss: 0.0223\n",
      "Epoch: 86, Batch(240/245), Loss: 0.0253\n",
      "epoch 86 duration 1.673 train_loss 0.029 val_loss 0.201 val_acc 0.9536\n",
      "Epoch: 87, Batch(20/245), Loss: 0.0341\n",
      "Epoch: 87, Batch(40/245), Loss: 0.0402\n",
      "Epoch: 87, Batch(60/245), Loss: 0.0631\n",
      "Epoch: 87, Batch(80/245), Loss: 0.0243\n",
      "Epoch: 87, Batch(100/245), Loss: 0.0320\n",
      "Epoch: 87, Batch(120/245), Loss: 0.0302\n",
      "Epoch: 87, Batch(140/245), Loss: 0.0302\n",
      "Epoch: 87, Batch(160/245), Loss: 0.0370\n",
      "Epoch: 87, Batch(180/245), Loss: 0.0213\n",
      "Epoch: 87, Batch(200/245), Loss: 0.0526\n",
      "Epoch: 87, Batch(220/245), Loss: 0.0349\n",
      "Epoch: 87, Batch(240/245), Loss: 0.0334\n",
      "epoch 87 duration 1.673 train_loss 0.036 val_loss 0.207 val_acc 0.9524\n",
      "Epoch: 88, Batch(20/245), Loss: 0.0275\n",
      "Epoch: 88, Batch(40/245), Loss: 0.0258\n",
      "Epoch: 88, Batch(60/245), Loss: 0.0405\n",
      "Epoch: 88, Batch(80/245), Loss: 0.0533\n",
      "Epoch: 88, Batch(100/245), Loss: 0.0235\n",
      "Epoch: 88, Batch(120/245), Loss: 0.0450\n",
      "Epoch: 88, Batch(140/245), Loss: 0.0448\n",
      "Epoch: 88, Batch(160/245), Loss: 0.0325\n",
      "Epoch: 88, Batch(180/245), Loss: 0.0226\n",
      "Epoch: 88, Batch(200/245), Loss: 0.0261\n",
      "Epoch: 88, Batch(220/245), Loss: 0.0342\n",
      "Epoch: 88, Batch(240/245), Loss: 0.0349\n",
      "epoch 88 duration 1.673 train_loss 0.034 val_loss 0.197 val_acc 0.9524\n",
      "Epoch: 89, Batch(20/245), Loss: 0.0300\n",
      "Epoch: 89, Batch(40/245), Loss: 0.0283\n",
      "Epoch: 89, Batch(60/245), Loss: 0.0186\n",
      "Epoch: 89, Batch(80/245), Loss: 0.0436\n",
      "Epoch: 89, Batch(100/245), Loss: 0.0260\n",
      "Epoch: 89, Batch(120/245), Loss: 0.0367\n",
      "Epoch: 89, Batch(140/245), Loss: 0.0342\n",
      "Epoch: 89, Batch(160/245), Loss: 0.0228\n",
      "Epoch: 89, Batch(180/245), Loss: 0.0438\n",
      "Epoch: 89, Batch(200/245), Loss: 0.0275\n",
      "Epoch: 89, Batch(220/245), Loss: 0.0252\n",
      "Epoch: 89, Batch(240/245), Loss: 0.0223\n",
      "epoch 89 duration 1.672 train_loss 0.030 val_loss 0.200 val_acc 0.9560\n",
      "Epoch: 90, Batch(20/245), Loss: 0.0279\n",
      "Epoch: 90, Batch(40/245), Loss: 0.0269\n",
      "Epoch: 90, Batch(60/245), Loss: 0.0348\n",
      "Epoch: 90, Batch(80/245), Loss: 0.0308\n",
      "Epoch: 90, Batch(100/245), Loss: 0.0455\n",
      "Epoch: 90, Batch(120/245), Loss: 0.0403\n",
      "Epoch: 90, Batch(140/245), Loss: 0.0309\n",
      "Epoch: 90, Batch(160/245), Loss: 0.0214\n",
      "Epoch: 90, Batch(180/245), Loss: 0.0329\n",
      "Epoch: 90, Batch(200/245), Loss: 0.0242\n",
      "Epoch: 90, Batch(220/245), Loss: 0.0256\n",
      "Epoch: 90, Batch(240/245), Loss: 0.0206\n",
      "epoch 90 duration 1.672 train_loss 0.030 val_loss 0.215 val_acc 0.9524\n",
      "Epoch: 91, Batch(20/245), Loss: 0.0236\n",
      "Epoch: 91, Batch(40/245), Loss: 0.0398\n",
      "Epoch: 91, Batch(60/245), Loss: 0.0489\n",
      "Epoch: 91, Batch(80/245), Loss: 0.0509\n",
      "Epoch: 91, Batch(100/245), Loss: 0.0282\n",
      "Epoch: 91, Batch(120/245), Loss: 0.0223\n",
      "Epoch: 91, Batch(140/245), Loss: 0.0319\n",
      "Epoch: 91, Batch(160/245), Loss: 0.0243\n",
      "Epoch: 91, Batch(180/245), Loss: 0.0281\n",
      "Epoch: 91, Batch(200/245), Loss: 0.0396\n",
      "Epoch: 91, Batch(220/245), Loss: 0.0262\n",
      "Epoch: 91, Batch(240/245), Loss: 0.0245\n",
      "epoch 91 duration 1.674 train_loss 0.032 val_loss 0.202 val_acc 0.9560\n",
      "Epoch: 92, Batch(20/245), Loss: 0.0564\n",
      "Epoch: 92, Batch(40/245), Loss: 0.0240\n",
      "Epoch: 92, Batch(60/245), Loss: 0.0286\n",
      "Epoch: 92, Batch(80/245), Loss: 0.0264\n",
      "Epoch: 92, Batch(100/245), Loss: 0.0207\n",
      "Epoch: 92, Batch(120/245), Loss: 0.0612\n",
      "Epoch: 92, Batch(140/245), Loss: 0.0284\n",
      "Epoch: 92, Batch(160/245), Loss: 0.0394\n",
      "Epoch: 92, Batch(180/245), Loss: 0.0376\n",
      "Epoch: 92, Batch(200/245), Loss: 0.0327\n",
      "Epoch: 92, Batch(220/245), Loss: 0.0400\n",
      "Epoch: 92, Batch(240/245), Loss: 0.0319\n",
      "epoch 92 duration 1.673 train_loss 0.035 val_loss 0.204 val_acc 0.9548\n",
      "Epoch: 93, Batch(20/245), Loss: 0.0367\n",
      "Epoch: 93, Batch(40/245), Loss: 0.0216\n",
      "Epoch: 93, Batch(60/245), Loss: 0.0512\n",
      "Epoch: 93, Batch(80/245), Loss: 0.0335\n",
      "Epoch: 93, Batch(100/245), Loss: 0.0305\n",
      "Epoch: 93, Batch(120/245), Loss: 0.0324\n",
      "Epoch: 93, Batch(140/245), Loss: 0.0234\n",
      "Epoch: 93, Batch(160/245), Loss: 0.0264\n",
      "Epoch: 93, Batch(180/245), Loss: 0.0255\n",
      "Epoch: 93, Batch(200/245), Loss: 0.0224\n",
      "Epoch: 93, Batch(220/245), Loss: 0.0329\n",
      "Epoch: 93, Batch(240/245), Loss: 0.0290\n",
      "epoch 93 duration 1.673 train_loss 0.030 val_loss 0.199 val_acc 0.9536\n",
      "Epoch: 94, Batch(20/245), Loss: 0.0230\n",
      "Epoch: 94, Batch(40/245), Loss: 0.0242\n",
      "Epoch: 94, Batch(60/245), Loss: 0.0252\n",
      "Epoch: 94, Batch(80/245), Loss: 0.0353\n",
      "Epoch: 94, Batch(100/245), Loss: 0.0320\n",
      "Epoch: 94, Batch(120/245), Loss: 0.0403\n",
      "Epoch: 94, Batch(140/245), Loss: 0.0468\n",
      "Epoch: 94, Batch(160/245), Loss: 0.0232\n",
      "Epoch: 94, Batch(180/245), Loss: 0.0315\n",
      "Epoch: 94, Batch(200/245), Loss: 0.0195\n",
      "Epoch: 94, Batch(220/245), Loss: 0.0266\n",
      "Epoch: 94, Batch(240/245), Loss: 0.0244\n",
      "epoch 94 duration 1.673 train_loss 0.029 val_loss 0.211 val_acc 0.9536\n",
      "Epoch: 95, Batch(20/245), Loss: 0.0240\n",
      "Epoch: 95, Batch(40/245), Loss: 0.0316\n",
      "Epoch: 95, Batch(60/245), Loss: 0.0302\n",
      "Epoch: 95, Batch(80/245), Loss: 0.0262\n",
      "Epoch: 95, Batch(100/245), Loss: 0.0298\n",
      "Epoch: 95, Batch(120/245), Loss: 0.0345\n",
      "Epoch: 95, Batch(140/245), Loss: 0.0244\n",
      "Epoch: 95, Batch(160/245), Loss: 0.0298\n",
      "Epoch: 95, Batch(180/245), Loss: 0.0348\n",
      "Epoch: 95, Batch(200/245), Loss: 0.0314\n",
      "Epoch: 95, Batch(220/245), Loss: 0.0336\n",
      "Epoch: 95, Batch(240/245), Loss: 0.0219\n",
      "epoch 95 duration 1.677 train_loss 0.030 val_loss 0.204 val_acc 0.9500\n",
      "Epoch: 96, Batch(20/245), Loss: 0.0183\n",
      "Epoch: 96, Batch(40/245), Loss: 0.0308\n",
      "Epoch: 96, Batch(60/245), Loss: 0.0209\n",
      "Epoch: 96, Batch(80/245), Loss: 0.0433\n",
      "Epoch: 96, Batch(100/245), Loss: 0.0306\n",
      "Epoch: 96, Batch(120/245), Loss: 0.0265\n",
      "Epoch: 96, Batch(140/245), Loss: 0.0252\n",
      "Epoch: 96, Batch(160/245), Loss: 0.0334\n",
      "Epoch: 96, Batch(180/245), Loss: 0.0483\n",
      "Epoch: 96, Batch(200/245), Loss: 0.0346\n",
      "Epoch: 96, Batch(220/245), Loss: 0.0235\n",
      "Epoch: 96, Batch(240/245), Loss: 0.0282\n",
      "epoch 96 duration 1.681 train_loss 0.030 val_loss 0.196 val_acc 0.9536\n",
      "Epoch: 97, Batch(20/245), Loss: 0.0182\n",
      "Epoch: 97, Batch(40/245), Loss: 0.0339\n",
      "Epoch: 97, Batch(60/245), Loss: 0.0204\n",
      "Epoch: 97, Batch(80/245), Loss: 0.0292\n",
      "Epoch: 97, Batch(100/245), Loss: 0.0468\n",
      "Epoch: 97, Batch(120/245), Loss: 0.0176\n",
      "Epoch: 97, Batch(140/245), Loss: 0.0252\n",
      "Epoch: 97, Batch(160/245), Loss: 0.0317\n",
      "Epoch: 97, Batch(180/245), Loss: 0.0209\n",
      "Epoch: 97, Batch(200/245), Loss: 0.0449\n",
      "Epoch: 97, Batch(220/245), Loss: 0.0205\n",
      "Epoch: 97, Batch(240/245), Loss: 0.0297\n",
      "epoch 97 duration 1.677 train_loss 0.029 val_loss 0.202 val_acc 0.9548\n",
      "Epoch: 98, Batch(20/245), Loss: 0.0271\n",
      "Epoch: 98, Batch(40/245), Loss: 0.0223\n",
      "Epoch: 98, Batch(60/245), Loss: 0.0393\n",
      "Epoch: 98, Batch(80/245), Loss: 0.0283\n",
      "Epoch: 98, Batch(100/245), Loss: 0.0208\n",
      "Epoch: 98, Batch(120/245), Loss: 0.0314\n",
      "Epoch: 98, Batch(140/245), Loss: 0.0240\n",
      "Epoch: 98, Batch(160/245), Loss: 0.0401\n",
      "Epoch: 98, Batch(180/245), Loss: 0.0216\n",
      "Epoch: 98, Batch(200/245), Loss: 0.0259\n",
      "Epoch: 98, Batch(220/245), Loss: 0.0292\n",
      "Epoch: 98, Batch(240/245), Loss: 0.0330\n",
      "epoch 98 duration 1.675 train_loss 0.028 val_loss 0.196 val_acc 0.9571\n",
      "Epoch: 99, Batch(20/245), Loss: 0.0271\n",
      "Epoch: 99, Batch(40/245), Loss: 0.0282\n",
      "Epoch: 99, Batch(60/245), Loss: 0.0281\n",
      "Epoch: 99, Batch(80/245), Loss: 0.0393\n",
      "Epoch: 99, Batch(100/245), Loss: 0.0343\n",
      "Epoch: 99, Batch(120/245), Loss: 0.0339\n",
      "Epoch: 99, Batch(140/245), Loss: 0.0258\n",
      "Epoch: 99, Batch(160/245), Loss: 0.0268\n",
      "Epoch: 99, Batch(180/245), Loss: 0.0380\n",
      "Epoch: 99, Batch(200/245), Loss: 0.0238\n",
      "Epoch: 99, Batch(220/245), Loss: 0.0153\n",
      "Epoch: 99, Batch(240/245), Loss: 0.0217\n",
      "epoch 99 duration 1.678 train_loss 0.028 val_loss 0.205 val_acc 0.9560\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T02:28:50.617887Z",
     "iopub.status.busy": "2024-12-16T02:28:50.617133Z",
     "iopub.status.idle": "2024-12-16T02:29:13.181343Z",
     "shell.execute_reply": "2024-12-16T02:29:13.180498Z",
     "shell.execute_reply.started": "2024-12-16T02:28:50.617854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main/test.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "#model parameters:  4628878\n",
      "average inference time:  18.372788617724463\n",
      "accuracy:  0.9595238095238096\n"
     ]
    }
   ],
   "source": [
    "!python test.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6290008,
     "sourceId": 10182333,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6310355,
     "sourceId": 10210139,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
