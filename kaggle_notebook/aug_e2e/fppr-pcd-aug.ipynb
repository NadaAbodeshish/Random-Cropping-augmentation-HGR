{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-16T00:17:51.262281Z",
     "iopub.status.busy": "2024-12-16T00:17:51.262030Z",
     "iopub.status.idle": "2024-12-16T00:17:58.965968Z",
     "shell.execute_reply": "2024-12-16T00:17:58.964869Z",
     "shell.execute_reply.started": "2024-12-16T00:17:51.262256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion'...\n",
      "remote: Enumerating objects: 780, done.\u001b[K\n",
      "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
      "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
      "remote: Total 780 (delta 76), reused 104 (delta 42), pack-reused 637 (from 1)\u001b[K\n",
      "Receiving objects: 100% (780/780), 166.71 MiB | 38.41 MiB/s, done.\n",
      "Resolving deltas: 100% (428/428), done.\n",
      "Updating files: 100% (213/213), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone hhttps://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:17:58.968556Z",
     "iopub.status.busy": "2024-12-16T00:17:58.968186Z",
     "iopub.status.idle": "2024-12-16T00:17:58.975937Z",
     "shell.execute_reply": "2024-12-16T00:17:58.975046Z",
     "shell.execute_reply.started": "2024-12-16T00:17:58.968515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/Random-Cropping-augmentation-HGR//FPPRC/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:17:58.977362Z",
     "iopub.status.busy": "2024-12-16T00:17:58.977053Z",
     "iopub.status.idle": "2024-12-16T00:17:58.985656Z",
     "shell.execute_reply": "2024-12-16T00:17:58.984781Z",
     "shell.execute_reply.started": "2024-12-16T00:17:58.977327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = \"/kaggle/working/Random-Cropping-augmentation-HGR/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512\"\n",
    "os.makedirs(base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:17:58.988649Z",
     "iopub.status.busy": "2024-12-16T00:17:58.988268Z",
     "iopub.status.idle": "2024-12-16T00:19:28.515536Z",
     "shell.execute_reply": "2024-12-16T00:19:28.514571Z",
     "shell.execute_reply.started": "2024-12-16T00:17:58.988592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/shrec-processed-aug/Random-Cropping-augmentation-HGR/FPPRC/data/shrec17/augmented-dataset/* /kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:19:28.517346Z",
     "iopub.status.busy": "2024-12-16T00:19:28.517020Z",
     "iopub.status.idle": "2024-12-16T00:19:28.523206Z",
     "shell.execute_reply": "2024-12-16T00:19:28.522396Z",
     "shell.execute_reply.started": "2024-12-16T00:19:28.517315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:19:28.524655Z",
     "iopub.status.busy": "2024-12-16T00:19:28.524378Z",
     "iopub.status.idle": "2024-12-16T00:19:28.536758Z",
     "shell.execute_reply": "2024-12-16T00:19:28.535892Z",
     "shell.execute_reply.started": "2024-12-16T00:19:28.524601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/Random-Cropping-augmentation-HGR/FPPRC/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:19:28.538241Z",
     "iopub.status.busy": "2024-12-16T00:19:28.537953Z",
     "iopub.status.idle": "2024-12-16T00:19:39.610155Z",
     "shell.execute_reply": "2024-12-16T00:19:39.608976Z",
     "shell.execute_reply.started": "2024-12-16T00:19:28.538218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs) (6.0.2)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: yacs\n",
      "Successfully installed yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:19:39.612266Z",
     "iopub.status.busy": "2024-12-16T00:19:39.611922Z",
     "iopub.status.idle": "2024-12-16T00:19:57.209358Z",
     "shell.execute_reply": "2024-12-16T00:19:57.208240Z",
     "shell.execute_reply.started": "2024-12-16T00:19:39.612237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bps\n",
      "  Downloading BPS-0.1.0-py3-none-any.whl.metadata (61 bytes)\n",
      "Downloading BPS-0.1.0-py3-none-any.whl (897 bytes)\n",
      "Installing collected packages: bps\n",
      "Successfully installed bps-0.1.0\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.6.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.6.2)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bps\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:19:57.211380Z",
     "iopub.status.busy": "2024-12-16T00:19:57.210983Z",
     "iopub.status.idle": "2024-12-16T03:10:50.705779Z",
     "shell.execute_reply": "2024-12-16T03:10:50.704667Z",
     "shell.execute_reply.started": "2024-12-16T00:19:57.211338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Epoch: 0, Batch(20/245), Loss: 2.5383\n",
      "Epoch: 0, Batch(40/245), Loss: 2.2219\n",
      "Epoch: 0, Batch(60/245), Loss: 2.1063\n",
      "Epoch: 0, Batch(80/245), Loss: 1.9817\n",
      "Epoch: 0, Batch(100/245), Loss: 1.8447\n",
      "Epoch: 0, Batch(120/245), Loss: 1.6789\n",
      "Epoch: 0, Batch(140/245), Loss: 1.6064\n",
      "Epoch: 0, Batch(160/245), Loss: 1.4482\n",
      "Epoch: 0, Batch(180/245), Loss: 1.4003\n",
      "Epoch: 0, Batch(200/245), Loss: 1.3578\n",
      "Epoch: 0, Batch(220/245), Loss: 1.1364\n",
      "Epoch: 0, Batch(240/245), Loss: 1.3071\n",
      "epoch 0 duration 1.570 train_loss 1.709 val_loss 0.992 val_acc 0.7500\n",
      "Epoch: 1, Batch(20/245), Loss: 1.1277\n",
      "Epoch: 1, Batch(40/245), Loss: 1.1786\n",
      "Epoch: 1, Batch(60/245), Loss: 1.0165\n",
      "Epoch: 1, Batch(80/245), Loss: 0.9470\n",
      "Epoch: 1, Batch(100/245), Loss: 1.0097\n",
      "Epoch: 1, Batch(120/245), Loss: 0.8831\n",
      "Epoch: 1, Batch(140/245), Loss: 0.8080\n",
      "Epoch: 1, Batch(160/245), Loss: 0.7624\n",
      "Epoch: 1, Batch(180/245), Loss: 0.8820\n",
      "Epoch: 1, Batch(200/245), Loss: 0.8270\n",
      "Epoch: 1, Batch(220/245), Loss: 0.8178\n",
      "Epoch: 1, Batch(240/245), Loss: 0.8333\n",
      "epoch 1 duration 1.709 train_loss 0.921 val_loss 0.627 val_acc 0.8560\n",
      "Epoch: 2, Batch(20/245), Loss: 0.6283\n",
      "Epoch: 2, Batch(40/245), Loss: 0.6709\n",
      "Epoch: 2, Batch(60/245), Loss: 0.6707\n",
      "Epoch: 2, Batch(80/245), Loss: 0.6978\n",
      "Epoch: 2, Batch(100/245), Loss: 0.5827\n",
      "Epoch: 2, Batch(120/245), Loss: 0.6955\n",
      "Epoch: 2, Batch(140/245), Loss: 0.5768\n",
      "Epoch: 2, Batch(160/245), Loss: 0.6031\n",
      "Epoch: 2, Batch(180/245), Loss: 0.5646\n",
      "Epoch: 2, Batch(200/245), Loss: 0.5724\n",
      "Epoch: 2, Batch(220/245), Loss: 0.5930\n",
      "Epoch: 2, Batch(240/245), Loss: 0.5430\n",
      "epoch 2 duration 1.707 train_loss 0.617 val_loss 0.457 val_acc 0.8821\n",
      "Epoch: 3, Batch(20/245), Loss: 0.4757\n",
      "Epoch: 3, Batch(40/245), Loss: 0.4312\n",
      "Epoch: 3, Batch(60/245), Loss: 0.4341\n",
      "Epoch: 3, Batch(80/245), Loss: 0.4479\n",
      "Epoch: 3, Batch(100/245), Loss: 0.4490\n",
      "Epoch: 3, Batch(120/245), Loss: 0.4825\n",
      "Epoch: 3, Batch(140/245), Loss: 0.5049\n",
      "Epoch: 3, Batch(160/245), Loss: 0.4830\n",
      "Epoch: 3, Batch(180/245), Loss: 0.4461\n",
      "Epoch: 3, Batch(200/245), Loss: 0.3868\n",
      "Epoch: 3, Batch(220/245), Loss: 0.4169\n",
      "Epoch: 3, Batch(240/245), Loss: 0.5176\n",
      "epoch 3 duration 1.707 train_loss 0.457 val_loss 0.431 val_acc 0.8952\n",
      "Epoch: 4, Batch(20/245), Loss: 0.3233\n",
      "Epoch: 4, Batch(40/245), Loss: 0.3708\n",
      "Epoch: 4, Batch(60/245), Loss: 0.3641\n",
      "Epoch: 4, Batch(80/245), Loss: 0.4209\n",
      "Epoch: 4, Batch(100/245), Loss: 0.3419\n",
      "Epoch: 4, Batch(120/245), Loss: 0.4049\n",
      "Epoch: 4, Batch(140/245), Loss: 0.4270\n",
      "Epoch: 4, Batch(160/245), Loss: 0.4210\n",
      "Epoch: 4, Batch(180/245), Loss: 0.3062\n",
      "Epoch: 4, Batch(200/245), Loss: 0.3721\n",
      "Epoch: 4, Batch(220/245), Loss: 0.3023\n",
      "Epoch: 4, Batch(240/245), Loss: 0.3067\n",
      "epoch 4 duration 1.710 train_loss 0.362 val_loss 0.303 val_acc 0.9262\n",
      "Epoch: 5, Batch(20/245), Loss: 0.2894\n",
      "Epoch: 5, Batch(40/245), Loss: 0.2795\n",
      "Epoch: 5, Batch(60/245), Loss: 0.3540\n",
      "Epoch: 5, Batch(80/245), Loss: 0.2730\n",
      "Epoch: 5, Batch(100/245), Loss: 0.3618\n",
      "Epoch: 5, Batch(120/245), Loss: 0.2524\n",
      "Epoch: 5, Batch(140/245), Loss: 0.3941\n",
      "Epoch: 5, Batch(160/245), Loss: 0.2622\n",
      "Epoch: 5, Batch(180/245), Loss: 0.3336\n",
      "Epoch: 5, Batch(200/245), Loss: 0.2834\n",
      "Epoch: 5, Batch(220/245), Loss: 0.3072\n",
      "Epoch: 5, Batch(240/245), Loss: 0.2918\n",
      "epoch 5 duration 1.710 train_loss 0.312 val_loss 0.342 val_acc 0.8988\n",
      "Epoch: 6, Batch(20/245), Loss: 0.2998\n",
      "Epoch: 6, Batch(40/245), Loss: 0.2716\n",
      "Epoch: 6, Batch(60/245), Loss: 0.2198\n",
      "Epoch: 6, Batch(80/245), Loss: 0.2604\n",
      "Epoch: 6, Batch(100/245), Loss: 0.2557\n",
      "Epoch: 6, Batch(120/245), Loss: 0.2433\n",
      "Epoch: 6, Batch(140/245), Loss: 0.2781\n",
      "Epoch: 6, Batch(160/245), Loss: 0.2476\n",
      "Epoch: 6, Batch(180/245), Loss: 0.2430\n",
      "Epoch: 6, Batch(200/245), Loss: 0.2445\n",
      "Epoch: 6, Batch(220/245), Loss: 0.2815\n",
      "Epoch: 6, Batch(240/245), Loss: 0.2543\n",
      "epoch 6 duration 1.708 train_loss 0.262 val_loss 0.330 val_acc 0.9119\n",
      "Epoch: 7, Batch(20/245), Loss: 0.2374\n",
      "Epoch: 7, Batch(40/245), Loss: 0.2729\n",
      "Epoch: 7, Batch(60/245), Loss: 0.2716\n",
      "Epoch: 7, Batch(80/245), Loss: 0.2622\n",
      "Epoch: 7, Batch(100/245), Loss: 0.2193\n",
      "Epoch: 7, Batch(120/245), Loss: 0.2730\n",
      "Epoch: 7, Batch(140/245), Loss: 0.2017\n",
      "Epoch: 7, Batch(160/245), Loss: 0.1941\n",
      "Epoch: 7, Batch(180/245), Loss: 0.2422\n",
      "Epoch: 7, Batch(200/245), Loss: 0.2320\n",
      "Epoch: 7, Batch(220/245), Loss: 0.2383\n",
      "Epoch: 7, Batch(240/245), Loss: 0.2746\n",
      "epoch 7 duration 1.713 train_loss 0.245 val_loss 0.292 val_acc 0.9250\n",
      "Epoch: 8, Batch(20/245), Loss: 0.2143\n",
      "Epoch: 8, Batch(40/245), Loss: 0.1963\n",
      "Epoch: 8, Batch(60/245), Loss: 0.2255\n",
      "Epoch: 8, Batch(80/245), Loss: 0.2246\n",
      "Epoch: 8, Batch(100/245), Loss: 0.2075\n",
      "Epoch: 8, Batch(120/245), Loss: 0.2399\n",
      "Epoch: 8, Batch(140/245), Loss: 0.3089\n",
      "Epoch: 8, Batch(160/245), Loss: 0.2920\n",
      "Epoch: 8, Batch(180/245), Loss: 0.2554\n",
      "Epoch: 8, Batch(200/245), Loss: 0.2427\n",
      "Epoch: 8, Batch(220/245), Loss: 0.2743\n",
      "Epoch: 8, Batch(240/245), Loss: 0.1962\n",
      "epoch 8 duration 1.702 train_loss 0.239 val_loss 0.246 val_acc 0.9417\n",
      "Epoch: 9, Batch(20/245), Loss: 0.1460\n",
      "Epoch: 9, Batch(40/245), Loss: 0.2277\n",
      "Epoch: 9, Batch(60/245), Loss: 0.2398\n",
      "Epoch: 9, Batch(80/245), Loss: 0.2369\n",
      "Epoch: 9, Batch(100/245), Loss: 0.1836\n",
      "Epoch: 9, Batch(120/245), Loss: 0.1970\n",
      "Epoch: 9, Batch(140/245), Loss: 0.2077\n",
      "Epoch: 9, Batch(160/245), Loss: 0.1840\n",
      "Epoch: 9, Batch(180/245), Loss: 0.1575\n",
      "Epoch: 9, Batch(200/245), Loss: 0.2347\n",
      "Epoch: 9, Batch(220/245), Loss: 0.1696\n",
      "Epoch: 9, Batch(240/245), Loss: 0.1930\n",
      "epoch 9 duration 1.712 train_loss 0.197 val_loss 0.295 val_acc 0.9310\n",
      "Epoch: 10, Batch(20/245), Loss: 0.1868\n",
      "Epoch: 10, Batch(40/245), Loss: 0.2149\n",
      "Epoch: 10, Batch(60/245), Loss: 0.2301\n",
      "Epoch: 10, Batch(80/245), Loss: 0.1963\n",
      "Epoch: 10, Batch(100/245), Loss: 0.1591\n",
      "Epoch: 10, Batch(120/245), Loss: 0.1665\n",
      "Epoch: 10, Batch(140/245), Loss: 0.2046\n",
      "Epoch: 10, Batch(160/245), Loss: 0.1716\n",
      "Epoch: 10, Batch(180/245), Loss: 0.1873\n",
      "Epoch: 10, Batch(200/245), Loss: 0.1592\n",
      "Epoch: 10, Batch(220/245), Loss: 0.1650\n",
      "Epoch: 10, Batch(240/245), Loss: 0.1626\n",
      "epoch 10 duration 1.711 train_loss 0.185 val_loss 0.259 val_acc 0.9238\n",
      "Epoch: 11, Batch(20/245), Loss: 0.1811\n",
      "Epoch: 11, Batch(40/245), Loss: 0.1313\n",
      "Epoch: 11, Batch(60/245), Loss: 0.1911\n",
      "Epoch: 11, Batch(80/245), Loss: 0.1854\n",
      "Epoch: 11, Batch(100/245), Loss: 0.2005\n",
      "Epoch: 11, Batch(120/245), Loss: 0.1561\n",
      "Epoch: 11, Batch(140/245), Loss: 0.1294\n",
      "Epoch: 11, Batch(160/245), Loss: 0.1788\n",
      "Epoch: 11, Batch(180/245), Loss: 0.1726\n",
      "Epoch: 11, Batch(200/245), Loss: 0.1882\n",
      "Epoch: 11, Batch(220/245), Loss: 0.1571\n",
      "Epoch: 11, Batch(240/245), Loss: 0.1623\n",
      "epoch 11 duration 1.706 train_loss 0.173 val_loss 0.237 val_acc 0.9357\n",
      "Epoch: 12, Batch(20/245), Loss: 0.1190\n",
      "Epoch: 12, Batch(40/245), Loss: 0.1244\n",
      "Epoch: 12, Batch(60/245), Loss: 0.1452\n",
      "Epoch: 12, Batch(80/245), Loss: 0.2446\n",
      "Epoch: 12, Batch(100/245), Loss: 0.1919\n",
      "Epoch: 12, Batch(120/245), Loss: 0.1822\n",
      "Epoch: 12, Batch(140/245), Loss: 0.1628\n",
      "Epoch: 12, Batch(160/245), Loss: 0.1910\n",
      "Epoch: 12, Batch(180/245), Loss: 0.1775\n",
      "Epoch: 12, Batch(200/245), Loss: 0.1576\n",
      "Epoch: 12, Batch(220/245), Loss: 0.1879\n",
      "Epoch: 12, Batch(240/245), Loss: 0.2035\n",
      "epoch 12 duration 1.708 train_loss 0.173 val_loss 0.257 val_acc 0.9357\n",
      "Epoch: 13, Batch(20/245), Loss: 0.1962\n",
      "Epoch: 13, Batch(40/245), Loss: 0.1355\n",
      "Epoch: 13, Batch(60/245), Loss: 0.1899\n",
      "Epoch: 13, Batch(80/245), Loss: 0.1305\n",
      "Epoch: 13, Batch(100/245), Loss: 0.1936\n",
      "Epoch: 13, Batch(120/245), Loss: 0.2521\n",
      "Epoch: 13, Batch(140/245), Loss: 0.1660\n",
      "Epoch: 13, Batch(160/245), Loss: 0.1427\n",
      "Epoch: 13, Batch(180/245), Loss: 0.1550\n",
      "Epoch: 13, Batch(200/245), Loss: 0.1701\n",
      "Epoch: 13, Batch(220/245), Loss: 0.1681\n",
      "Epoch: 13, Batch(240/245), Loss: 0.1573\n",
      "epoch 13 duration 1.708 train_loss 0.174 val_loss 0.319 val_acc 0.9143\n",
      "Epoch: 14, Batch(20/245), Loss: 0.1208\n",
      "Epoch: 14, Batch(40/245), Loss: 0.1327\n",
      "Epoch: 14, Batch(60/245), Loss: 0.1279\n",
      "Epoch: 14, Batch(80/245), Loss: 0.1387\n",
      "Epoch: 14, Batch(100/245), Loss: 0.1424\n",
      "Epoch: 14, Batch(120/245), Loss: 0.1175\n",
      "Epoch: 14, Batch(140/245), Loss: 0.1134\n",
      "Epoch: 14, Batch(160/245), Loss: 0.1575\n",
      "Epoch: 14, Batch(180/245), Loss: 0.1713\n",
      "Epoch: 14, Batch(200/245), Loss: 0.1288\n",
      "Epoch: 14, Batch(220/245), Loss: 0.1389\n",
      "Epoch: 14, Batch(240/245), Loss: 0.1834\n",
      "epoch 14 duration 1.711 train_loss 0.138 val_loss 0.400 val_acc 0.8893\n",
      "Epoch: 15, Batch(20/245), Loss: 0.1687\n",
      "Epoch: 15, Batch(40/245), Loss: 0.1640\n",
      "Epoch: 15, Batch(60/245), Loss: 0.1844\n",
      "Epoch: 15, Batch(80/245), Loss: 0.1848\n",
      "Epoch: 15, Batch(100/245), Loss: 0.2002\n",
      "Epoch: 15, Batch(120/245), Loss: 0.1678\n",
      "Epoch: 15, Batch(140/245), Loss: 0.1334\n",
      "Epoch: 15, Batch(160/245), Loss: 0.1860\n",
      "Epoch: 15, Batch(180/245), Loss: 0.2067\n",
      "Epoch: 15, Batch(200/245), Loss: 0.2659\n",
      "Epoch: 15, Batch(220/245), Loss: 0.1623\n",
      "Epoch: 15, Batch(240/245), Loss: 0.1229\n",
      "epoch 15 duration 1.708 train_loss 0.177 val_loss 0.295 val_acc 0.9143\n",
      "Epoch: 16, Batch(20/245), Loss: 0.1350\n",
      "Epoch: 16, Batch(40/245), Loss: 0.1435\n",
      "Epoch: 16, Batch(60/245), Loss: 0.1208\n",
      "Epoch: 16, Batch(80/245), Loss: 0.1933\n",
      "Epoch: 16, Batch(100/245), Loss: 0.1256\n",
      "Epoch: 16, Batch(120/245), Loss: 0.1377\n",
      "Epoch: 16, Batch(140/245), Loss: 0.1412\n",
      "Epoch: 16, Batch(160/245), Loss: 0.1989\n",
      "Epoch: 16, Batch(180/245), Loss: 0.1454\n",
      "Epoch: 16, Batch(200/245), Loss: 0.1384\n",
      "Epoch: 16, Batch(220/245), Loss: 0.1345\n",
      "Epoch: 16, Batch(240/245), Loss: 0.2253\n",
      "epoch 16 duration 1.705 train_loss 0.153 val_loss 0.365 val_acc 0.9083\n",
      "Epoch: 17, Batch(20/245), Loss: 0.1388\n",
      "Epoch: 17, Batch(40/245), Loss: 0.1066\n",
      "Epoch: 17, Batch(60/245), Loss: 0.1253\n",
      "Epoch: 17, Batch(80/245), Loss: 0.1488\n",
      "Epoch: 17, Batch(100/245), Loss: 0.1038\n",
      "Epoch: 17, Batch(120/245), Loss: 0.1319\n",
      "Epoch: 17, Batch(140/245), Loss: 0.1841\n",
      "Epoch: 17, Batch(160/245), Loss: 0.1738\n",
      "Epoch: 17, Batch(180/245), Loss: 0.1621\n",
      "Epoch: 17, Batch(200/245), Loss: 0.1362\n",
      "Epoch: 17, Batch(220/245), Loss: 0.2158\n",
      "Epoch: 17, Batch(240/245), Loss: 0.1886\n",
      "epoch 17 duration 1.707 train_loss 0.151 val_loss 0.257 val_acc 0.9345\n",
      "Epoch: 18, Batch(20/245), Loss: 0.1613\n",
      "Epoch: 18, Batch(40/245), Loss: 0.2103\n",
      "Epoch: 18, Batch(60/245), Loss: 0.1255\n",
      "Epoch: 18, Batch(80/245), Loss: 0.1539\n",
      "Epoch: 18, Batch(100/245), Loss: 0.1840\n",
      "Epoch: 18, Batch(120/245), Loss: 0.1321\n",
      "Epoch: 18, Batch(140/245), Loss: 0.1468\n",
      "Epoch: 18, Batch(160/245), Loss: 0.1163\n",
      "Epoch: 18, Batch(180/245), Loss: 0.1530\n",
      "Epoch: 18, Batch(200/245), Loss: 0.1235\n",
      "Epoch: 18, Batch(220/245), Loss: 0.1735\n",
      "Epoch: 18, Batch(240/245), Loss: 0.1725\n",
      "epoch 18 duration 1.707 train_loss 0.153 val_loss 0.256 val_acc 0.9345\n",
      "Epoch: 19, Batch(20/245), Loss: 0.1042\n",
      "Epoch: 19, Batch(40/245), Loss: 0.1115\n",
      "Epoch: 19, Batch(60/245), Loss: 0.0939\n",
      "Epoch: 19, Batch(80/245), Loss: 0.0822\n",
      "Epoch: 19, Batch(100/245), Loss: 0.1094\n",
      "Epoch: 19, Batch(120/245), Loss: 0.1044\n",
      "Epoch: 19, Batch(140/245), Loss: 0.1297\n",
      "Epoch: 19, Batch(160/245), Loss: 0.1274\n",
      "Epoch: 19, Batch(180/245), Loss: 0.1379\n",
      "Epoch: 19, Batch(200/245), Loss: 0.1300\n",
      "Epoch: 19, Batch(220/245), Loss: 0.1451\n",
      "Epoch: 19, Batch(240/245), Loss: 0.1887\n",
      "epoch 19 duration 1.709 train_loss 0.125 val_loss 0.217 val_acc 0.9524\n",
      "Epoch: 20, Batch(20/245), Loss: 0.1034\n",
      "Epoch: 20, Batch(40/245), Loss: 0.1489\n",
      "Epoch: 20, Batch(60/245), Loss: 0.1562\n",
      "Epoch: 20, Batch(80/245), Loss: 0.0918\n",
      "Epoch: 20, Batch(100/245), Loss: 0.0946\n",
      "Epoch: 20, Batch(120/245), Loss: 0.0696\n",
      "Epoch: 20, Batch(140/245), Loss: 0.0897\n",
      "Epoch: 20, Batch(160/245), Loss: 0.1037\n",
      "Epoch: 20, Batch(180/245), Loss: 0.1191\n",
      "Epoch: 20, Batch(200/245), Loss: 0.1103\n",
      "Epoch: 20, Batch(220/245), Loss: 0.1256\n",
      "Epoch: 20, Batch(240/245), Loss: 0.1143\n",
      "epoch 20 duration 1.708 train_loss 0.113 val_loss 0.317 val_acc 0.9250\n",
      "Epoch: 21, Batch(20/245), Loss: 0.1488\n",
      "Epoch: 21, Batch(40/245), Loss: 0.0881\n",
      "Epoch: 21, Batch(60/245), Loss: 0.1188\n",
      "Epoch: 21, Batch(80/245), Loss: 0.1935\n",
      "Epoch: 21, Batch(100/245), Loss: 0.1796\n",
      "Epoch: 21, Batch(120/245), Loss: 0.2429\n",
      "Epoch: 21, Batch(140/245), Loss: 0.1200\n",
      "Epoch: 21, Batch(160/245), Loss: 0.1702\n",
      "Epoch: 21, Batch(180/245), Loss: 0.1461\n",
      "Epoch: 21, Batch(200/245), Loss: 0.1464\n",
      "Epoch: 21, Batch(220/245), Loss: 0.1284\n",
      "Epoch: 21, Batch(240/245), Loss: 0.1195\n",
      "epoch 21 duration 1.707 train_loss 0.152 val_loss 0.227 val_acc 0.9381\n",
      "Epoch: 22, Batch(20/245), Loss: 0.1026\n",
      "Epoch: 22, Batch(40/245), Loss: 0.1575\n",
      "Epoch: 22, Batch(60/245), Loss: 0.1000\n",
      "Epoch: 22, Batch(80/245), Loss: 0.1826\n",
      "Epoch: 22, Batch(100/245), Loss: 0.1092\n",
      "Epoch: 22, Batch(120/245), Loss: 0.1104\n",
      "Epoch: 22, Batch(140/245), Loss: 0.1220\n",
      "Epoch: 22, Batch(160/245), Loss: 0.1606\n",
      "Epoch: 22, Batch(180/245), Loss: 0.1260\n",
      "Epoch: 22, Batch(200/245), Loss: 0.1817\n",
      "Epoch: 22, Batch(220/245), Loss: 0.1690\n",
      "Epoch: 22, Batch(240/245), Loss: 0.1438\n",
      "epoch 22 duration 1.706 train_loss 0.138 val_loss 0.184 val_acc 0.9500\n",
      "Epoch: 23, Batch(20/245), Loss: 0.1367\n",
      "Epoch: 23, Batch(40/245), Loss: 0.1289\n",
      "Epoch: 23, Batch(60/245), Loss: 0.1404\n",
      "Epoch: 23, Batch(80/245), Loss: 0.0991\n",
      "Epoch: 23, Batch(100/245), Loss: 0.0953\n",
      "Epoch: 23, Batch(120/245), Loss: 0.0922\n",
      "Epoch: 23, Batch(140/245), Loss: 0.1287\n",
      "Epoch: 23, Batch(160/245), Loss: 0.1599\n",
      "Epoch: 23, Batch(180/245), Loss: 0.1673\n",
      "Epoch: 23, Batch(200/245), Loss: 0.1856\n",
      "Epoch: 23, Batch(220/245), Loss: 0.1567\n",
      "Epoch: 23, Batch(240/245), Loss: 0.1361\n",
      "epoch 23 duration 1.711 train_loss 0.136 val_loss 0.533 val_acc 0.8583\n",
      "Epoch: 24, Batch(20/245), Loss: 0.1662\n",
      "Epoch: 24, Batch(40/245), Loss: 0.1618\n",
      "Epoch: 24, Batch(60/245), Loss: 0.1229\n",
      "Epoch: 24, Batch(80/245), Loss: 0.1082\n",
      "Epoch: 24, Batch(100/245), Loss: 0.1531\n",
      "Epoch: 24, Batch(120/245), Loss: 0.1031\n",
      "Epoch: 24, Batch(140/245), Loss: 0.1664\n",
      "Epoch: 24, Batch(160/245), Loss: 0.1237\n",
      "Epoch: 24, Batch(180/245), Loss: 0.1973\n",
      "Epoch: 24, Batch(200/245), Loss: 0.0968\n",
      "Epoch: 24, Batch(220/245), Loss: 0.1322\n",
      "Epoch: 24, Batch(240/245), Loss: 0.1168\n",
      "epoch 24 duration 1.712 train_loss 0.138 val_loss 0.239 val_acc 0.9369\n",
      "Epoch: 25, Batch(20/245), Loss: 0.0798\n",
      "Epoch: 25, Batch(40/245), Loss: 0.1029\n",
      "Epoch: 25, Batch(60/245), Loss: 0.0915\n",
      "Epoch: 25, Batch(80/245), Loss: 0.1053\n",
      "Epoch: 25, Batch(100/245), Loss: 0.1426\n",
      "Epoch: 25, Batch(120/245), Loss: 0.1197\n",
      "Epoch: 25, Batch(140/245), Loss: 0.1951\n",
      "Epoch: 25, Batch(160/245), Loss: 0.1027\n",
      "Epoch: 25, Batch(180/245), Loss: 0.1158\n",
      "Epoch: 25, Batch(200/245), Loss: 0.1382\n",
      "Epoch: 25, Batch(220/245), Loss: 0.1152\n",
      "Epoch: 25, Batch(240/245), Loss: 0.1330\n",
      "epoch 25 duration 1.707 train_loss 0.120 val_loss 0.207 val_acc 0.9500\n",
      "Epoch: 26, Batch(20/245), Loss: 0.0894\n",
      "Epoch: 26, Batch(40/245), Loss: 0.1066\n",
      "Epoch: 26, Batch(60/245), Loss: 0.1507\n",
      "Epoch: 26, Batch(80/245), Loss: 0.1656\n",
      "Epoch: 26, Batch(100/245), Loss: 0.1197\n",
      "Epoch: 26, Batch(120/245), Loss: 0.1166\n",
      "Epoch: 26, Batch(140/245), Loss: 0.1714\n",
      "Epoch: 26, Batch(160/245), Loss: 0.1390\n",
      "Epoch: 26, Batch(180/245), Loss: 0.1238\n",
      "Epoch: 26, Batch(200/245), Loss: 0.1166\n",
      "Epoch: 26, Batch(220/245), Loss: 0.1534\n",
      "Epoch: 26, Batch(240/245), Loss: 0.1080\n",
      "epoch 26 duration 1.708 train_loss 0.130 val_loss 0.243 val_acc 0.9369\n",
      "Epoch: 27, Batch(20/245), Loss: 0.0733\n",
      "Epoch: 27, Batch(40/245), Loss: 0.0791\n",
      "Epoch: 27, Batch(60/245), Loss: 0.0979\n",
      "Epoch: 27, Batch(80/245), Loss: 0.1084\n",
      "Epoch: 27, Batch(100/245), Loss: 0.1192\n",
      "Epoch: 27, Batch(120/245), Loss: 0.1329\n",
      "Epoch: 27, Batch(140/245), Loss: 0.1870\n",
      "Epoch: 27, Batch(160/245), Loss: 0.1404\n",
      "Epoch: 27, Batch(180/245), Loss: 0.1472\n",
      "Epoch: 27, Batch(200/245), Loss: 0.1606\n",
      "Epoch: 27, Batch(220/245), Loss: 0.0963\n",
      "Epoch: 27, Batch(240/245), Loss: 0.1137\n",
      "epoch 27 duration 1.705 train_loss 0.121 val_loss 0.239 val_acc 0.9440\n",
      "Epoch: 28, Batch(20/245), Loss: 0.0986\n",
      "Epoch: 28, Batch(40/245), Loss: 0.0943\n",
      "Epoch: 28, Batch(60/245), Loss: 0.1135\n",
      "Epoch: 28, Batch(80/245), Loss: 0.1297\n",
      "Epoch: 28, Batch(100/245), Loss: 0.0962\n",
      "Epoch: 28, Batch(120/245), Loss: 0.1318\n",
      "Epoch: 28, Batch(140/245), Loss: 0.1733\n",
      "Epoch: 28, Batch(160/245), Loss: 0.0988\n",
      "Epoch: 28, Batch(180/245), Loss: 0.1049\n",
      "Epoch: 28, Batch(200/245), Loss: 0.1484\n",
      "Epoch: 28, Batch(220/245), Loss: 0.1498\n",
      "Epoch: 28, Batch(240/245), Loss: 0.1491\n",
      "epoch 28 duration 1.707 train_loss 0.124 val_loss 0.219 val_acc 0.9512\n",
      "Epoch: 29, Batch(20/245), Loss: 0.1063\n",
      "Epoch: 29, Batch(40/245), Loss: 0.1030\n",
      "Epoch: 29, Batch(60/245), Loss: 0.1007\n",
      "Epoch: 29, Batch(80/245), Loss: 0.1184\n",
      "Epoch: 29, Batch(100/245), Loss: 0.1397\n",
      "Epoch: 29, Batch(120/245), Loss: 0.1221\n",
      "Epoch: 29, Batch(140/245), Loss: 0.0730\n",
      "Epoch: 29, Batch(160/245), Loss: 0.1272\n",
      "Epoch: 29, Batch(180/245), Loss: 0.1030\n",
      "Epoch: 29, Batch(200/245), Loss: 0.1075\n",
      "Epoch: 29, Batch(220/245), Loss: 0.1151\n",
      "Epoch: 29, Batch(240/245), Loss: 0.1026\n",
      "epoch 29 duration 1.706 train_loss 0.110 val_loss 0.253 val_acc 0.9345\n",
      "Epoch: 30, Batch(20/245), Loss: 0.1678\n",
      "Epoch: 30, Batch(40/245), Loss: 0.0856\n",
      "Epoch: 30, Batch(60/245), Loss: 0.1239\n",
      "Epoch: 30, Batch(80/245), Loss: 0.0676\n",
      "Epoch: 30, Batch(100/245), Loss: 0.0887\n",
      "Epoch: 30, Batch(120/245), Loss: 0.0872\n",
      "Epoch: 30, Batch(140/245), Loss: 0.0850\n",
      "Epoch: 30, Batch(160/245), Loss: 0.1860\n",
      "Epoch: 30, Batch(180/245), Loss: 0.1704\n",
      "Epoch: 30, Batch(200/245), Loss: 0.1177\n",
      "Epoch: 30, Batch(220/245), Loss: 0.1540\n",
      "Epoch: 30, Batch(240/245), Loss: 0.1234\n",
      "epoch 30 duration 1.711 train_loss 0.124 val_loss 0.261 val_acc 0.9357\n",
      "Epoch: 31, Batch(20/245), Loss: 0.1026\n",
      "Epoch: 31, Batch(40/245), Loss: 0.1680\n",
      "Epoch: 31, Batch(60/245), Loss: 0.2233\n",
      "Epoch: 31, Batch(80/245), Loss: 0.1469\n",
      "Epoch: 31, Batch(100/245), Loss: 0.1134\n",
      "Epoch: 31, Batch(120/245), Loss: 0.0950\n",
      "Epoch: 31, Batch(140/245), Loss: 0.1369\n",
      "Epoch: 31, Batch(160/245), Loss: 0.1229\n",
      "Epoch: 31, Batch(180/245), Loss: 0.1400\n",
      "Epoch: 31, Batch(200/245), Loss: 0.1350\n",
      "Epoch: 31, Batch(220/245), Loss: 0.0928\n",
      "Epoch: 31, Batch(240/245), Loss: 0.0803\n",
      "epoch 31 duration 1.708 train_loss 0.129 val_loss 0.242 val_acc 0.9333\n",
      "Epoch: 32, Batch(20/245), Loss: 0.1597\n",
      "Epoch: 32, Batch(40/245), Loss: 0.1739\n",
      "Epoch: 32, Batch(60/245), Loss: 0.1183\n",
      "Epoch: 32, Batch(80/245), Loss: 0.1165\n",
      "Epoch: 32, Batch(100/245), Loss: 0.1369\n",
      "Epoch: 32, Batch(120/245), Loss: 0.1560\n",
      "Epoch: 32, Batch(140/245), Loss: 0.0903\n",
      "Epoch: 32, Batch(160/245), Loss: 0.2119\n",
      "Epoch: 32, Batch(180/245), Loss: 0.1517\n",
      "Epoch: 32, Batch(200/245), Loss: 0.1904\n",
      "Epoch: 32, Batch(220/245), Loss: 0.1668\n",
      "Epoch: 32, Batch(240/245), Loss: 0.1126\n",
      "epoch 32 duration 1.706 train_loss 0.149 val_loss 0.185 val_acc 0.9536\n",
      "Epoch: 33, Batch(20/245), Loss: 0.1213\n",
      "Epoch: 33, Batch(40/245), Loss: 0.1073\n",
      "Epoch: 33, Batch(60/245), Loss: 0.1889\n",
      "Epoch: 33, Batch(80/245), Loss: 0.1713\n",
      "Epoch: 33, Batch(100/245), Loss: 0.1369\n",
      "Epoch: 33, Batch(120/245), Loss: 0.1136\n",
      "Epoch: 33, Batch(140/245), Loss: 0.0984\n",
      "Epoch: 33, Batch(160/245), Loss: 0.0657\n",
      "Epoch: 33, Batch(180/245), Loss: 0.1554\n",
      "Epoch: 33, Batch(200/245), Loss: 0.1598\n",
      "Epoch: 33, Batch(220/245), Loss: 0.1451\n",
      "Epoch: 33, Batch(240/245), Loss: 0.0919\n",
      "epoch 33 duration 1.707 train_loss 0.129 val_loss 0.329 val_acc 0.9179\n",
      "Epoch: 34, Batch(20/245), Loss: 0.0842\n",
      "Epoch: 34, Batch(40/245), Loss: 0.0758\n",
      "Epoch: 34, Batch(60/245), Loss: 0.0862\n",
      "Epoch: 34, Batch(80/245), Loss: 0.0743\n",
      "Epoch: 34, Batch(100/245), Loss: 0.0560\n",
      "Epoch: 34, Batch(120/245), Loss: 0.1054\n",
      "Epoch: 34, Batch(140/245), Loss: 0.1193\n",
      "Epoch: 34, Batch(160/245), Loss: 0.1136\n",
      "Epoch: 34, Batch(180/245), Loss: 0.1080\n",
      "Epoch: 34, Batch(200/245), Loss: 0.1319\n",
      "Epoch: 34, Batch(220/245), Loss: 0.1142\n",
      "Epoch: 34, Batch(240/245), Loss: 0.0792\n",
      "epoch 34 duration 1.710 train_loss 0.096 val_loss 0.191 val_acc 0.9548\n",
      "Epoch: 35, Batch(20/245), Loss: 0.1058\n",
      "Epoch: 35, Batch(40/245), Loss: 0.0748\n",
      "Epoch: 35, Batch(60/245), Loss: 0.0601\n",
      "Epoch: 35, Batch(80/245), Loss: 0.0505\n",
      "Epoch: 35, Batch(100/245), Loss: 0.1076\n",
      "Epoch: 35, Batch(120/245), Loss: 0.0634\n",
      "Epoch: 35, Batch(140/245), Loss: 0.1325\n",
      "Epoch: 35, Batch(160/245), Loss: 0.1613\n",
      "Epoch: 35, Batch(180/245), Loss: 0.1312\n",
      "Epoch: 35, Batch(200/245), Loss: 0.1026\n",
      "Epoch: 35, Batch(220/245), Loss: 0.1176\n",
      "Epoch: 35, Batch(240/245), Loss: 0.1744\n",
      "epoch 35 duration 1.707 train_loss 0.106 val_loss 0.263 val_acc 0.9393\n",
      "Epoch: 36, Batch(20/245), Loss: 0.0897\n",
      "Epoch: 36, Batch(40/245), Loss: 0.0710\n",
      "Epoch: 36, Batch(60/245), Loss: 0.0915\n",
      "Epoch: 36, Batch(80/245), Loss: 0.0795\n",
      "Epoch: 36, Batch(100/245), Loss: 0.1395\n",
      "Epoch: 36, Batch(120/245), Loss: 0.1482\n",
      "Epoch: 36, Batch(140/245), Loss: 0.1645\n",
      "Epoch: 36, Batch(160/245), Loss: 0.1443\n",
      "Epoch: 36, Batch(180/245), Loss: 0.1209\n",
      "Epoch: 36, Batch(200/245), Loss: 0.1250\n",
      "Epoch: 36, Batch(220/245), Loss: 0.2190\n",
      "Epoch: 36, Batch(240/245), Loss: 0.1235\n",
      "epoch 36 duration 1.709 train_loss 0.127 val_loss 0.263 val_acc 0.9250\n",
      "Epoch: 37, Batch(20/245), Loss: 0.0642\n",
      "Epoch: 37, Batch(40/245), Loss: 0.1298\n",
      "Epoch: 37, Batch(60/245), Loss: 0.0937\n",
      "Epoch: 37, Batch(80/245), Loss: 0.1140\n",
      "Epoch: 37, Batch(100/245), Loss: 0.1153\n",
      "Epoch: 37, Batch(120/245), Loss: 0.0995\n",
      "Epoch: 37, Batch(140/245), Loss: 0.1450\n",
      "Epoch: 37, Batch(160/245), Loss: 0.1743\n",
      "Epoch: 37, Batch(180/245), Loss: 0.1561\n",
      "Epoch: 37, Batch(200/245), Loss: 0.1125\n",
      "Epoch: 37, Batch(220/245), Loss: 0.0915\n",
      "Epoch: 37, Batch(240/245), Loss: 0.0589\n",
      "epoch 37 duration 1.706 train_loss 0.113 val_loss 0.187 val_acc 0.9548\n",
      "Epoch: 38, Batch(20/245), Loss: 0.1040\n",
      "Epoch: 38, Batch(40/245), Loss: 0.0960\n",
      "Epoch: 38, Batch(60/245), Loss: 0.1317\n",
      "Epoch: 38, Batch(80/245), Loss: 0.0879\n",
      "Epoch: 38, Batch(100/245), Loss: 0.1396\n",
      "Epoch: 38, Batch(120/245), Loss: 0.1525\n",
      "Epoch: 38, Batch(140/245), Loss: 0.1163\n",
      "Epoch: 38, Batch(160/245), Loss: 0.0922\n",
      "Epoch: 38, Batch(180/245), Loss: 0.0715\n",
      "Epoch: 38, Batch(200/245), Loss: 0.1195\n",
      "Epoch: 38, Batch(220/245), Loss: 0.1158\n",
      "Epoch: 38, Batch(240/245), Loss: 0.1923\n",
      "epoch 38 duration 1.703 train_loss 0.118 val_loss 0.211 val_acc 0.9464\n",
      "Epoch: 39, Batch(20/245), Loss: 0.1015\n",
      "Epoch: 39, Batch(40/245), Loss: 0.1101\n",
      "Epoch: 39, Batch(60/245), Loss: 0.1306\n",
      "Epoch: 39, Batch(80/245), Loss: 0.0621\n",
      "Epoch: 39, Batch(100/245), Loss: 0.0657\n",
      "Epoch: 39, Batch(120/245), Loss: 0.1027\n",
      "Epoch: 39, Batch(140/245), Loss: 0.1244\n",
      "Epoch: 39, Batch(160/245), Loss: 0.0994\n",
      "Epoch: 39, Batch(180/245), Loss: 0.1323\n",
      "Epoch: 39, Batch(200/245), Loss: 0.1364\n",
      "Epoch: 39, Batch(220/245), Loss: 0.1810\n",
      "Epoch: 39, Batch(240/245), Loss: 0.2054\n",
      "epoch 39 duration 1.704 train_loss 0.121 val_loss 0.186 val_acc 0.9536\n",
      "Epoch: 40, Batch(20/245), Loss: 0.1801\n",
      "Epoch: 40, Batch(40/245), Loss: 0.1198\n",
      "Epoch: 40, Batch(60/245), Loss: 0.0783\n",
      "Epoch: 40, Batch(80/245), Loss: 0.1008\n",
      "Epoch: 40, Batch(100/245), Loss: 0.1009\n",
      "Epoch: 40, Batch(120/245), Loss: 0.1137\n",
      "Epoch: 40, Batch(140/245), Loss: 0.1527\n",
      "Epoch: 40, Batch(160/245), Loss: 0.1831\n",
      "Epoch: 40, Batch(180/245), Loss: 0.2453\n",
      "Epoch: 40, Batch(200/245), Loss: 0.1212\n",
      "Epoch: 40, Batch(220/245), Loss: 0.1308\n",
      "Epoch: 40, Batch(240/245), Loss: 0.1507\n",
      "epoch 40 duration 1.705 train_loss 0.139 val_loss 0.303 val_acc 0.9238\n",
      "Epoch: 41, Batch(20/245), Loss: 0.1353\n",
      "Epoch: 41, Batch(40/245), Loss: 0.1158\n",
      "Epoch: 41, Batch(60/245), Loss: 0.1571\n",
      "Epoch: 41, Batch(80/245), Loss: 0.1567\n",
      "Epoch: 41, Batch(100/245), Loss: 0.0962\n",
      "Epoch: 41, Batch(120/245), Loss: 0.1169\n",
      "Epoch: 41, Batch(140/245), Loss: 0.1318\n",
      "Epoch: 41, Batch(160/245), Loss: 0.1253\n",
      "Epoch: 41, Batch(180/245), Loss: 0.0748\n",
      "Epoch: 41, Batch(200/245), Loss: 0.0857\n",
      "Epoch: 41, Batch(220/245), Loss: 0.0912\n",
      "Epoch: 41, Batch(240/245), Loss: 0.0771\n",
      "epoch 41 duration 1.706 train_loss 0.113 val_loss 0.193 val_acc 0.9488\n",
      "Epoch: 42, Batch(20/245), Loss: 0.0908\n",
      "Epoch: 42, Batch(40/245), Loss: 0.0589\n",
      "Epoch: 42, Batch(60/245), Loss: 0.0901\n",
      "Epoch: 42, Batch(80/245), Loss: 0.0840\n",
      "Epoch: 42, Batch(100/245), Loss: 0.1138\n",
      "Epoch: 42, Batch(120/245), Loss: 0.1432\n",
      "Epoch: 42, Batch(140/245), Loss: 0.1200\n",
      "Epoch: 42, Batch(160/245), Loss: 0.1217\n",
      "Epoch: 42, Batch(180/245), Loss: 0.1302\n",
      "Epoch: 42, Batch(200/245), Loss: 0.1233\n",
      "Epoch: 42, Batch(220/245), Loss: 0.1209\n",
      "Epoch: 42, Batch(240/245), Loss: 0.1407\n",
      "epoch 42 duration 1.708 train_loss 0.112 val_loss 0.229 val_acc 0.9345\n",
      "Epoch: 43, Batch(20/245), Loss: 0.0597\n",
      "Epoch: 43, Batch(40/245), Loss: 0.1044\n",
      "Epoch: 43, Batch(60/245), Loss: 0.1037\n",
      "Epoch: 43, Batch(80/245), Loss: 0.1245\n",
      "Epoch: 43, Batch(100/245), Loss: 0.1172\n",
      "Epoch: 43, Batch(120/245), Loss: 0.1349\n",
      "Epoch: 43, Batch(140/245), Loss: 0.1289\n",
      "Epoch: 43, Batch(160/245), Loss: 0.1935\n",
      "Epoch: 43, Batch(180/245), Loss: 0.1293\n",
      "Epoch: 43, Batch(200/245), Loss: 0.0980\n",
      "Epoch: 43, Batch(220/245), Loss: 0.0965\n",
      "Epoch: 43, Batch(240/245), Loss: 0.0957\n",
      "epoch 43 duration 1.707 train_loss 0.115 val_loss 0.190 val_acc 0.9500\n",
      "Epoch: 44, Batch(20/245), Loss: 0.1108\n",
      "Epoch: 44, Batch(40/245), Loss: 0.0990\n",
      "Epoch: 44, Batch(60/245), Loss: 0.1248\n",
      "Epoch: 44, Batch(80/245), Loss: 0.0927\n",
      "Epoch: 44, Batch(100/245), Loss: 0.1123\n",
      "Epoch: 44, Batch(120/245), Loss: 0.1588\n",
      "Epoch: 44, Batch(140/245), Loss: 0.0944\n",
      "Epoch: 44, Batch(160/245), Loss: 0.1018\n",
      "Epoch: 44, Batch(180/245), Loss: 0.0732\n",
      "Epoch: 44, Batch(200/245), Loss: 0.0651\n",
      "Epoch: 44, Batch(220/245), Loss: 0.0813\n",
      "Epoch: 44, Batch(240/245), Loss: 0.1183\n",
      "epoch 44 duration 1.706 train_loss 0.105 val_loss 0.286 val_acc 0.9298\n",
      "Epoch: 45, Batch(20/245), Loss: 0.1497\n",
      "Epoch: 45, Batch(40/245), Loss: 0.0751\n",
      "Epoch: 45, Batch(60/245), Loss: 0.0684\n",
      "Epoch: 45, Batch(80/245), Loss: 0.0391\n",
      "Epoch: 45, Batch(100/245), Loss: 0.0810\n",
      "Epoch: 45, Batch(120/245), Loss: 0.0830\n",
      "Epoch: 45, Batch(140/245), Loss: 0.0863\n",
      "Epoch: 45, Batch(160/245), Loss: 0.1262\n",
      "Epoch: 45, Batch(180/245), Loss: 0.1325\n",
      "Epoch: 45, Batch(200/245), Loss: 0.1092\n",
      "Epoch: 45, Batch(220/245), Loss: 0.1364\n",
      "Epoch: 45, Batch(240/245), Loss: 0.1469\n",
      "epoch 45 duration 1.707 train_loss 0.103 val_loss 0.490 val_acc 0.8750\n",
      "Epoch: 46, Batch(20/245), Loss: 0.1135\n",
      "Epoch: 46, Batch(40/245), Loss: 0.0908\n",
      "Epoch: 46, Batch(60/245), Loss: 0.0888\n",
      "Epoch: 46, Batch(80/245), Loss: 0.0778\n",
      "Epoch: 46, Batch(100/245), Loss: 0.1493\n",
      "Epoch: 46, Batch(120/245), Loss: 0.1413\n",
      "Epoch: 46, Batch(140/245), Loss: 0.0641\n",
      "Epoch: 46, Batch(160/245), Loss: 0.1100\n",
      "Epoch: 46, Batch(180/245), Loss: 0.1058\n",
      "Epoch: 46, Batch(200/245), Loss: 0.0706\n",
      "Epoch: 46, Batch(220/245), Loss: 0.1206\n",
      "Epoch: 46, Batch(240/245), Loss: 0.1740\n",
      "epoch 46 duration 1.704 train_loss 0.108 val_loss 0.218 val_acc 0.9440\n",
      "Epoch: 47, Batch(20/245), Loss: 0.1459\n",
      "Epoch: 47, Batch(40/245), Loss: 0.1962\n",
      "Epoch: 47, Batch(60/245), Loss: 0.1791\n",
      "Epoch: 47, Batch(80/245), Loss: 0.1073\n",
      "Epoch: 47, Batch(100/245), Loss: 0.1243\n",
      "Epoch: 47, Batch(120/245), Loss: 0.0680\n",
      "Epoch: 47, Batch(140/245), Loss: 0.0942\n",
      "Epoch: 47, Batch(160/245), Loss: 0.1127\n",
      "Epoch: 47, Batch(180/245), Loss: 0.1339\n",
      "Epoch: 47, Batch(200/245), Loss: 0.1359\n",
      "Epoch: 47, Batch(220/245), Loss: 0.2313\n",
      "Epoch: 47, Batch(240/245), Loss: 0.1873\n",
      "epoch 47 duration 1.705 train_loss 0.143 val_loss 0.252 val_acc 0.9405\n",
      "Epoch: 48, Batch(20/245), Loss: 0.1607\n",
      "Epoch: 48, Batch(40/245), Loss: 0.1113\n",
      "Epoch: 48, Batch(60/245), Loss: 0.1066\n",
      "Epoch: 48, Batch(80/245), Loss: 0.0758\n",
      "Epoch: 48, Batch(100/245), Loss: 0.0930\n",
      "Epoch: 48, Batch(120/245), Loss: 0.0629\n",
      "Epoch: 48, Batch(140/245), Loss: 0.0624\n",
      "Epoch: 48, Batch(160/245), Loss: 0.1408\n",
      "Epoch: 48, Batch(180/245), Loss: 0.1526\n",
      "Epoch: 48, Batch(200/245), Loss: 0.1647\n",
      "Epoch: 48, Batch(220/245), Loss: 0.1623\n",
      "Epoch: 48, Batch(240/245), Loss: 0.1886\n",
      "epoch 48 duration 1.705 train_loss 0.122 val_loss 0.170 val_acc 0.9571\n",
      "Epoch: 49, Batch(20/245), Loss: 0.1059\n",
      "Epoch: 49, Batch(40/245), Loss: 0.1835\n",
      "Epoch: 49, Batch(60/245), Loss: 0.0586\n",
      "Epoch: 49, Batch(80/245), Loss: 0.0862\n",
      "Epoch: 49, Batch(100/245), Loss: 0.1065\n",
      "Epoch: 49, Batch(120/245), Loss: 0.0995\n",
      "Epoch: 49, Batch(140/245), Loss: 0.0996\n",
      "Epoch: 49, Batch(160/245), Loss: 0.0507\n",
      "Epoch: 49, Batch(180/245), Loss: 0.0913\n",
      "Epoch: 49, Batch(200/245), Loss: 0.0913\n",
      "Epoch: 49, Batch(220/245), Loss: 0.0607\n",
      "Epoch: 49, Batch(240/245), Loss: 0.0785\n",
      "epoch 49 duration 1.708 train_loss 0.092 val_loss 0.224 val_acc 0.9369\n",
      "Epoch: 50, Batch(20/245), Loss: 0.1014\n",
      "Epoch: 50, Batch(40/245), Loss: 0.0842\n",
      "Epoch: 50, Batch(60/245), Loss: 0.1436\n",
      "Epoch: 50, Batch(80/245), Loss: 0.0927\n",
      "Epoch: 50, Batch(100/245), Loss: 0.0599\n",
      "Epoch: 50, Batch(120/245), Loss: 0.0762\n",
      "Epoch: 50, Batch(140/245), Loss: 0.0383\n",
      "Epoch: 50, Batch(160/245), Loss: 0.0967\n",
      "Epoch: 50, Batch(180/245), Loss: 0.1182\n",
      "Epoch: 50, Batch(200/245), Loss: 0.0773\n",
      "Epoch: 50, Batch(220/245), Loss: 0.0449\n",
      "Epoch: 50, Batch(240/245), Loss: 0.0663\n",
      "epoch 50 duration 1.706 train_loss 0.084 val_loss 0.150 val_acc 0.9607\n",
      "Epoch: 51, Batch(20/245), Loss: 0.0612\n",
      "Epoch: 51, Batch(40/245), Loss: 0.0650\n",
      "Epoch: 51, Batch(60/245), Loss: 0.0434\n",
      "Epoch: 51, Batch(80/245), Loss: 0.0407\n",
      "Epoch: 51, Batch(100/245), Loss: 0.0571\n",
      "Epoch: 51, Batch(120/245), Loss: 0.0690\n",
      "Epoch: 51, Batch(140/245), Loss: 0.0501\n",
      "Epoch: 51, Batch(160/245), Loss: 0.0660\n",
      "Epoch: 51, Batch(180/245), Loss: 0.0537\n",
      "Epoch: 51, Batch(200/245), Loss: 0.0785\n",
      "Epoch: 51, Batch(220/245), Loss: 0.0407\n",
      "Epoch: 51, Batch(240/245), Loss: 0.0657\n",
      "epoch 51 duration 1.712 train_loss 0.058 val_loss 0.147 val_acc 0.9631\n",
      "Epoch: 52, Batch(20/245), Loss: 0.0735\n",
      "Epoch: 52, Batch(40/245), Loss: 0.0451\n",
      "Epoch: 52, Batch(60/245), Loss: 0.0638\n",
      "Epoch: 52, Batch(80/245), Loss: 0.0342\n",
      "Epoch: 52, Batch(100/245), Loss: 0.0396\n",
      "Epoch: 52, Batch(120/245), Loss: 0.0506\n",
      "Epoch: 52, Batch(140/245), Loss: 0.0740\n",
      "Epoch: 52, Batch(160/245), Loss: 0.0801\n",
      "Epoch: 52, Batch(180/245), Loss: 0.0304\n",
      "Epoch: 52, Batch(200/245), Loss: 0.0709\n",
      "Epoch: 52, Batch(220/245), Loss: 0.0475\n",
      "Epoch: 52, Batch(240/245), Loss: 0.0378\n",
      "epoch 52 duration 1.706 train_loss 0.053 val_loss 0.155 val_acc 0.9607\n",
      "Epoch: 53, Batch(20/245), Loss: 0.0436\n",
      "Epoch: 53, Batch(40/245), Loss: 0.0357\n",
      "Epoch: 53, Batch(60/245), Loss: 0.0597\n",
      "Epoch: 53, Batch(80/245), Loss: 0.0415\n",
      "Epoch: 53, Batch(100/245), Loss: 0.0609\n",
      "Epoch: 53, Batch(120/245), Loss: 0.0634\n",
      "Epoch: 53, Batch(140/245), Loss: 0.0506\n",
      "Epoch: 53, Batch(160/245), Loss: 0.0438\n",
      "Epoch: 53, Batch(180/245), Loss: 0.0790\n",
      "Epoch: 53, Batch(200/245), Loss: 0.0392\n",
      "Epoch: 53, Batch(220/245), Loss: 0.0304\n",
      "Epoch: 53, Batch(240/245), Loss: 0.0528\n",
      "epoch 53 duration 1.705 train_loss 0.049 val_loss 0.166 val_acc 0.9619\n",
      "Epoch: 54, Batch(20/245), Loss: 0.0455\n",
      "Epoch: 54, Batch(40/245), Loss: 0.0355\n",
      "Epoch: 54, Batch(60/245), Loss: 0.0407\n",
      "Epoch: 54, Batch(80/245), Loss: 0.0430\n",
      "Epoch: 54, Batch(100/245), Loss: 0.0287\n",
      "Epoch: 54, Batch(120/245), Loss: 0.0534\n",
      "Epoch: 54, Batch(140/245), Loss: 0.0571\n",
      "Epoch: 54, Batch(160/245), Loss: 0.0528\n",
      "Epoch: 54, Batch(180/245), Loss: 0.0485\n",
      "Epoch: 54, Batch(200/245), Loss: 0.0521\n",
      "Epoch: 54, Batch(220/245), Loss: 0.0412\n",
      "Epoch: 54, Batch(240/245), Loss: 0.0417\n",
      "epoch 54 duration 1.707 train_loss 0.045 val_loss 0.163 val_acc 0.9619\n",
      "Epoch: 55, Batch(20/245), Loss: 0.0304\n",
      "Epoch: 55, Batch(40/245), Loss: 0.0511\n",
      "Epoch: 55, Batch(60/245), Loss: 0.0357\n",
      "Epoch: 55, Batch(80/245), Loss: 0.0318\n",
      "Epoch: 55, Batch(100/245), Loss: 0.0680\n",
      "Epoch: 55, Batch(120/245), Loss: 0.0663\n",
      "Epoch: 55, Batch(140/245), Loss: 0.0531\n",
      "Epoch: 55, Batch(160/245), Loss: 0.0411\n",
      "Epoch: 55, Batch(180/245), Loss: 0.0771\n",
      "Epoch: 55, Batch(200/245), Loss: 0.0455\n",
      "Epoch: 55, Batch(220/245), Loss: 0.0487\n",
      "Epoch: 55, Batch(240/245), Loss: 0.0521\n",
      "epoch 55 duration 1.710 train_loss 0.050 val_loss 0.167 val_acc 0.9607\n",
      "Epoch: 56, Batch(20/245), Loss: 0.0833\n",
      "Epoch: 56, Batch(40/245), Loss: 0.0427\n",
      "Epoch: 56, Batch(60/245), Loss: 0.0451\n",
      "Epoch: 56, Batch(80/245), Loss: 0.0596\n",
      "Epoch: 56, Batch(100/245), Loss: 0.0973\n",
      "Epoch: 56, Batch(120/245), Loss: 0.0437\n",
      "Epoch: 56, Batch(140/245), Loss: 0.0564\n",
      "Epoch: 56, Batch(160/245), Loss: 0.0328\n",
      "Epoch: 56, Batch(180/245), Loss: 0.0413\n",
      "Epoch: 56, Batch(200/245), Loss: 0.0630\n",
      "Epoch: 56, Batch(220/245), Loss: 0.0390\n",
      "Epoch: 56, Batch(240/245), Loss: 0.0400\n",
      "epoch 56 duration 1.709 train_loss 0.053 val_loss 0.174 val_acc 0.9548\n",
      "Epoch: 57, Batch(20/245), Loss: 0.0465\n",
      "Epoch: 57, Batch(40/245), Loss: 0.0355\n",
      "Epoch: 57, Batch(60/245), Loss: 0.0677\n",
      "Epoch: 57, Batch(80/245), Loss: 0.0639\n",
      "Epoch: 57, Batch(100/245), Loss: 0.0354\n",
      "Epoch: 57, Batch(120/245), Loss: 0.0303\n",
      "Epoch: 57, Batch(140/245), Loss: 0.0269\n",
      "Epoch: 57, Batch(160/245), Loss: 0.0373\n",
      "Epoch: 57, Batch(180/245), Loss: 0.0645\n",
      "Epoch: 57, Batch(200/245), Loss: 0.0531\n",
      "Epoch: 57, Batch(220/245), Loss: 0.0448\n",
      "Epoch: 57, Batch(240/245), Loss: 0.0400\n",
      "epoch 57 duration 1.704 train_loss 0.047 val_loss 0.170 val_acc 0.9607\n",
      "Epoch: 58, Batch(20/245), Loss: 0.0343\n",
      "Epoch: 58, Batch(40/245), Loss: 0.0428\n",
      "Epoch: 58, Batch(60/245), Loss: 0.0375\n",
      "Epoch: 58, Batch(80/245), Loss: 0.0342\n",
      "Epoch: 58, Batch(100/245), Loss: 0.0525\n",
      "Epoch: 58, Batch(120/245), Loss: 0.0364\n",
      "Epoch: 58, Batch(140/245), Loss: 0.0503\n",
      "Epoch: 58, Batch(160/245), Loss: 0.0505\n",
      "Epoch: 58, Batch(180/245), Loss: 0.0438\n",
      "Epoch: 58, Batch(200/245), Loss: 0.0605\n",
      "Epoch: 58, Batch(220/245), Loss: 0.0461\n",
      "Epoch: 58, Batch(240/245), Loss: 0.0286\n",
      "epoch 58 duration 1.709 train_loss 0.043 val_loss 0.159 val_acc 0.9595\n",
      "Epoch: 59, Batch(20/245), Loss: 0.0487\n",
      "Epoch: 59, Batch(40/245), Loss: 0.0603\n",
      "Epoch: 59, Batch(60/245), Loss: 0.0267\n",
      "Epoch: 59, Batch(80/245), Loss: 0.0334\n",
      "Epoch: 59, Batch(100/245), Loss: 0.0488\n",
      "Epoch: 59, Batch(120/245), Loss: 0.0507\n",
      "Epoch: 59, Batch(140/245), Loss: 0.0627\n",
      "Epoch: 59, Batch(160/245), Loss: 0.0770\n",
      "Epoch: 59, Batch(180/245), Loss: 0.0288\n",
      "Epoch: 59, Batch(200/245), Loss: 0.0598\n",
      "Epoch: 59, Batch(220/245), Loss: 0.0455\n",
      "Epoch: 59, Batch(240/245), Loss: 0.0274\n",
      "epoch 59 duration 1.711 train_loss 0.047 val_loss 0.147 val_acc 0.9643\n",
      "Epoch: 60, Batch(20/245), Loss: 0.0660\n",
      "Epoch: 60, Batch(40/245), Loss: 0.0269\n",
      "Epoch: 60, Batch(60/245), Loss: 0.0475\n",
      "Epoch: 60, Batch(80/245), Loss: 0.0682\n",
      "Epoch: 60, Batch(100/245), Loss: 0.0628\n",
      "Epoch: 60, Batch(120/245), Loss: 0.0553\n",
      "Epoch: 60, Batch(140/245), Loss: 0.0426\n",
      "Epoch: 60, Batch(160/245), Loss: 0.0604\n",
      "Epoch: 60, Batch(180/245), Loss: 0.0319\n",
      "Epoch: 60, Batch(200/245), Loss: 0.0456\n",
      "Epoch: 60, Batch(220/245), Loss: 0.0446\n",
      "Epoch: 60, Batch(240/245), Loss: 0.0544\n",
      "epoch 60 duration 1.706 train_loss 0.050 val_loss 0.156 val_acc 0.9595\n",
      "Epoch: 61, Batch(20/245), Loss: 0.0350\n",
      "Epoch: 61, Batch(40/245), Loss: 0.0481\n",
      "Epoch: 61, Batch(60/245), Loss: 0.0767\n",
      "Epoch: 61, Batch(80/245), Loss: 0.0441\n",
      "Epoch: 61, Batch(100/245), Loss: 0.0434\n",
      "Epoch: 61, Batch(120/245), Loss: 0.0436\n",
      "Epoch: 61, Batch(140/245), Loss: 0.0224\n",
      "Epoch: 61, Batch(160/245), Loss: 0.0789\n",
      "Epoch: 61, Batch(180/245), Loss: 0.0334\n",
      "Epoch: 61, Batch(200/245), Loss: 0.0404\n",
      "Epoch: 61, Batch(220/245), Loss: 0.0494\n",
      "Epoch: 61, Batch(240/245), Loss: 0.0379\n",
      "epoch 61 duration 1.712 train_loss 0.046 val_loss 0.143 val_acc 0.9643\n",
      "Epoch: 62, Batch(20/245), Loss: 0.0516\n",
      "Epoch: 62, Batch(40/245), Loss: 0.0319\n",
      "Epoch: 62, Batch(60/245), Loss: 0.0350\n",
      "Epoch: 62, Batch(80/245), Loss: 0.0325\n",
      "Epoch: 62, Batch(100/245), Loss: 0.0434\n",
      "Epoch: 62, Batch(120/245), Loss: 0.0386\n",
      "Epoch: 62, Batch(140/245), Loss: 0.0580\n",
      "Epoch: 62, Batch(160/245), Loss: 0.0404\n",
      "Epoch: 62, Batch(180/245), Loss: 0.0321\n",
      "Epoch: 62, Batch(200/245), Loss: 0.0441\n",
      "Epoch: 62, Batch(220/245), Loss: 0.0329\n",
      "Epoch: 62, Batch(240/245), Loss: 0.0276\n",
      "epoch 62 duration 1.710 train_loss 0.039 val_loss 0.146 val_acc 0.9595\n",
      "Epoch: 63, Batch(20/245), Loss: 0.0355\n",
      "Epoch: 63, Batch(40/245), Loss: 0.0264\n",
      "Epoch: 63, Batch(60/245), Loss: 0.0312\n",
      "Epoch: 63, Batch(80/245), Loss: 0.1008\n",
      "Epoch: 63, Batch(100/245), Loss: 0.0336\n",
      "Epoch: 63, Batch(120/245), Loss: 0.0517\n",
      "Epoch: 63, Batch(140/245), Loss: 0.0306\n",
      "Epoch: 63, Batch(160/245), Loss: 0.0407\n",
      "Epoch: 63, Batch(180/245), Loss: 0.0268\n",
      "Epoch: 63, Batch(200/245), Loss: 0.0305\n",
      "Epoch: 63, Batch(220/245), Loss: 0.0607\n",
      "Epoch: 63, Batch(240/245), Loss: 0.0481\n",
      "epoch 63 duration 1.708 train_loss 0.043 val_loss 0.155 val_acc 0.9571\n",
      "Epoch: 64, Batch(20/245), Loss: 0.0440\n",
      "Epoch: 64, Batch(40/245), Loss: 0.0308\n",
      "Epoch: 64, Batch(60/245), Loss: 0.0365\n",
      "Epoch: 64, Batch(80/245), Loss: 0.0406\n",
      "Epoch: 64, Batch(100/245), Loss: 0.0372\n",
      "Epoch: 64, Batch(120/245), Loss: 0.0409\n",
      "Epoch: 64, Batch(140/245), Loss: 0.0433\n",
      "Epoch: 64, Batch(160/245), Loss: 0.0338\n",
      "Epoch: 64, Batch(180/245), Loss: 0.0390\n",
      "Epoch: 64, Batch(200/245), Loss: 0.0246\n",
      "Epoch: 64, Batch(220/245), Loss: 0.0436\n",
      "Epoch: 64, Batch(240/245), Loss: 0.0251\n",
      "epoch 64 duration 1.712 train_loss 0.037 val_loss 0.163 val_acc 0.9619\n",
      "Epoch: 65, Batch(20/245), Loss: 0.0290\n",
      "Epoch: 65, Batch(40/245), Loss: 0.0396\n",
      "Epoch: 65, Batch(60/245), Loss: 0.0478\n",
      "Epoch: 65, Batch(80/245), Loss: 0.0316\n",
      "Epoch: 65, Batch(100/245), Loss: 0.0459\n",
      "Epoch: 65, Batch(120/245), Loss: 0.0315\n",
      "Epoch: 65, Batch(140/245), Loss: 0.0369\n",
      "Epoch: 65, Batch(160/245), Loss: 0.0331\n",
      "Epoch: 65, Batch(180/245), Loss: 0.0326\n",
      "Epoch: 65, Batch(200/245), Loss: 0.0292\n",
      "Epoch: 65, Batch(220/245), Loss: 0.0413\n",
      "Epoch: 65, Batch(240/245), Loss: 0.0275\n",
      "epoch 65 duration 1.712 train_loss 0.037 val_loss 0.162 val_acc 0.9560\n",
      "Epoch: 66, Batch(20/245), Loss: 0.0435\n",
      "Epoch: 66, Batch(40/245), Loss: 0.0543\n",
      "Epoch: 66, Batch(60/245), Loss: 0.0285\n",
      "Epoch: 66, Batch(80/245), Loss: 0.0342\n",
      "Epoch: 66, Batch(100/245), Loss: 0.0687\n",
      "Epoch: 66, Batch(120/245), Loss: 0.0602\n",
      "Epoch: 66, Batch(140/245), Loss: 0.0657\n",
      "Epoch: 66, Batch(160/245), Loss: 0.0354\n",
      "Epoch: 66, Batch(180/245), Loss: 0.0360\n",
      "Epoch: 66, Batch(200/245), Loss: 0.0467\n",
      "Epoch: 66, Batch(220/245), Loss: 0.0471\n",
      "Epoch: 66, Batch(240/245), Loss: 0.0337\n",
      "epoch 66 duration 1.710 train_loss 0.046 val_loss 0.190 val_acc 0.9536\n",
      "Epoch: 67, Batch(20/245), Loss: 0.0251\n",
      "Epoch: 67, Batch(40/245), Loss: 0.0448\n",
      "Epoch: 67, Batch(60/245), Loss: 0.0317\n",
      "Epoch: 67, Batch(80/245), Loss: 0.0335\n",
      "Epoch: 67, Batch(100/245), Loss: 0.0328\n",
      "Epoch: 67, Batch(120/245), Loss: 0.0476\n",
      "Epoch: 67, Batch(140/245), Loss: 0.0255\n",
      "Epoch: 67, Batch(160/245), Loss: 0.0283\n",
      "Epoch: 67, Batch(180/245), Loss: 0.0392\n",
      "Epoch: 67, Batch(200/245), Loss: 0.0423\n",
      "Epoch: 67, Batch(220/245), Loss: 0.0453\n",
      "Epoch: 67, Batch(240/245), Loss: 0.0296\n",
      "epoch 67 duration 1.708 train_loss 0.035 val_loss 0.169 val_acc 0.9619\n",
      "Epoch: 68, Batch(20/245), Loss: 0.0213\n",
      "Epoch: 68, Batch(40/245), Loss: 0.0253\n",
      "Epoch: 68, Batch(60/245), Loss: 0.0472\n",
      "Epoch: 68, Batch(80/245), Loss: 0.0341\n",
      "Epoch: 68, Batch(100/245), Loss: 0.0312\n",
      "Epoch: 68, Batch(120/245), Loss: 0.0423\n",
      "Epoch: 68, Batch(140/245), Loss: 0.0369\n",
      "Epoch: 68, Batch(160/245), Loss: 0.0433\n",
      "Epoch: 68, Batch(180/245), Loss: 0.0375\n",
      "Epoch: 68, Batch(200/245), Loss: 0.0426\n",
      "Epoch: 68, Batch(220/245), Loss: 0.0466\n",
      "Epoch: 68, Batch(240/245), Loss: 0.0215\n",
      "epoch 68 duration 1.716 train_loss 0.038 val_loss 0.164 val_acc 0.9607\n",
      "Epoch: 69, Batch(20/245), Loss: 0.0606\n",
      "Epoch: 69, Batch(40/245), Loss: 0.0502\n",
      "Epoch: 69, Batch(60/245), Loss: 0.0381\n",
      "Epoch: 69, Batch(80/245), Loss: 0.0344\n",
      "Epoch: 69, Batch(100/245), Loss: 0.0319\n",
      "Epoch: 69, Batch(120/245), Loss: 0.0453\n",
      "Epoch: 69, Batch(140/245), Loss: 0.0333\n",
      "Epoch: 69, Batch(160/245), Loss: 0.0247\n",
      "Epoch: 69, Batch(180/245), Loss: 0.0596\n",
      "Epoch: 69, Batch(200/245), Loss: 0.0265\n",
      "Epoch: 69, Batch(220/245), Loss: 0.0439\n",
      "Epoch: 69, Batch(240/245), Loss: 0.0481\n",
      "epoch 69 duration 1.704 train_loss 0.042 val_loss 0.160 val_acc 0.9619\n",
      "Epoch: 70, Batch(20/245), Loss: 0.0425\n",
      "Epoch: 70, Batch(40/245), Loss: 0.0586\n",
      "Epoch: 70, Batch(60/245), Loss: 0.0375\n",
      "Epoch: 70, Batch(80/245), Loss: 0.0288\n",
      "Epoch: 70, Batch(100/245), Loss: 0.0402\n",
      "Epoch: 70, Batch(120/245), Loss: 0.0271\n",
      "Epoch: 70, Batch(140/245), Loss: 0.0526\n",
      "Epoch: 70, Batch(160/245), Loss: 0.0507\n",
      "Epoch: 70, Batch(180/245), Loss: 0.0292\n",
      "Epoch: 70, Batch(200/245), Loss: 0.0229\n",
      "Epoch: 70, Batch(220/245), Loss: 0.0295\n",
      "Epoch: 70, Batch(240/245), Loss: 0.0615\n",
      "epoch 70 duration 1.714 train_loss 0.040 val_loss 0.163 val_acc 0.9583\n",
      "Epoch: 71, Batch(20/245), Loss: 0.0344\n",
      "Epoch: 71, Batch(40/245), Loss: 0.0373\n",
      "Epoch: 71, Batch(60/245), Loss: 0.0280\n",
      "Epoch: 71, Batch(80/245), Loss: 0.0492\n",
      "Epoch: 71, Batch(100/245), Loss: 0.0396\n",
      "Epoch: 71, Batch(120/245), Loss: 0.0549\n",
      "Epoch: 71, Batch(140/245), Loss: 0.0390\n",
      "Epoch: 71, Batch(160/245), Loss: 0.0253\n",
      "Epoch: 71, Batch(180/245), Loss: 0.0369\n",
      "Epoch: 71, Batch(200/245), Loss: 0.0327\n",
      "Epoch: 71, Batch(220/245), Loss: 0.0310\n",
      "Epoch: 71, Batch(240/245), Loss: 0.0552\n",
      "epoch 71 duration 1.706 train_loss 0.039 val_loss 0.174 val_acc 0.9524\n",
      "Epoch: 72, Batch(20/245), Loss: 0.0307\n",
      "Epoch: 72, Batch(40/245), Loss: 0.0371\n",
      "Epoch: 72, Batch(60/245), Loss: 0.0630\n",
      "Epoch: 72, Batch(80/245), Loss: 0.0327\n",
      "Epoch: 72, Batch(100/245), Loss: 0.0409\n",
      "Epoch: 72, Batch(120/245), Loss: 0.0890\n",
      "Epoch: 72, Batch(140/245), Loss: 0.0328\n",
      "Epoch: 72, Batch(160/245), Loss: 0.0413\n",
      "Epoch: 72, Batch(180/245), Loss: 0.0249\n",
      "Epoch: 72, Batch(200/245), Loss: 0.0369\n",
      "Epoch: 72, Batch(220/245), Loss: 0.0373\n",
      "Epoch: 72, Batch(240/245), Loss: 0.0440\n",
      "epoch 72 duration 1.709 train_loss 0.042 val_loss 0.170 val_acc 0.9560\n",
      "Epoch: 73, Batch(20/245), Loss: 0.0280\n",
      "Epoch: 73, Batch(40/245), Loss: 0.0482\n",
      "Epoch: 73, Batch(60/245), Loss: 0.0620\n",
      "Epoch: 73, Batch(80/245), Loss: 0.0470\n",
      "Epoch: 73, Batch(100/245), Loss: 0.0302\n",
      "Epoch: 73, Batch(120/245), Loss: 0.0400\n",
      "Epoch: 73, Batch(140/245), Loss: 0.0208\n",
      "Epoch: 73, Batch(160/245), Loss: 0.0507\n",
      "Epoch: 73, Batch(180/245), Loss: 0.0288\n",
      "Epoch: 73, Batch(200/245), Loss: 0.0490\n",
      "Epoch: 73, Batch(220/245), Loss: 0.0534\n",
      "Epoch: 73, Batch(240/245), Loss: 0.0329\n",
      "epoch 73 duration 1.708 train_loss 0.041 val_loss 0.149 val_acc 0.9643\n",
      "Epoch: 74, Batch(20/245), Loss: 0.0470\n",
      "Epoch: 74, Batch(40/245), Loss: 0.0502\n",
      "Epoch: 74, Batch(60/245), Loss: 0.0357\n",
      "Epoch: 74, Batch(80/245), Loss: 0.0343\n",
      "Epoch: 74, Batch(100/245), Loss: 0.0418\n",
      "Epoch: 74, Batch(120/245), Loss: 0.0192\n",
      "Epoch: 74, Batch(140/245), Loss: 0.0357\n",
      "Epoch: 74, Batch(160/245), Loss: 0.0471\n",
      "Epoch: 74, Batch(180/245), Loss: 0.0348\n",
      "Epoch: 74, Batch(200/245), Loss: 0.0310\n",
      "Epoch: 74, Batch(220/245), Loss: 0.0447\n",
      "Epoch: 74, Batch(240/245), Loss: 0.0442\n",
      "epoch 74 duration 1.712 train_loss 0.039 val_loss 0.161 val_acc 0.9619\n",
      "Epoch: 75, Batch(20/245), Loss: 0.0340\n",
      "Epoch: 75, Batch(40/245), Loss: 0.0219\n",
      "Epoch: 75, Batch(60/245), Loss: 0.0317\n",
      "Epoch: 75, Batch(80/245), Loss: 0.0295\n",
      "Epoch: 75, Batch(100/245), Loss: 0.0654\n",
      "Epoch: 75, Batch(120/245), Loss: 0.0429\n",
      "Epoch: 75, Batch(140/245), Loss: 0.0300\n",
      "Epoch: 75, Batch(160/245), Loss: 0.0487\n",
      "Epoch: 75, Batch(180/245), Loss: 0.0510\n",
      "Epoch: 75, Batch(200/245), Loss: 0.0324\n",
      "Epoch: 75, Batch(220/245), Loss: 0.0677\n",
      "Epoch: 75, Batch(240/245), Loss: 0.0426\n",
      "epoch 75 duration 1.706 train_loss 0.041 val_loss 0.168 val_acc 0.9595\n",
      "Epoch: 76, Batch(20/245), Loss: 0.0320\n",
      "Epoch: 76, Batch(40/245), Loss: 0.0368\n",
      "Epoch: 76, Batch(60/245), Loss: 0.0446\n",
      "Epoch: 76, Batch(80/245), Loss: 0.0299\n",
      "Epoch: 76, Batch(100/245), Loss: 0.0259\n",
      "Epoch: 76, Batch(120/245), Loss: 0.0705\n",
      "Epoch: 76, Batch(140/245), Loss: 0.0347\n",
      "Epoch: 76, Batch(160/245), Loss: 0.0298\n",
      "Epoch: 76, Batch(180/245), Loss: 0.0423\n",
      "Epoch: 76, Batch(200/245), Loss: 0.0323\n",
      "Epoch: 76, Batch(220/245), Loss: 0.0246\n",
      "Epoch: 76, Batch(240/245), Loss: 0.0313\n",
      "epoch 76 duration 1.712 train_loss 0.036 val_loss 0.166 val_acc 0.9631\n",
      "Epoch: 77, Batch(20/245), Loss: 0.0562\n",
      "Epoch: 77, Batch(40/245), Loss: 0.0298\n",
      "Epoch: 77, Batch(60/245), Loss: 0.0378\n",
      "Epoch: 77, Batch(80/245), Loss: 0.0329\n",
      "Epoch: 77, Batch(100/245), Loss: 0.0308\n",
      "Epoch: 77, Batch(120/245), Loss: 0.0296\n",
      "Epoch: 77, Batch(140/245), Loss: 0.0346\n",
      "Epoch: 77, Batch(160/245), Loss: 0.0509\n",
      "Epoch: 77, Batch(180/245), Loss: 0.0354\n",
      "Epoch: 77, Batch(200/245), Loss: 0.0360\n",
      "Epoch: 77, Batch(220/245), Loss: 0.0297\n",
      "Epoch: 77, Batch(240/245), Loss: 0.0305\n",
      "epoch 77 duration 1.706 train_loss 0.037 val_loss 0.183 val_acc 0.9607\n",
      "Epoch: 78, Batch(20/245), Loss: 0.0364\n",
      "Epoch: 78, Batch(40/245), Loss: 0.0336\n",
      "Epoch: 78, Batch(60/245), Loss: 0.0472\n",
      "Epoch: 78, Batch(80/245), Loss: 0.0607\n",
      "Epoch: 78, Batch(100/245), Loss: 0.0298\n",
      "Epoch: 78, Batch(120/245), Loss: 0.0418\n",
      "Epoch: 78, Batch(140/245), Loss: 0.0298\n",
      "Epoch: 78, Batch(160/245), Loss: 0.0306\n",
      "Epoch: 78, Batch(180/245), Loss: 0.0298\n",
      "Epoch: 78, Batch(200/245), Loss: 0.0267\n",
      "Epoch: 78, Batch(220/245), Loss: 0.0392\n",
      "Epoch: 78, Batch(240/245), Loss: 0.0541\n",
      "epoch 78 duration 1.713 train_loss 0.038 val_loss 0.182 val_acc 0.9571\n",
      "Epoch: 79, Batch(20/245), Loss: 0.0496\n",
      "Epoch: 79, Batch(40/245), Loss: 0.0213\n",
      "Epoch: 79, Batch(60/245), Loss: 0.0455\n",
      "Epoch: 79, Batch(80/245), Loss: 0.0291\n",
      "Epoch: 79, Batch(100/245), Loss: 0.0274\n",
      "Epoch: 79, Batch(120/245), Loss: 0.0545\n",
      "Epoch: 79, Batch(140/245), Loss: 0.0331\n",
      "Epoch: 79, Batch(160/245), Loss: 0.0444\n",
      "Epoch: 79, Batch(180/245), Loss: 0.0263\n",
      "Epoch: 79, Batch(200/245), Loss: 0.0350\n",
      "Epoch: 79, Batch(220/245), Loss: 0.0609\n",
      "Epoch: 79, Batch(240/245), Loss: 0.0293\n",
      "epoch 79 duration 1.712 train_loss 0.038 val_loss 0.194 val_acc 0.9524\n",
      "Epoch: 80, Batch(20/245), Loss: 0.0226\n",
      "Epoch: 80, Batch(40/245), Loss: 0.0446\n",
      "Epoch: 80, Batch(60/245), Loss: 0.0330\n",
      "Epoch: 80, Batch(80/245), Loss: 0.0292\n",
      "Epoch: 80, Batch(100/245), Loss: 0.0446\n",
      "Epoch: 80, Batch(120/245), Loss: 0.0323\n",
      "Epoch: 80, Batch(140/245), Loss: 0.0473\n",
      "Epoch: 80, Batch(160/245), Loss: 0.0223\n",
      "Epoch: 80, Batch(180/245), Loss: 0.0275\n",
      "Epoch: 80, Batch(200/245), Loss: 0.0489\n",
      "Epoch: 80, Batch(220/245), Loss: 0.0294\n",
      "Epoch: 80, Batch(240/245), Loss: 0.0284\n",
      "epoch 80 duration 1.710 train_loss 0.034 val_loss 0.186 val_acc 0.9571\n",
      "Epoch: 81, Batch(20/245), Loss: 0.0379\n",
      "Epoch: 81, Batch(40/245), Loss: 0.0390\n",
      "Epoch: 81, Batch(60/245), Loss: 0.0308\n",
      "Epoch: 81, Batch(80/245), Loss: 0.0291\n",
      "Epoch: 81, Batch(100/245), Loss: 0.0359\n",
      "Epoch: 81, Batch(120/245), Loss: 0.0357\n",
      "Epoch: 81, Batch(140/245), Loss: 0.0317\n",
      "Epoch: 81, Batch(160/245), Loss: 0.0252\n",
      "Epoch: 81, Batch(180/245), Loss: 0.0241\n",
      "Epoch: 81, Batch(200/245), Loss: 0.0279\n",
      "Epoch: 81, Batch(220/245), Loss: 0.0394\n",
      "Epoch: 81, Batch(240/245), Loss: 0.0255\n",
      "epoch 81 duration 1.706 train_loss 0.032 val_loss 0.186 val_acc 0.9571\n",
      "Epoch: 82, Batch(20/245), Loss: 0.0409\n",
      "Epoch: 82, Batch(40/245), Loss: 0.0338\n",
      "Epoch: 82, Batch(60/245), Loss: 0.0240\n",
      "Epoch: 82, Batch(80/245), Loss: 0.0536\n",
      "Epoch: 82, Batch(100/245), Loss: 0.0376\n",
      "Epoch: 82, Batch(120/245), Loss: 0.0426\n",
      "Epoch: 82, Batch(140/245), Loss: 0.0250\n",
      "Epoch: 82, Batch(160/245), Loss: 0.0356\n",
      "Epoch: 82, Batch(180/245), Loss: 0.0255\n",
      "Epoch: 82, Batch(200/245), Loss: 0.0331\n",
      "Epoch: 82, Batch(220/245), Loss: 0.0347\n",
      "Epoch: 82, Batch(240/245), Loss: 0.0215\n",
      "epoch 82 duration 1.703 train_loss 0.034 val_loss 0.181 val_acc 0.9571\n",
      "Epoch: 83, Batch(20/245), Loss: 0.0300\n",
      "Epoch: 83, Batch(40/245), Loss: 0.0304\n",
      "Epoch: 83, Batch(60/245), Loss: 0.0316\n",
      "Epoch: 83, Batch(80/245), Loss: 0.0305\n",
      "Epoch: 83, Batch(100/245), Loss: 0.0459\n",
      "Epoch: 83, Batch(120/245), Loss: 0.0292\n",
      "Epoch: 83, Batch(140/245), Loss: 0.0411\n",
      "Epoch: 83, Batch(160/245), Loss: 0.0278\n",
      "Epoch: 83, Batch(180/245), Loss: 0.0158\n",
      "Epoch: 83, Batch(200/245), Loss: 0.0249\n",
      "Epoch: 83, Batch(220/245), Loss: 0.0274\n",
      "Epoch: 83, Batch(240/245), Loss: 0.0330\n",
      "epoch 83 duration 1.704 train_loss 0.031 val_loss 0.182 val_acc 0.9571\n",
      "Epoch: 84, Batch(20/245), Loss: 0.0384\n",
      "Epoch: 84, Batch(40/245), Loss: 0.0297\n",
      "Epoch: 84, Batch(60/245), Loss: 0.0367\n",
      "Epoch: 84, Batch(80/245), Loss: 0.0293\n",
      "Epoch: 84, Batch(100/245), Loss: 0.0393\n",
      "Epoch: 84, Batch(120/245), Loss: 0.0295\n",
      "Epoch: 84, Batch(140/245), Loss: 0.0293\n",
      "Epoch: 84, Batch(160/245), Loss: 0.0470\n",
      "Epoch: 84, Batch(180/245), Loss: 0.0429\n",
      "Epoch: 84, Batch(200/245), Loss: 0.0229\n",
      "Epoch: 84, Batch(220/245), Loss: 0.0249\n",
      "Epoch: 84, Batch(240/245), Loss: 0.0319\n",
      "epoch 84 duration 1.706 train_loss 0.034 val_loss 0.174 val_acc 0.9583\n",
      "Epoch: 85, Batch(20/245), Loss: 0.0371\n",
      "Epoch: 85, Batch(40/245), Loss: 0.0240\n",
      "Epoch: 85, Batch(60/245), Loss: 0.0348\n",
      "Epoch: 85, Batch(80/245), Loss: 0.0285\n",
      "Epoch: 85, Batch(100/245), Loss: 0.0230\n",
      "Epoch: 85, Batch(120/245), Loss: 0.0352\n",
      "Epoch: 85, Batch(140/245), Loss: 0.0336\n",
      "Epoch: 85, Batch(160/245), Loss: 0.0329\n",
      "Epoch: 85, Batch(180/245), Loss: 0.0300\n",
      "Epoch: 85, Batch(200/245), Loss: 0.0324\n",
      "Epoch: 85, Batch(220/245), Loss: 0.0432\n",
      "Epoch: 85, Batch(240/245), Loss: 0.0544\n",
      "epoch 85 duration 1.706 train_loss 0.034 val_loss 0.178 val_acc 0.9560\n",
      "Epoch: 86, Batch(20/245), Loss: 0.0342\n",
      "Epoch: 86, Batch(40/245), Loss: 0.0304\n",
      "Epoch: 86, Batch(60/245), Loss: 0.0533\n",
      "Epoch: 86, Batch(80/245), Loss: 0.0420\n",
      "Epoch: 86, Batch(100/245), Loss: 0.0583\n",
      "Epoch: 86, Batch(120/245), Loss: 0.0263\n",
      "Epoch: 86, Batch(140/245), Loss: 0.0380\n",
      "Epoch: 86, Batch(160/245), Loss: 0.0354\n",
      "Epoch: 86, Batch(180/245), Loss: 0.0413\n",
      "Epoch: 86, Batch(200/245), Loss: 0.0333\n",
      "Epoch: 86, Batch(220/245), Loss: 0.0444\n",
      "Epoch: 86, Batch(240/245), Loss: 0.0272\n",
      "epoch 86 duration 1.711 train_loss 0.038 val_loss 0.175 val_acc 0.9571\n",
      "Epoch: 87, Batch(20/245), Loss: 0.0292\n",
      "Epoch: 87, Batch(40/245), Loss: 0.0343\n",
      "Epoch: 87, Batch(60/245), Loss: 0.0291\n",
      "Epoch: 87, Batch(80/245), Loss: 0.0435\n",
      "Epoch: 87, Batch(100/245), Loss: 0.0275\n",
      "Epoch: 87, Batch(120/245), Loss: 0.0268\n",
      "Epoch: 87, Batch(140/245), Loss: 0.0208\n",
      "Epoch: 87, Batch(160/245), Loss: 0.0227\n",
      "Epoch: 87, Batch(180/245), Loss: 0.0329\n",
      "Epoch: 87, Batch(200/245), Loss: 0.0442\n",
      "Epoch: 87, Batch(220/245), Loss: 0.0166\n",
      "Epoch: 87, Batch(240/245), Loss: 0.0270\n",
      "epoch 87 duration 1.710 train_loss 0.030 val_loss 0.183 val_acc 0.9571\n",
      "Epoch: 88, Batch(20/245), Loss: 0.0245\n",
      "Epoch: 88, Batch(40/245), Loss: 0.0294\n",
      "Epoch: 88, Batch(60/245), Loss: 0.0394\n",
      "Epoch: 88, Batch(80/245), Loss: 0.0314\n",
      "Epoch: 88, Batch(100/245), Loss: 0.0233\n",
      "Epoch: 88, Batch(120/245), Loss: 0.0235\n",
      "Epoch: 88, Batch(140/245), Loss: 0.0544\n",
      "Epoch: 88, Batch(160/245), Loss: 0.0236\n",
      "Epoch: 88, Batch(180/245), Loss: 0.0246\n",
      "Epoch: 88, Batch(200/245), Loss: 0.0242\n",
      "Epoch: 88, Batch(220/245), Loss: 0.0236\n",
      "Epoch: 88, Batch(240/245), Loss: 0.0335\n",
      "epoch 88 duration 1.713 train_loss 0.030 val_loss 0.177 val_acc 0.9571\n",
      "Epoch: 89, Batch(20/245), Loss: 0.0250\n",
      "Epoch: 89, Batch(40/245), Loss: 0.0368\n",
      "Epoch: 89, Batch(60/245), Loss: 0.0209\n",
      "Epoch: 89, Batch(80/245), Loss: 0.0256\n",
      "Epoch: 89, Batch(100/245), Loss: 0.0449\n",
      "Epoch: 89, Batch(120/245), Loss: 0.0491\n",
      "Epoch: 89, Batch(140/245), Loss: 0.0378\n",
      "Epoch: 89, Batch(160/245), Loss: 0.0364\n",
      "Epoch: 89, Batch(180/245), Loss: 0.0328\n",
      "Epoch: 89, Batch(200/245), Loss: 0.0318\n",
      "Epoch: 89, Batch(220/245), Loss: 0.0275\n",
      "Epoch: 89, Batch(240/245), Loss: 0.0421\n",
      "epoch 89 duration 1.714 train_loss 0.034 val_loss 0.176 val_acc 0.9571\n",
      "Epoch: 90, Batch(20/245), Loss: 0.0288\n",
      "Epoch: 90, Batch(40/245), Loss: 0.0293\n",
      "Epoch: 90, Batch(60/245), Loss: 0.0263\n",
      "Epoch: 90, Batch(80/245), Loss: 0.0279\n",
      "Epoch: 90, Batch(100/245), Loss: 0.0300\n",
      "Epoch: 90, Batch(120/245), Loss: 0.0305\n",
      "Epoch: 90, Batch(140/245), Loss: 0.0174\n",
      "Epoch: 90, Batch(160/245), Loss: 0.0253\n",
      "Epoch: 90, Batch(180/245), Loss: 0.0378\n",
      "Epoch: 90, Batch(200/245), Loss: 0.0283\n",
      "Epoch: 90, Batch(220/245), Loss: 0.0561\n",
      "Epoch: 90, Batch(240/245), Loss: 0.0269\n",
      "epoch 90 duration 1.708 train_loss 0.031 val_loss 0.175 val_acc 0.9583\n",
      "Epoch: 91, Batch(20/245), Loss: 0.0371\n",
      "Epoch: 91, Batch(40/245), Loss: 0.0251\n",
      "Epoch: 91, Batch(60/245), Loss: 0.0263\n",
      "Epoch: 91, Batch(80/245), Loss: 0.0385\n",
      "Epoch: 91, Batch(100/245), Loss: 0.0278\n",
      "Epoch: 91, Batch(120/245), Loss: 0.0277\n",
      "Epoch: 91, Batch(140/245), Loss: 0.0342\n",
      "Epoch: 91, Batch(160/245), Loss: 0.0332\n",
      "Epoch: 91, Batch(180/245), Loss: 0.0445\n",
      "Epoch: 91, Batch(200/245), Loss: 0.0530\n",
      "Epoch: 91, Batch(220/245), Loss: 0.0273\n",
      "Epoch: 91, Batch(240/245), Loss: 0.0373\n",
      "epoch 91 duration 1.707 train_loss 0.034 val_loss 0.171 val_acc 0.9607\n",
      "Epoch: 92, Batch(20/245), Loss: 0.0305\n",
      "Epoch: 92, Batch(40/245), Loss: 0.0243\n",
      "Epoch: 92, Batch(60/245), Loss: 0.0295\n",
      "Epoch: 92, Batch(80/245), Loss: 0.0364\n",
      "Epoch: 92, Batch(100/245), Loss: 0.0437\n",
      "Epoch: 92, Batch(120/245), Loss: 0.0283\n",
      "Epoch: 92, Batch(140/245), Loss: 0.0408\n",
      "Epoch: 92, Batch(160/245), Loss: 0.0231\n",
      "Epoch: 92, Batch(180/245), Loss: 0.0504\n",
      "Epoch: 92, Batch(200/245), Loss: 0.0235\n",
      "Epoch: 92, Batch(220/245), Loss: 0.0452\n",
      "Epoch: 92, Batch(240/245), Loss: 0.0350\n",
      "epoch 92 duration 1.708 train_loss 0.034 val_loss 0.181 val_acc 0.9548\n",
      "Epoch: 93, Batch(20/245), Loss: 0.0471\n",
      "Epoch: 93, Batch(40/245), Loss: 0.0489\n",
      "Epoch: 93, Batch(60/245), Loss: 0.0255\n",
      "Epoch: 93, Batch(80/245), Loss: 0.0369\n",
      "Epoch: 93, Batch(100/245), Loss: 0.0228\n",
      "Epoch: 93, Batch(120/245), Loss: 0.0271\n",
      "Epoch: 93, Batch(140/245), Loss: 0.0274\n",
      "Epoch: 93, Batch(160/245), Loss: 0.0225\n",
      "Epoch: 93, Batch(180/245), Loss: 0.0220\n",
      "Epoch: 93, Batch(200/245), Loss: 0.0235\n",
      "Epoch: 93, Batch(220/245), Loss: 0.0449\n",
      "Epoch: 93, Batch(240/245), Loss: 0.0317\n",
      "epoch 93 duration 1.708 train_loss 0.032 val_loss 0.177 val_acc 0.9560\n",
      "Epoch: 94, Batch(20/245), Loss: 0.0194\n",
      "Epoch: 94, Batch(40/245), Loss: 0.0587\n",
      "Epoch: 94, Batch(60/245), Loss: 0.0256\n",
      "Epoch: 94, Batch(80/245), Loss: 0.0303\n",
      "Epoch: 94, Batch(100/245), Loss: 0.0249\n",
      "Epoch: 94, Batch(120/245), Loss: 0.0372\n",
      "Epoch: 94, Batch(140/245), Loss: 0.0367\n",
      "Epoch: 94, Batch(160/245), Loss: 0.0222\n",
      "Epoch: 94, Batch(180/245), Loss: 0.0260\n",
      "Epoch: 94, Batch(200/245), Loss: 0.0216\n",
      "Epoch: 94, Batch(220/245), Loss: 0.0301\n",
      "Epoch: 94, Batch(240/245), Loss: 0.0255\n",
      "epoch 94 duration 1.708 train_loss 0.031 val_loss 0.180 val_acc 0.9583\n",
      "Epoch: 95, Batch(20/245), Loss: 0.0305\n",
      "Epoch: 95, Batch(40/245), Loss: 0.0413\n",
      "Epoch: 95, Batch(60/245), Loss: 0.0230\n",
      "Epoch: 95, Batch(80/245), Loss: 0.0366\n",
      "Epoch: 95, Batch(100/245), Loss: 0.0349\n",
      "Epoch: 95, Batch(120/245), Loss: 0.0218\n",
      "Epoch: 95, Batch(140/245), Loss: 0.0367\n",
      "Epoch: 95, Batch(160/245), Loss: 0.0257\n",
      "Epoch: 95, Batch(180/245), Loss: 0.0404\n",
      "Epoch: 95, Batch(200/245), Loss: 0.0185\n",
      "Epoch: 95, Batch(220/245), Loss: 0.0212\n",
      "Epoch: 95, Batch(240/245), Loss: 0.0426\n",
      "epoch 95 duration 1.707 train_loss 0.031 val_loss 0.185 val_acc 0.9536\n",
      "Epoch: 96, Batch(20/245), Loss: 0.0377\n",
      "Epoch: 96, Batch(40/245), Loss: 0.0284\n",
      "Epoch: 96, Batch(60/245), Loss: 0.0322\n",
      "Epoch: 96, Batch(80/245), Loss: 0.0309\n",
      "Epoch: 96, Batch(100/245), Loss: 0.0470\n",
      "Epoch: 96, Batch(120/245), Loss: 0.0255\n",
      "Epoch: 96, Batch(140/245), Loss: 0.0310\n",
      "Epoch: 96, Batch(160/245), Loss: 0.0287\n",
      "Epoch: 96, Batch(180/245), Loss: 0.0243\n",
      "Epoch: 96, Batch(200/245), Loss: 0.0530\n",
      "Epoch: 96, Batch(220/245), Loss: 0.0258\n",
      "Epoch: 96, Batch(240/245), Loss: 0.0232\n",
      "epoch 96 duration 1.710 train_loss 0.032 val_loss 0.175 val_acc 0.9583\n",
      "Epoch: 97, Batch(20/245), Loss: 0.0303\n",
      "Epoch: 97, Batch(40/245), Loss: 0.0354\n",
      "Epoch: 97, Batch(60/245), Loss: 0.0291\n",
      "Epoch: 97, Batch(80/245), Loss: 0.0329\n",
      "Epoch: 97, Batch(100/245), Loss: 0.0291\n",
      "Epoch: 97, Batch(120/245), Loss: 0.0259\n",
      "Epoch: 97, Batch(140/245), Loss: 0.0216\n",
      "Epoch: 97, Batch(160/245), Loss: 0.0334\n",
      "Epoch: 97, Batch(180/245), Loss: 0.0295\n",
      "Epoch: 97, Batch(200/245), Loss: 0.0439\n",
      "Epoch: 97, Batch(220/245), Loss: 0.0189\n",
      "Epoch: 97, Batch(240/245), Loss: 0.0394\n",
      "epoch 97 duration 1.707 train_loss 0.031 val_loss 0.180 val_acc 0.9548\n",
      "Epoch: 98, Batch(20/245), Loss: 0.0602\n",
      "Epoch: 98, Batch(40/245), Loss: 0.0283\n",
      "Epoch: 98, Batch(60/245), Loss: 0.0409\n",
      "Epoch: 98, Batch(80/245), Loss: 0.0353\n",
      "Epoch: 98, Batch(100/245), Loss: 0.0227\n",
      "Epoch: 98, Batch(120/245), Loss: 0.0263\n",
      "Epoch: 98, Batch(140/245), Loss: 0.0393\n",
      "Epoch: 98, Batch(160/245), Loss: 0.0356\n",
      "Epoch: 98, Batch(180/245), Loss: 0.0396\n",
      "Epoch: 98, Batch(200/245), Loss: 0.0455\n",
      "Epoch: 98, Batch(220/245), Loss: 0.0400\n",
      "Epoch: 98, Batch(240/245), Loss: 0.0409\n",
      "epoch 98 duration 1.710 train_loss 0.038 val_loss 0.174 val_acc 0.9583\n",
      "Epoch: 99, Batch(20/245), Loss: 0.0271\n",
      "Epoch: 99, Batch(40/245), Loss: 0.0360\n",
      "Epoch: 99, Batch(60/245), Loss: 0.0418\n",
      "Epoch: 99, Batch(80/245), Loss: 0.0246\n",
      "Epoch: 99, Batch(100/245), Loss: 0.0277\n",
      "Epoch: 99, Batch(120/245), Loss: 0.0286\n",
      "Epoch: 99, Batch(140/245), Loss: 0.0173\n",
      "Epoch: 99, Batch(160/245), Loss: 0.0335\n",
      "Epoch: 99, Batch(180/245), Loss: 0.0337\n",
      "Epoch: 99, Batch(200/245), Loss: 0.0433\n",
      "Epoch: 99, Batch(220/245), Loss: 0.0409\n",
      "Epoch: 99, Batch(240/245), Loss: 0.0458\n",
      "epoch 99 duration 1.712 train_loss 0.033 val_loss 0.177 val_acc 0.9595\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:10:50.709586Z",
     "iopub.status.busy": "2024-12-16T03:10:50.708923Z",
     "iopub.status.idle": "2024-12-16T03:11:13.586247Z",
     "shell.execute_reply": "2024-12-16T03:11:13.585397Z",
     "shell.execute_reply.started": "2024-12-16T03:10:50.709553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main/test.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n",
      "#model parameters:  4628878\n",
      "average inference time:  18.32238265446254\n",
      "accuracy:  0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "!python test.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6290008,
     "sourceId": 10182333,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6310476,
     "sourceId": 10210288,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
