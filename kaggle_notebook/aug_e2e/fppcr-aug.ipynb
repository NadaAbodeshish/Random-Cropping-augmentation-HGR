{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10182333,"sourceType":"datasetVersion","datasetId":6290008},{"sourceId":10210288,"sourceType":"datasetVersion","datasetId":6310476}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/NadaAbodeshish/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:17:51.262030Z","iopub.execute_input":"2024-12-16T00:17:51.262281Z","iopub.status.idle":"2024-12-16T00:17:58.965968Z","shell.execute_reply.started":"2024-12-16T00:17:51.262256Z","shell.execute_reply":"2024-12-16T00:17:58.964869Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion'...\nremote: Enumerating objects: 780, done.\u001b[K\nremote: Counting objects: 100% (143/143), done.\u001b[K\nremote: Compressing objects: 100% (101/101), done.\u001b[K\nremote: Total 780 (delta 76), reused 104 (delta 42), pack-reused 637 (from 1)\u001b[K\nReceiving objects: 100% (780/780), 166.71 MiB | 38.41 MiB/s, done.\nResolving deltas: 100% (428/428), done.\nUpdating files: 100% (213/213), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd /kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:17:58.968186Z","iopub.execute_input":"2024-12-16T00:17:58.968556Z","iopub.status.idle":"2024-12-16T00:17:58.975937Z","shell.execute_reply.started":"2024-12-16T00:17:58.968515Z","shell.execute_reply":"2024-12-16T00:17:58.975046Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nbase_dir = \"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512\"\nos.makedirs(base_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:17:58.977053Z","iopub.execute_input":"2024-12-16T00:17:58.977362Z","iopub.status.idle":"2024-12-16T00:17:58.985656Z","shell.execute_reply.started":"2024-12-16T00:17:58.977327Z","shell.execute_reply":"2024-12-16T00:17:58.984781Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!cp -r /kaggle/input/shrec-processed-aug/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/augmented-dataset/* /kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/data/shrec17/Processed_HandGestureDataset_SHREC2017/dbscanCluster_numPts=512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:17:58.988268Z","iopub.execute_input":"2024-12-16T00:17:58.988649Z","iopub.status.idle":"2024-12-16T00:19:28.515536Z","shell.execute_reply.started":"2024-12-16T00:17:58.988592Z","shell.execute_reply":"2024-12-16T00:19:28.514571Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:19:28.517020Z","iopub.execute_input":"2024-12-16T00:19:28.517346Z","iopub.status.idle":"2024-12-16T00:19:28.523206Z","shell.execute_reply.started":"2024-12-16T00:19:28.517315Z","shell.execute_reply":"2024-12-16T00:19:28.522396Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%cd /kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:19:28.524378Z","iopub.execute_input":"2024-12-16T00:19:28.524655Z","iopub.status.idle":"2024-12-16T00:19:28.536758Z","shell.execute_reply.started":"2024-12-16T00:19:28.524601Z","shell.execute_reply":"2024-12-16T00:19:28.535892Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install yacs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:19:28.537953Z","iopub.execute_input":"2024-12-16T00:19:28.538241Z","iopub.status.idle":"2024-12-16T00:19:39.610155Z","shell.execute_reply.started":"2024-12-16T00:19:28.538218Z","shell.execute_reply":"2024-12-16T00:19:39.608976Z"}},"outputs":[{"name":"stdout","text":"Collecting yacs\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs) (6.0.2)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nInstalling collected packages: yacs\nSuccessfully installed yacs-0.1.8\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install bps\n!pip install torch-geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:19:39.611922Z","iopub.execute_input":"2024-12-16T00:19:39.612266Z","iopub.status.idle":"2024-12-16T00:19:57.209358Z","shell.execute_reply.started":"2024-12-16T00:19:39.612237Z","shell.execute_reply":"2024-12-16T00:19:57.208240Z"}},"outputs":[{"name":"stdout","text":"Collecting bps\n  Downloading BPS-0.1.0-py3-none-any.whl.metadata (61 bytes)\nDownloading BPS-0.1.0-py3-none-any.whl (897 bytes)\nInstalling collected packages: bps\nSuccessfully installed bps-0.1.0\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.6.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.6.2)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python train.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T00:19:57.210983Z","iopub.execute_input":"2024-12-16T00:19:57.211380Z","iopub.status.idle":"2024-12-16T03:10:50.705779Z","shell.execute_reply.started":"2024-12-16T00:19:57.211338Z","shell.execute_reply":"2024-12-16T03:10:50.704667Z"}},"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\nEpoch: 0, Batch(20/245), Loss: 2.5383\nEpoch: 0, Batch(40/245), Loss: 2.2219\nEpoch: 0, Batch(60/245), Loss: 2.1063\nEpoch: 0, Batch(80/245), Loss: 1.9817\nEpoch: 0, Batch(100/245), Loss: 1.8447\nEpoch: 0, Batch(120/245), Loss: 1.6789\nEpoch: 0, Batch(140/245), Loss: 1.6064\nEpoch: 0, Batch(160/245), Loss: 1.4482\nEpoch: 0, Batch(180/245), Loss: 1.4003\nEpoch: 0, Batch(200/245), Loss: 1.3578\nEpoch: 0, Batch(220/245), Loss: 1.1364\nEpoch: 0, Batch(240/245), Loss: 1.3071\nepoch 0 duration 1.570 train_loss 1.709 val_loss 0.992 val_acc 0.7500\nEpoch: 1, Batch(20/245), Loss: 1.1277\nEpoch: 1, Batch(40/245), Loss: 1.1786\nEpoch: 1, Batch(60/245), Loss: 1.0165\nEpoch: 1, Batch(80/245), Loss: 0.9470\nEpoch: 1, Batch(100/245), Loss: 1.0097\nEpoch: 1, Batch(120/245), Loss: 0.8831\nEpoch: 1, Batch(140/245), Loss: 0.8080\nEpoch: 1, Batch(160/245), Loss: 0.7624\nEpoch: 1, Batch(180/245), Loss: 0.8820\nEpoch: 1, Batch(200/245), Loss: 0.8270\nEpoch: 1, Batch(220/245), Loss: 0.8178\nEpoch: 1, Batch(240/245), Loss: 0.8333\nepoch 1 duration 1.709 train_loss 0.921 val_loss 0.627 val_acc 0.8560\nEpoch: 2, Batch(20/245), Loss: 0.6283\nEpoch: 2, Batch(40/245), Loss: 0.6709\nEpoch: 2, Batch(60/245), Loss: 0.6707\nEpoch: 2, Batch(80/245), Loss: 0.6978\nEpoch: 2, Batch(100/245), Loss: 0.5827\nEpoch: 2, Batch(120/245), Loss: 0.6955\nEpoch: 2, Batch(140/245), Loss: 0.5768\nEpoch: 2, Batch(160/245), Loss: 0.6031\nEpoch: 2, Batch(180/245), Loss: 0.5646\nEpoch: 2, Batch(200/245), Loss: 0.5724\nEpoch: 2, Batch(220/245), Loss: 0.5930\nEpoch: 2, Batch(240/245), Loss: 0.5430\nepoch 2 duration 1.707 train_loss 0.617 val_loss 0.457 val_acc 0.8821\nEpoch: 3, Batch(20/245), Loss: 0.4757\nEpoch: 3, Batch(40/245), Loss: 0.4312\nEpoch: 3, Batch(60/245), Loss: 0.4341\nEpoch: 3, Batch(80/245), Loss: 0.4479\nEpoch: 3, Batch(100/245), Loss: 0.4490\nEpoch: 3, Batch(120/245), Loss: 0.4825\nEpoch: 3, Batch(140/245), Loss: 0.5049\nEpoch: 3, Batch(160/245), Loss: 0.4830\nEpoch: 3, Batch(180/245), Loss: 0.4461\nEpoch: 3, Batch(200/245), Loss: 0.3868\nEpoch: 3, Batch(220/245), Loss: 0.4169\nEpoch: 3, Batch(240/245), Loss: 0.5176\nepoch 3 duration 1.707 train_loss 0.457 val_loss 0.431 val_acc 0.8952\nEpoch: 4, Batch(20/245), Loss: 0.3233\nEpoch: 4, Batch(40/245), Loss: 0.3708\nEpoch: 4, Batch(60/245), Loss: 0.3641\nEpoch: 4, Batch(80/245), Loss: 0.4209\nEpoch: 4, Batch(100/245), Loss: 0.3419\nEpoch: 4, Batch(120/245), Loss: 0.4049\nEpoch: 4, Batch(140/245), Loss: 0.4270\nEpoch: 4, Batch(160/245), Loss: 0.4210\nEpoch: 4, Batch(180/245), Loss: 0.3062\nEpoch: 4, Batch(200/245), Loss: 0.3721\nEpoch: 4, Batch(220/245), Loss: 0.3023\nEpoch: 4, Batch(240/245), Loss: 0.3067\nepoch 4 duration 1.710 train_loss 0.362 val_loss 0.303 val_acc 0.9262\nEpoch: 5, Batch(20/245), Loss: 0.2894\nEpoch: 5, Batch(40/245), Loss: 0.2795\nEpoch: 5, Batch(60/245), Loss: 0.3540\nEpoch: 5, Batch(80/245), Loss: 0.2730\nEpoch: 5, Batch(100/245), Loss: 0.3618\nEpoch: 5, Batch(120/245), Loss: 0.2524\nEpoch: 5, Batch(140/245), Loss: 0.3941\nEpoch: 5, Batch(160/245), Loss: 0.2622\nEpoch: 5, Batch(180/245), Loss: 0.3336\nEpoch: 5, Batch(200/245), Loss: 0.2834\nEpoch: 5, Batch(220/245), Loss: 0.3072\nEpoch: 5, Batch(240/245), Loss: 0.2918\nepoch 5 duration 1.710 train_loss 0.312 val_loss 0.342 val_acc 0.8988\nEpoch: 6, Batch(20/245), Loss: 0.2998\nEpoch: 6, Batch(40/245), Loss: 0.2716\nEpoch: 6, Batch(60/245), Loss: 0.2198\nEpoch: 6, Batch(80/245), Loss: 0.2604\nEpoch: 6, Batch(100/245), Loss: 0.2557\nEpoch: 6, Batch(120/245), Loss: 0.2433\nEpoch: 6, Batch(140/245), Loss: 0.2781\nEpoch: 6, Batch(160/245), Loss: 0.2476\nEpoch: 6, Batch(180/245), Loss: 0.2430\nEpoch: 6, Batch(200/245), Loss: 0.2445\nEpoch: 6, Batch(220/245), Loss: 0.2815\nEpoch: 6, Batch(240/245), Loss: 0.2543\nepoch 6 duration 1.708 train_loss 0.262 val_loss 0.330 val_acc 0.9119\nEpoch: 7, Batch(20/245), Loss: 0.2374\nEpoch: 7, Batch(40/245), Loss: 0.2729\nEpoch: 7, Batch(60/245), Loss: 0.2716\nEpoch: 7, Batch(80/245), Loss: 0.2622\nEpoch: 7, Batch(100/245), Loss: 0.2193\nEpoch: 7, Batch(120/245), Loss: 0.2730\nEpoch: 7, Batch(140/245), Loss: 0.2017\nEpoch: 7, Batch(160/245), Loss: 0.1941\nEpoch: 7, Batch(180/245), Loss: 0.2422\nEpoch: 7, Batch(200/245), Loss: 0.2320\nEpoch: 7, Batch(220/245), Loss: 0.2383\nEpoch: 7, Batch(240/245), Loss: 0.2746\nepoch 7 duration 1.713 train_loss 0.245 val_loss 0.292 val_acc 0.9250\nEpoch: 8, Batch(20/245), Loss: 0.2143\nEpoch: 8, Batch(40/245), Loss: 0.1963\nEpoch: 8, Batch(60/245), Loss: 0.2255\nEpoch: 8, Batch(80/245), Loss: 0.2246\nEpoch: 8, Batch(100/245), Loss: 0.2075\nEpoch: 8, Batch(120/245), Loss: 0.2399\nEpoch: 8, Batch(140/245), Loss: 0.3089\nEpoch: 8, Batch(160/245), Loss: 0.2920\nEpoch: 8, Batch(180/245), Loss: 0.2554\nEpoch: 8, Batch(200/245), Loss: 0.2427\nEpoch: 8, Batch(220/245), Loss: 0.2743\nEpoch: 8, Batch(240/245), Loss: 0.1962\nepoch 8 duration 1.702 train_loss 0.239 val_loss 0.246 val_acc 0.9417\nEpoch: 9, Batch(20/245), Loss: 0.1460\nEpoch: 9, Batch(40/245), Loss: 0.2277\nEpoch: 9, Batch(60/245), Loss: 0.2398\nEpoch: 9, Batch(80/245), Loss: 0.2369\nEpoch: 9, Batch(100/245), Loss: 0.1836\nEpoch: 9, Batch(120/245), Loss: 0.1970\nEpoch: 9, Batch(140/245), Loss: 0.2077\nEpoch: 9, Batch(160/245), Loss: 0.1840\nEpoch: 9, Batch(180/245), Loss: 0.1575\nEpoch: 9, Batch(200/245), Loss: 0.2347\nEpoch: 9, Batch(220/245), Loss: 0.1696\nEpoch: 9, Batch(240/245), Loss: 0.1930\nepoch 9 duration 1.712 train_loss 0.197 val_loss 0.295 val_acc 0.9310\nEpoch: 10, Batch(20/245), Loss: 0.1868\nEpoch: 10, Batch(40/245), Loss: 0.2149\nEpoch: 10, Batch(60/245), Loss: 0.2301\nEpoch: 10, Batch(80/245), Loss: 0.1963\nEpoch: 10, Batch(100/245), Loss: 0.1591\nEpoch: 10, Batch(120/245), Loss: 0.1665\nEpoch: 10, Batch(140/245), Loss: 0.2046\nEpoch: 10, Batch(160/245), Loss: 0.1716\nEpoch: 10, Batch(180/245), Loss: 0.1873\nEpoch: 10, Batch(200/245), Loss: 0.1592\nEpoch: 10, Batch(220/245), Loss: 0.1650\nEpoch: 10, Batch(240/245), Loss: 0.1626\nepoch 10 duration 1.711 train_loss 0.185 val_loss 0.259 val_acc 0.9238\nEpoch: 11, Batch(20/245), Loss: 0.1811\nEpoch: 11, Batch(40/245), Loss: 0.1313\nEpoch: 11, Batch(60/245), Loss: 0.1911\nEpoch: 11, Batch(80/245), Loss: 0.1854\nEpoch: 11, Batch(100/245), Loss: 0.2005\nEpoch: 11, Batch(120/245), Loss: 0.1561\nEpoch: 11, Batch(140/245), Loss: 0.1294\nEpoch: 11, Batch(160/245), Loss: 0.1788\nEpoch: 11, Batch(180/245), Loss: 0.1726\nEpoch: 11, Batch(200/245), Loss: 0.1882\nEpoch: 11, Batch(220/245), Loss: 0.1571\nEpoch: 11, Batch(240/245), Loss: 0.1623\nepoch 11 duration 1.706 train_loss 0.173 val_loss 0.237 val_acc 0.9357\nEpoch: 12, Batch(20/245), Loss: 0.1190\nEpoch: 12, Batch(40/245), Loss: 0.1244\nEpoch: 12, Batch(60/245), Loss: 0.1452\nEpoch: 12, Batch(80/245), Loss: 0.2446\nEpoch: 12, Batch(100/245), Loss: 0.1919\nEpoch: 12, Batch(120/245), Loss: 0.1822\nEpoch: 12, Batch(140/245), Loss: 0.1628\nEpoch: 12, Batch(160/245), Loss: 0.1910\nEpoch: 12, Batch(180/245), Loss: 0.1775\nEpoch: 12, Batch(200/245), Loss: 0.1576\nEpoch: 12, Batch(220/245), Loss: 0.1879\nEpoch: 12, Batch(240/245), Loss: 0.2035\nepoch 12 duration 1.708 train_loss 0.173 val_loss 0.257 val_acc 0.9357\nEpoch: 13, Batch(20/245), Loss: 0.1962\nEpoch: 13, Batch(40/245), Loss: 0.1355\nEpoch: 13, Batch(60/245), Loss: 0.1899\nEpoch: 13, Batch(80/245), Loss: 0.1305\nEpoch: 13, Batch(100/245), Loss: 0.1936\nEpoch: 13, Batch(120/245), Loss: 0.2521\nEpoch: 13, Batch(140/245), Loss: 0.1660\nEpoch: 13, Batch(160/245), Loss: 0.1427\nEpoch: 13, Batch(180/245), Loss: 0.1550\nEpoch: 13, Batch(200/245), Loss: 0.1701\nEpoch: 13, Batch(220/245), Loss: 0.1681\nEpoch: 13, Batch(240/245), Loss: 0.1573\nepoch 13 duration 1.708 train_loss 0.174 val_loss 0.319 val_acc 0.9143\nEpoch: 14, Batch(20/245), Loss: 0.1208\nEpoch: 14, Batch(40/245), Loss: 0.1327\nEpoch: 14, Batch(60/245), Loss: 0.1279\nEpoch: 14, Batch(80/245), Loss: 0.1387\nEpoch: 14, Batch(100/245), Loss: 0.1424\nEpoch: 14, Batch(120/245), Loss: 0.1175\nEpoch: 14, Batch(140/245), Loss: 0.1134\nEpoch: 14, Batch(160/245), Loss: 0.1575\nEpoch: 14, Batch(180/245), Loss: 0.1713\nEpoch: 14, Batch(200/245), Loss: 0.1288\nEpoch: 14, Batch(220/245), Loss: 0.1389\nEpoch: 14, Batch(240/245), Loss: 0.1834\nepoch 14 duration 1.711 train_loss 0.138 val_loss 0.400 val_acc 0.8893\nEpoch: 15, Batch(20/245), Loss: 0.1687\nEpoch: 15, Batch(40/245), Loss: 0.1640\nEpoch: 15, Batch(60/245), Loss: 0.1844\nEpoch: 15, Batch(80/245), Loss: 0.1848\nEpoch: 15, Batch(100/245), Loss: 0.2002\nEpoch: 15, Batch(120/245), Loss: 0.1678\nEpoch: 15, Batch(140/245), Loss: 0.1334\nEpoch: 15, Batch(160/245), Loss: 0.1860\nEpoch: 15, Batch(180/245), Loss: 0.2067\nEpoch: 15, Batch(200/245), Loss: 0.2659\nEpoch: 15, Batch(220/245), Loss: 0.1623\nEpoch: 15, Batch(240/245), Loss: 0.1229\nepoch 15 duration 1.708 train_loss 0.177 val_loss 0.295 val_acc 0.9143\nEpoch: 16, Batch(20/245), Loss: 0.1350\nEpoch: 16, Batch(40/245), Loss: 0.1435\nEpoch: 16, Batch(60/245), Loss: 0.1208\nEpoch: 16, Batch(80/245), Loss: 0.1933\nEpoch: 16, Batch(100/245), Loss: 0.1256\nEpoch: 16, Batch(120/245), Loss: 0.1377\nEpoch: 16, Batch(140/245), Loss: 0.1412\nEpoch: 16, Batch(160/245), Loss: 0.1989\nEpoch: 16, Batch(180/245), Loss: 0.1454\nEpoch: 16, Batch(200/245), Loss: 0.1384\nEpoch: 16, Batch(220/245), Loss: 0.1345\nEpoch: 16, Batch(240/245), Loss: 0.2253\nepoch 16 duration 1.705 train_loss 0.153 val_loss 0.365 val_acc 0.9083\nEpoch: 17, Batch(20/245), Loss: 0.1388\nEpoch: 17, Batch(40/245), Loss: 0.1066\nEpoch: 17, Batch(60/245), Loss: 0.1253\nEpoch: 17, Batch(80/245), Loss: 0.1488\nEpoch: 17, Batch(100/245), Loss: 0.1038\nEpoch: 17, Batch(120/245), Loss: 0.1319\nEpoch: 17, Batch(140/245), Loss: 0.1841\nEpoch: 17, Batch(160/245), Loss: 0.1738\nEpoch: 17, Batch(180/245), Loss: 0.1621\nEpoch: 17, Batch(200/245), Loss: 0.1362\nEpoch: 17, Batch(220/245), Loss: 0.2158\nEpoch: 17, Batch(240/245), Loss: 0.1886\nepoch 17 duration 1.707 train_loss 0.151 val_loss 0.257 val_acc 0.9345\nEpoch: 18, Batch(20/245), Loss: 0.1613\nEpoch: 18, Batch(40/245), Loss: 0.2103\nEpoch: 18, Batch(60/245), Loss: 0.1255\nEpoch: 18, Batch(80/245), Loss: 0.1539\nEpoch: 18, Batch(100/245), Loss: 0.1840\nEpoch: 18, Batch(120/245), Loss: 0.1321\nEpoch: 18, Batch(140/245), Loss: 0.1468\nEpoch: 18, Batch(160/245), Loss: 0.1163\nEpoch: 18, Batch(180/245), Loss: 0.1530\nEpoch: 18, Batch(200/245), Loss: 0.1235\nEpoch: 18, Batch(220/245), Loss: 0.1735\nEpoch: 18, Batch(240/245), Loss: 0.1725\nepoch 18 duration 1.707 train_loss 0.153 val_loss 0.256 val_acc 0.9345\nEpoch: 19, Batch(20/245), Loss: 0.1042\nEpoch: 19, Batch(40/245), Loss: 0.1115\nEpoch: 19, Batch(60/245), Loss: 0.0939\nEpoch: 19, Batch(80/245), Loss: 0.0822\nEpoch: 19, Batch(100/245), Loss: 0.1094\nEpoch: 19, Batch(120/245), Loss: 0.1044\nEpoch: 19, Batch(140/245), Loss: 0.1297\nEpoch: 19, Batch(160/245), Loss: 0.1274\nEpoch: 19, Batch(180/245), Loss: 0.1379\nEpoch: 19, Batch(200/245), Loss: 0.1300\nEpoch: 19, Batch(220/245), Loss: 0.1451\nEpoch: 19, Batch(240/245), Loss: 0.1887\nepoch 19 duration 1.709 train_loss 0.125 val_loss 0.217 val_acc 0.9524\nEpoch: 20, Batch(20/245), Loss: 0.1034\nEpoch: 20, Batch(40/245), Loss: 0.1489\nEpoch: 20, Batch(60/245), Loss: 0.1562\nEpoch: 20, Batch(80/245), Loss: 0.0918\nEpoch: 20, Batch(100/245), Loss: 0.0946\nEpoch: 20, Batch(120/245), Loss: 0.0696\nEpoch: 20, Batch(140/245), Loss: 0.0897\nEpoch: 20, Batch(160/245), Loss: 0.1037\nEpoch: 20, Batch(180/245), Loss: 0.1191\nEpoch: 20, Batch(200/245), Loss: 0.1103\nEpoch: 20, Batch(220/245), Loss: 0.1256\nEpoch: 20, Batch(240/245), Loss: 0.1143\nepoch 20 duration 1.708 train_loss 0.113 val_loss 0.317 val_acc 0.9250\nEpoch: 21, Batch(20/245), Loss: 0.1488\nEpoch: 21, Batch(40/245), Loss: 0.0881\nEpoch: 21, Batch(60/245), Loss: 0.1188\nEpoch: 21, Batch(80/245), Loss: 0.1935\nEpoch: 21, Batch(100/245), Loss: 0.1796\nEpoch: 21, Batch(120/245), Loss: 0.2429\nEpoch: 21, Batch(140/245), Loss: 0.1200\nEpoch: 21, Batch(160/245), Loss: 0.1702\nEpoch: 21, Batch(180/245), Loss: 0.1461\nEpoch: 21, Batch(200/245), Loss: 0.1464\nEpoch: 21, Batch(220/245), Loss: 0.1284\nEpoch: 21, Batch(240/245), Loss: 0.1195\nepoch 21 duration 1.707 train_loss 0.152 val_loss 0.227 val_acc 0.9381\nEpoch: 22, Batch(20/245), Loss: 0.1026\nEpoch: 22, Batch(40/245), Loss: 0.1575\nEpoch: 22, Batch(60/245), Loss: 0.1000\nEpoch: 22, Batch(80/245), Loss: 0.1826\nEpoch: 22, Batch(100/245), Loss: 0.1092\nEpoch: 22, Batch(120/245), Loss: 0.1104\nEpoch: 22, Batch(140/245), Loss: 0.1220\nEpoch: 22, Batch(160/245), Loss: 0.1606\nEpoch: 22, Batch(180/245), Loss: 0.1260\nEpoch: 22, Batch(200/245), Loss: 0.1817\nEpoch: 22, Batch(220/245), Loss: 0.1690\nEpoch: 22, Batch(240/245), Loss: 0.1438\nepoch 22 duration 1.706 train_loss 0.138 val_loss 0.184 val_acc 0.9500\nEpoch: 23, Batch(20/245), Loss: 0.1367\nEpoch: 23, Batch(40/245), Loss: 0.1289\nEpoch: 23, Batch(60/245), Loss: 0.1404\nEpoch: 23, Batch(80/245), Loss: 0.0991\nEpoch: 23, Batch(100/245), Loss: 0.0953\nEpoch: 23, Batch(120/245), Loss: 0.0922\nEpoch: 23, Batch(140/245), Loss: 0.1287\nEpoch: 23, Batch(160/245), Loss: 0.1599\nEpoch: 23, Batch(180/245), Loss: 0.1673\nEpoch: 23, Batch(200/245), Loss: 0.1856\nEpoch: 23, Batch(220/245), Loss: 0.1567\nEpoch: 23, Batch(240/245), Loss: 0.1361\nepoch 23 duration 1.711 train_loss 0.136 val_loss 0.533 val_acc 0.8583\nEpoch: 24, Batch(20/245), Loss: 0.1662\nEpoch: 24, Batch(40/245), Loss: 0.1618\nEpoch: 24, Batch(60/245), Loss: 0.1229\nEpoch: 24, Batch(80/245), Loss: 0.1082\nEpoch: 24, Batch(100/245), Loss: 0.1531\nEpoch: 24, Batch(120/245), Loss: 0.1031\nEpoch: 24, Batch(140/245), Loss: 0.1664\nEpoch: 24, Batch(160/245), Loss: 0.1237\nEpoch: 24, Batch(180/245), Loss: 0.1973\nEpoch: 24, Batch(200/245), Loss: 0.0968\nEpoch: 24, Batch(220/245), Loss: 0.1322\nEpoch: 24, Batch(240/245), Loss: 0.1168\nepoch 24 duration 1.712 train_loss 0.138 val_loss 0.239 val_acc 0.9369\nEpoch: 25, Batch(20/245), Loss: 0.0798\nEpoch: 25, Batch(40/245), Loss: 0.1029\nEpoch: 25, Batch(60/245), Loss: 0.0915\nEpoch: 25, Batch(80/245), Loss: 0.1053\nEpoch: 25, Batch(100/245), Loss: 0.1426\nEpoch: 25, Batch(120/245), Loss: 0.1197\nEpoch: 25, Batch(140/245), Loss: 0.1951\nEpoch: 25, Batch(160/245), Loss: 0.1027\nEpoch: 25, Batch(180/245), Loss: 0.1158\nEpoch: 25, Batch(200/245), Loss: 0.1382\nEpoch: 25, Batch(220/245), Loss: 0.1152\nEpoch: 25, Batch(240/245), Loss: 0.1330\nepoch 25 duration 1.707 train_loss 0.120 val_loss 0.207 val_acc 0.9500\nEpoch: 26, Batch(20/245), Loss: 0.0894\nEpoch: 26, Batch(40/245), Loss: 0.1066\nEpoch: 26, Batch(60/245), Loss: 0.1507\nEpoch: 26, Batch(80/245), Loss: 0.1656\nEpoch: 26, Batch(100/245), Loss: 0.1197\nEpoch: 26, Batch(120/245), Loss: 0.1166\nEpoch: 26, Batch(140/245), Loss: 0.1714\nEpoch: 26, Batch(160/245), Loss: 0.1390\nEpoch: 26, Batch(180/245), Loss: 0.1238\nEpoch: 26, Batch(200/245), Loss: 0.1166\nEpoch: 26, Batch(220/245), Loss: 0.1534\nEpoch: 26, Batch(240/245), Loss: 0.1080\nepoch 26 duration 1.708 train_loss 0.130 val_loss 0.243 val_acc 0.9369\nEpoch: 27, Batch(20/245), Loss: 0.0733\nEpoch: 27, Batch(40/245), Loss: 0.0791\nEpoch: 27, Batch(60/245), Loss: 0.0979\nEpoch: 27, Batch(80/245), Loss: 0.1084\nEpoch: 27, Batch(100/245), Loss: 0.1192\nEpoch: 27, Batch(120/245), Loss: 0.1329\nEpoch: 27, Batch(140/245), Loss: 0.1870\nEpoch: 27, Batch(160/245), Loss: 0.1404\nEpoch: 27, Batch(180/245), Loss: 0.1472\nEpoch: 27, Batch(200/245), Loss: 0.1606\nEpoch: 27, Batch(220/245), Loss: 0.0963\nEpoch: 27, Batch(240/245), Loss: 0.1137\nepoch 27 duration 1.705 train_loss 0.121 val_loss 0.239 val_acc 0.9440\nEpoch: 28, Batch(20/245), Loss: 0.0986\nEpoch: 28, Batch(40/245), Loss: 0.0943\nEpoch: 28, Batch(60/245), Loss: 0.1135\nEpoch: 28, Batch(80/245), Loss: 0.1297\nEpoch: 28, Batch(100/245), Loss: 0.0962\nEpoch: 28, Batch(120/245), Loss: 0.1318\nEpoch: 28, Batch(140/245), Loss: 0.1733\nEpoch: 28, Batch(160/245), Loss: 0.0988\nEpoch: 28, Batch(180/245), Loss: 0.1049\nEpoch: 28, Batch(200/245), Loss: 0.1484\nEpoch: 28, Batch(220/245), Loss: 0.1498\nEpoch: 28, Batch(240/245), Loss: 0.1491\nepoch 28 duration 1.707 train_loss 0.124 val_loss 0.219 val_acc 0.9512\nEpoch: 29, Batch(20/245), Loss: 0.1063\nEpoch: 29, Batch(40/245), Loss: 0.1030\nEpoch: 29, Batch(60/245), Loss: 0.1007\nEpoch: 29, Batch(80/245), Loss: 0.1184\nEpoch: 29, Batch(100/245), Loss: 0.1397\nEpoch: 29, Batch(120/245), Loss: 0.1221\nEpoch: 29, Batch(140/245), Loss: 0.0730\nEpoch: 29, Batch(160/245), Loss: 0.1272\nEpoch: 29, Batch(180/245), Loss: 0.1030\nEpoch: 29, Batch(200/245), Loss: 0.1075\nEpoch: 29, Batch(220/245), Loss: 0.1151\nEpoch: 29, Batch(240/245), Loss: 0.1026\nepoch 29 duration 1.706 train_loss 0.110 val_loss 0.253 val_acc 0.9345\nEpoch: 30, Batch(20/245), Loss: 0.1678\nEpoch: 30, Batch(40/245), Loss: 0.0856\nEpoch: 30, Batch(60/245), Loss: 0.1239\nEpoch: 30, Batch(80/245), Loss: 0.0676\nEpoch: 30, Batch(100/245), Loss: 0.0887\nEpoch: 30, Batch(120/245), Loss: 0.0872\nEpoch: 30, Batch(140/245), Loss: 0.0850\nEpoch: 30, Batch(160/245), Loss: 0.1860\nEpoch: 30, Batch(180/245), Loss: 0.1704\nEpoch: 30, Batch(200/245), Loss: 0.1177\nEpoch: 30, Batch(220/245), Loss: 0.1540\nEpoch: 30, Batch(240/245), Loss: 0.1234\nepoch 30 duration 1.711 train_loss 0.124 val_loss 0.261 val_acc 0.9357\nEpoch: 31, Batch(20/245), Loss: 0.1026\nEpoch: 31, Batch(40/245), Loss: 0.1680\nEpoch: 31, Batch(60/245), Loss: 0.2233\nEpoch: 31, Batch(80/245), Loss: 0.1469\nEpoch: 31, Batch(100/245), Loss: 0.1134\nEpoch: 31, Batch(120/245), Loss: 0.0950\nEpoch: 31, Batch(140/245), Loss: 0.1369\nEpoch: 31, Batch(160/245), Loss: 0.1229\nEpoch: 31, Batch(180/245), Loss: 0.1400\nEpoch: 31, Batch(200/245), Loss: 0.1350\nEpoch: 31, Batch(220/245), Loss: 0.0928\nEpoch: 31, Batch(240/245), Loss: 0.0803\nepoch 31 duration 1.708 train_loss 0.129 val_loss 0.242 val_acc 0.9333\nEpoch: 32, Batch(20/245), Loss: 0.1597\nEpoch: 32, Batch(40/245), Loss: 0.1739\nEpoch: 32, Batch(60/245), Loss: 0.1183\nEpoch: 32, Batch(80/245), Loss: 0.1165\nEpoch: 32, Batch(100/245), Loss: 0.1369\nEpoch: 32, Batch(120/245), Loss: 0.1560\nEpoch: 32, Batch(140/245), Loss: 0.0903\nEpoch: 32, Batch(160/245), Loss: 0.2119\nEpoch: 32, Batch(180/245), Loss: 0.1517\nEpoch: 32, Batch(200/245), Loss: 0.1904\nEpoch: 32, Batch(220/245), Loss: 0.1668\nEpoch: 32, Batch(240/245), Loss: 0.1126\nepoch 32 duration 1.706 train_loss 0.149 val_loss 0.185 val_acc 0.9536\nEpoch: 33, Batch(20/245), Loss: 0.1213\nEpoch: 33, Batch(40/245), Loss: 0.1073\nEpoch: 33, Batch(60/245), Loss: 0.1889\nEpoch: 33, Batch(80/245), Loss: 0.1713\nEpoch: 33, Batch(100/245), Loss: 0.1369\nEpoch: 33, Batch(120/245), Loss: 0.1136\nEpoch: 33, Batch(140/245), Loss: 0.0984\nEpoch: 33, Batch(160/245), Loss: 0.0657\nEpoch: 33, Batch(180/245), Loss: 0.1554\nEpoch: 33, Batch(200/245), Loss: 0.1598\nEpoch: 33, Batch(220/245), Loss: 0.1451\nEpoch: 33, Batch(240/245), Loss: 0.0919\nepoch 33 duration 1.707 train_loss 0.129 val_loss 0.329 val_acc 0.9179\nEpoch: 34, Batch(20/245), Loss: 0.0842\nEpoch: 34, Batch(40/245), Loss: 0.0758\nEpoch: 34, Batch(60/245), Loss: 0.0862\nEpoch: 34, Batch(80/245), Loss: 0.0743\nEpoch: 34, Batch(100/245), Loss: 0.0560\nEpoch: 34, Batch(120/245), Loss: 0.1054\nEpoch: 34, Batch(140/245), Loss: 0.1193\nEpoch: 34, Batch(160/245), Loss: 0.1136\nEpoch: 34, Batch(180/245), Loss: 0.1080\nEpoch: 34, Batch(200/245), Loss: 0.1319\nEpoch: 34, Batch(220/245), Loss: 0.1142\nEpoch: 34, Batch(240/245), Loss: 0.0792\nepoch 34 duration 1.710 train_loss 0.096 val_loss 0.191 val_acc 0.9548\nEpoch: 35, Batch(20/245), Loss: 0.1058\nEpoch: 35, Batch(40/245), Loss: 0.0748\nEpoch: 35, Batch(60/245), Loss: 0.0601\nEpoch: 35, Batch(80/245), Loss: 0.0505\nEpoch: 35, Batch(100/245), Loss: 0.1076\nEpoch: 35, Batch(120/245), Loss: 0.0634\nEpoch: 35, Batch(140/245), Loss: 0.1325\nEpoch: 35, Batch(160/245), Loss: 0.1613\nEpoch: 35, Batch(180/245), Loss: 0.1312\nEpoch: 35, Batch(200/245), Loss: 0.1026\nEpoch: 35, Batch(220/245), Loss: 0.1176\nEpoch: 35, Batch(240/245), Loss: 0.1744\nepoch 35 duration 1.707 train_loss 0.106 val_loss 0.263 val_acc 0.9393\nEpoch: 36, Batch(20/245), Loss: 0.0897\nEpoch: 36, Batch(40/245), Loss: 0.0710\nEpoch: 36, Batch(60/245), Loss: 0.0915\nEpoch: 36, Batch(80/245), Loss: 0.0795\nEpoch: 36, Batch(100/245), Loss: 0.1395\nEpoch: 36, Batch(120/245), Loss: 0.1482\nEpoch: 36, Batch(140/245), Loss: 0.1645\nEpoch: 36, Batch(160/245), Loss: 0.1443\nEpoch: 36, Batch(180/245), Loss: 0.1209\nEpoch: 36, Batch(200/245), Loss: 0.1250\nEpoch: 36, Batch(220/245), Loss: 0.2190\nEpoch: 36, Batch(240/245), Loss: 0.1235\nepoch 36 duration 1.709 train_loss 0.127 val_loss 0.263 val_acc 0.9250\nEpoch: 37, Batch(20/245), Loss: 0.0642\nEpoch: 37, Batch(40/245), Loss: 0.1298\nEpoch: 37, Batch(60/245), Loss: 0.0937\nEpoch: 37, Batch(80/245), Loss: 0.1140\nEpoch: 37, Batch(100/245), Loss: 0.1153\nEpoch: 37, Batch(120/245), Loss: 0.0995\nEpoch: 37, Batch(140/245), Loss: 0.1450\nEpoch: 37, Batch(160/245), Loss: 0.1743\nEpoch: 37, Batch(180/245), Loss: 0.1561\nEpoch: 37, Batch(200/245), Loss: 0.1125\nEpoch: 37, Batch(220/245), Loss: 0.0915\nEpoch: 37, Batch(240/245), Loss: 0.0589\nepoch 37 duration 1.706 train_loss 0.113 val_loss 0.187 val_acc 0.9548\nEpoch: 38, Batch(20/245), Loss: 0.1040\nEpoch: 38, Batch(40/245), Loss: 0.0960\nEpoch: 38, Batch(60/245), Loss: 0.1317\nEpoch: 38, Batch(80/245), Loss: 0.0879\nEpoch: 38, Batch(100/245), Loss: 0.1396\nEpoch: 38, Batch(120/245), Loss: 0.1525\nEpoch: 38, Batch(140/245), Loss: 0.1163\nEpoch: 38, Batch(160/245), Loss: 0.0922\nEpoch: 38, Batch(180/245), Loss: 0.0715\nEpoch: 38, Batch(200/245), Loss: 0.1195\nEpoch: 38, Batch(220/245), Loss: 0.1158\nEpoch: 38, Batch(240/245), Loss: 0.1923\nepoch 38 duration 1.703 train_loss 0.118 val_loss 0.211 val_acc 0.9464\nEpoch: 39, Batch(20/245), Loss: 0.1015\nEpoch: 39, Batch(40/245), Loss: 0.1101\nEpoch: 39, Batch(60/245), Loss: 0.1306\nEpoch: 39, Batch(80/245), Loss: 0.0621\nEpoch: 39, Batch(100/245), Loss: 0.0657\nEpoch: 39, Batch(120/245), Loss: 0.1027\nEpoch: 39, Batch(140/245), Loss: 0.1244\nEpoch: 39, Batch(160/245), Loss: 0.0994\nEpoch: 39, Batch(180/245), Loss: 0.1323\nEpoch: 39, Batch(200/245), Loss: 0.1364\nEpoch: 39, Batch(220/245), Loss: 0.1810\nEpoch: 39, Batch(240/245), Loss: 0.2054\nepoch 39 duration 1.704 train_loss 0.121 val_loss 0.186 val_acc 0.9536\nEpoch: 40, Batch(20/245), Loss: 0.1801\nEpoch: 40, Batch(40/245), Loss: 0.1198\nEpoch: 40, Batch(60/245), Loss: 0.0783\nEpoch: 40, Batch(80/245), Loss: 0.1008\nEpoch: 40, Batch(100/245), Loss: 0.1009\nEpoch: 40, Batch(120/245), Loss: 0.1137\nEpoch: 40, Batch(140/245), Loss: 0.1527\nEpoch: 40, Batch(160/245), Loss: 0.1831\nEpoch: 40, Batch(180/245), Loss: 0.2453\nEpoch: 40, Batch(200/245), Loss: 0.1212\nEpoch: 40, Batch(220/245), Loss: 0.1308\nEpoch: 40, Batch(240/245), Loss: 0.1507\nepoch 40 duration 1.705 train_loss 0.139 val_loss 0.303 val_acc 0.9238\nEpoch: 41, Batch(20/245), Loss: 0.1353\nEpoch: 41, Batch(40/245), Loss: 0.1158\nEpoch: 41, Batch(60/245), Loss: 0.1571\nEpoch: 41, Batch(80/245), Loss: 0.1567\nEpoch: 41, Batch(100/245), Loss: 0.0962\nEpoch: 41, Batch(120/245), Loss: 0.1169\nEpoch: 41, Batch(140/245), Loss: 0.1318\nEpoch: 41, Batch(160/245), Loss: 0.1253\nEpoch: 41, Batch(180/245), Loss: 0.0748\nEpoch: 41, Batch(200/245), Loss: 0.0857\nEpoch: 41, Batch(220/245), Loss: 0.0912\nEpoch: 41, Batch(240/245), Loss: 0.0771\nepoch 41 duration 1.706 train_loss 0.113 val_loss 0.193 val_acc 0.9488\nEpoch: 42, Batch(20/245), Loss: 0.0908\nEpoch: 42, Batch(40/245), Loss: 0.0589\nEpoch: 42, Batch(60/245), Loss: 0.0901\nEpoch: 42, Batch(80/245), Loss: 0.0840\nEpoch: 42, Batch(100/245), Loss: 0.1138\nEpoch: 42, Batch(120/245), Loss: 0.1432\nEpoch: 42, Batch(140/245), Loss: 0.1200\nEpoch: 42, Batch(160/245), Loss: 0.1217\nEpoch: 42, Batch(180/245), Loss: 0.1302\nEpoch: 42, Batch(200/245), Loss: 0.1233\nEpoch: 42, Batch(220/245), Loss: 0.1209\nEpoch: 42, Batch(240/245), Loss: 0.1407\nepoch 42 duration 1.708 train_loss 0.112 val_loss 0.229 val_acc 0.9345\nEpoch: 43, Batch(20/245), Loss: 0.0597\nEpoch: 43, Batch(40/245), Loss: 0.1044\nEpoch: 43, Batch(60/245), Loss: 0.1037\nEpoch: 43, Batch(80/245), Loss: 0.1245\nEpoch: 43, Batch(100/245), Loss: 0.1172\nEpoch: 43, Batch(120/245), Loss: 0.1349\nEpoch: 43, Batch(140/245), Loss: 0.1289\nEpoch: 43, Batch(160/245), Loss: 0.1935\nEpoch: 43, Batch(180/245), Loss: 0.1293\nEpoch: 43, Batch(200/245), Loss: 0.0980\nEpoch: 43, Batch(220/245), Loss: 0.0965\nEpoch: 43, Batch(240/245), Loss: 0.0957\nepoch 43 duration 1.707 train_loss 0.115 val_loss 0.190 val_acc 0.9500\nEpoch: 44, Batch(20/245), Loss: 0.1108\nEpoch: 44, Batch(40/245), Loss: 0.0990\nEpoch: 44, Batch(60/245), Loss: 0.1248\nEpoch: 44, Batch(80/245), Loss: 0.0927\nEpoch: 44, Batch(100/245), Loss: 0.1123\nEpoch: 44, Batch(120/245), Loss: 0.1588\nEpoch: 44, Batch(140/245), Loss: 0.0944\nEpoch: 44, Batch(160/245), Loss: 0.1018\nEpoch: 44, Batch(180/245), Loss: 0.0732\nEpoch: 44, Batch(200/245), Loss: 0.0651\nEpoch: 44, Batch(220/245), Loss: 0.0813\nEpoch: 44, Batch(240/245), Loss: 0.1183\nepoch 44 duration 1.706 train_loss 0.105 val_loss 0.286 val_acc 0.9298\nEpoch: 45, Batch(20/245), Loss: 0.1497\nEpoch: 45, Batch(40/245), Loss: 0.0751\nEpoch: 45, Batch(60/245), Loss: 0.0684\nEpoch: 45, Batch(80/245), Loss: 0.0391\nEpoch: 45, Batch(100/245), Loss: 0.0810\nEpoch: 45, Batch(120/245), Loss: 0.0830\nEpoch: 45, Batch(140/245), Loss: 0.0863\nEpoch: 45, Batch(160/245), Loss: 0.1262\nEpoch: 45, Batch(180/245), Loss: 0.1325\nEpoch: 45, Batch(200/245), Loss: 0.1092\nEpoch: 45, Batch(220/245), Loss: 0.1364\nEpoch: 45, Batch(240/245), Loss: 0.1469\nepoch 45 duration 1.707 train_loss 0.103 val_loss 0.490 val_acc 0.8750\nEpoch: 46, Batch(20/245), Loss: 0.1135\nEpoch: 46, Batch(40/245), Loss: 0.0908\nEpoch: 46, Batch(60/245), Loss: 0.0888\nEpoch: 46, Batch(80/245), Loss: 0.0778\nEpoch: 46, Batch(100/245), Loss: 0.1493\nEpoch: 46, Batch(120/245), Loss: 0.1413\nEpoch: 46, Batch(140/245), Loss: 0.0641\nEpoch: 46, Batch(160/245), Loss: 0.1100\nEpoch: 46, Batch(180/245), Loss: 0.1058\nEpoch: 46, Batch(200/245), Loss: 0.0706\nEpoch: 46, Batch(220/245), Loss: 0.1206\nEpoch: 46, Batch(240/245), Loss: 0.1740\nepoch 46 duration 1.704 train_loss 0.108 val_loss 0.218 val_acc 0.9440\nEpoch: 47, Batch(20/245), Loss: 0.1459\nEpoch: 47, Batch(40/245), Loss: 0.1962\nEpoch: 47, Batch(60/245), Loss: 0.1791\nEpoch: 47, Batch(80/245), Loss: 0.1073\nEpoch: 47, Batch(100/245), Loss: 0.1243\nEpoch: 47, Batch(120/245), Loss: 0.0680\nEpoch: 47, Batch(140/245), Loss: 0.0942\nEpoch: 47, Batch(160/245), Loss: 0.1127\nEpoch: 47, Batch(180/245), Loss: 0.1339\nEpoch: 47, Batch(200/245), Loss: 0.1359\nEpoch: 47, Batch(220/245), Loss: 0.2313\nEpoch: 47, Batch(240/245), Loss: 0.1873\nepoch 47 duration 1.705 train_loss 0.143 val_loss 0.252 val_acc 0.9405\nEpoch: 48, Batch(20/245), Loss: 0.1607\nEpoch: 48, Batch(40/245), Loss: 0.1113\nEpoch: 48, Batch(60/245), Loss: 0.1066\nEpoch: 48, Batch(80/245), Loss: 0.0758\nEpoch: 48, Batch(100/245), Loss: 0.0930\nEpoch: 48, Batch(120/245), Loss: 0.0629\nEpoch: 48, Batch(140/245), Loss: 0.0624\nEpoch: 48, Batch(160/245), Loss: 0.1408\nEpoch: 48, Batch(180/245), Loss: 0.1526\nEpoch: 48, Batch(200/245), Loss: 0.1647\nEpoch: 48, Batch(220/245), Loss: 0.1623\nEpoch: 48, Batch(240/245), Loss: 0.1886\nepoch 48 duration 1.705 train_loss 0.122 val_loss 0.170 val_acc 0.9571\nEpoch: 49, Batch(20/245), Loss: 0.1059\nEpoch: 49, Batch(40/245), Loss: 0.1835\nEpoch: 49, Batch(60/245), Loss: 0.0586\nEpoch: 49, Batch(80/245), Loss: 0.0862\nEpoch: 49, Batch(100/245), Loss: 0.1065\nEpoch: 49, Batch(120/245), Loss: 0.0995\nEpoch: 49, Batch(140/245), Loss: 0.0996\nEpoch: 49, Batch(160/245), Loss: 0.0507\nEpoch: 49, Batch(180/245), Loss: 0.0913\nEpoch: 49, Batch(200/245), Loss: 0.0913\nEpoch: 49, Batch(220/245), Loss: 0.0607\nEpoch: 49, Batch(240/245), Loss: 0.0785\nepoch 49 duration 1.708 train_loss 0.092 val_loss 0.224 val_acc 0.9369\nEpoch: 50, Batch(20/245), Loss: 0.1014\nEpoch: 50, Batch(40/245), Loss: 0.0842\nEpoch: 50, Batch(60/245), Loss: 0.1436\nEpoch: 50, Batch(80/245), Loss: 0.0927\nEpoch: 50, Batch(100/245), Loss: 0.0599\nEpoch: 50, Batch(120/245), Loss: 0.0762\nEpoch: 50, Batch(140/245), Loss: 0.0383\nEpoch: 50, Batch(160/245), Loss: 0.0967\nEpoch: 50, Batch(180/245), Loss: 0.1182\nEpoch: 50, Batch(200/245), Loss: 0.0773\nEpoch: 50, Batch(220/245), Loss: 0.0449\nEpoch: 50, Batch(240/245), Loss: 0.0663\nepoch 50 duration 1.706 train_loss 0.084 val_loss 0.150 val_acc 0.9607\nEpoch: 51, Batch(20/245), Loss: 0.0612\nEpoch: 51, Batch(40/245), Loss: 0.0650\nEpoch: 51, Batch(60/245), Loss: 0.0434\nEpoch: 51, Batch(80/245), Loss: 0.0407\nEpoch: 51, Batch(100/245), Loss: 0.0571\nEpoch: 51, Batch(120/245), Loss: 0.0690\nEpoch: 51, Batch(140/245), Loss: 0.0501\nEpoch: 51, Batch(160/245), Loss: 0.0660\nEpoch: 51, Batch(180/245), Loss: 0.0537\nEpoch: 51, Batch(200/245), Loss: 0.0785\nEpoch: 51, Batch(220/245), Loss: 0.0407\nEpoch: 51, Batch(240/245), Loss: 0.0657\nepoch 51 duration 1.712 train_loss 0.058 val_loss 0.147 val_acc 0.9631\nEpoch: 52, Batch(20/245), Loss: 0.0735\nEpoch: 52, Batch(40/245), Loss: 0.0451\nEpoch: 52, Batch(60/245), Loss: 0.0638\nEpoch: 52, Batch(80/245), Loss: 0.0342\nEpoch: 52, Batch(100/245), Loss: 0.0396\nEpoch: 52, Batch(120/245), Loss: 0.0506\nEpoch: 52, Batch(140/245), Loss: 0.0740\nEpoch: 52, Batch(160/245), Loss: 0.0801\nEpoch: 52, Batch(180/245), Loss: 0.0304\nEpoch: 52, Batch(200/245), Loss: 0.0709\nEpoch: 52, Batch(220/245), Loss: 0.0475\nEpoch: 52, Batch(240/245), Loss: 0.0378\nepoch 52 duration 1.706 train_loss 0.053 val_loss 0.155 val_acc 0.9607\nEpoch: 53, Batch(20/245), Loss: 0.0436\nEpoch: 53, Batch(40/245), Loss: 0.0357\nEpoch: 53, Batch(60/245), Loss: 0.0597\nEpoch: 53, Batch(80/245), Loss: 0.0415\nEpoch: 53, Batch(100/245), Loss: 0.0609\nEpoch: 53, Batch(120/245), Loss: 0.0634\nEpoch: 53, Batch(140/245), Loss: 0.0506\nEpoch: 53, Batch(160/245), Loss: 0.0438\nEpoch: 53, Batch(180/245), Loss: 0.0790\nEpoch: 53, Batch(200/245), Loss: 0.0392\nEpoch: 53, Batch(220/245), Loss: 0.0304\nEpoch: 53, Batch(240/245), Loss: 0.0528\nepoch 53 duration 1.705 train_loss 0.049 val_loss 0.166 val_acc 0.9619\nEpoch: 54, Batch(20/245), Loss: 0.0455\nEpoch: 54, Batch(40/245), Loss: 0.0355\nEpoch: 54, Batch(60/245), Loss: 0.0407\nEpoch: 54, Batch(80/245), Loss: 0.0430\nEpoch: 54, Batch(100/245), Loss: 0.0287\nEpoch: 54, Batch(120/245), Loss: 0.0534\nEpoch: 54, Batch(140/245), Loss: 0.0571\nEpoch: 54, Batch(160/245), Loss: 0.0528\nEpoch: 54, Batch(180/245), Loss: 0.0485\nEpoch: 54, Batch(200/245), Loss: 0.0521\nEpoch: 54, Batch(220/245), Loss: 0.0412\nEpoch: 54, Batch(240/245), Loss: 0.0417\nepoch 54 duration 1.707 train_loss 0.045 val_loss 0.163 val_acc 0.9619\nEpoch: 55, Batch(20/245), Loss: 0.0304\nEpoch: 55, Batch(40/245), Loss: 0.0511\nEpoch: 55, Batch(60/245), Loss: 0.0357\nEpoch: 55, Batch(80/245), Loss: 0.0318\nEpoch: 55, Batch(100/245), Loss: 0.0680\nEpoch: 55, Batch(120/245), Loss: 0.0663\nEpoch: 55, Batch(140/245), Loss: 0.0531\nEpoch: 55, Batch(160/245), Loss: 0.0411\nEpoch: 55, Batch(180/245), Loss: 0.0771\nEpoch: 55, Batch(200/245), Loss: 0.0455\nEpoch: 55, Batch(220/245), Loss: 0.0487\nEpoch: 55, Batch(240/245), Loss: 0.0521\nepoch 55 duration 1.710 train_loss 0.050 val_loss 0.167 val_acc 0.9607\nEpoch: 56, Batch(20/245), Loss: 0.0833\nEpoch: 56, Batch(40/245), Loss: 0.0427\nEpoch: 56, Batch(60/245), Loss: 0.0451\nEpoch: 56, Batch(80/245), Loss: 0.0596\nEpoch: 56, Batch(100/245), Loss: 0.0973\nEpoch: 56, Batch(120/245), Loss: 0.0437\nEpoch: 56, Batch(140/245), Loss: 0.0564\nEpoch: 56, Batch(160/245), Loss: 0.0328\nEpoch: 56, Batch(180/245), Loss: 0.0413\nEpoch: 56, Batch(200/245), Loss: 0.0630\nEpoch: 56, Batch(220/245), Loss: 0.0390\nEpoch: 56, Batch(240/245), Loss: 0.0400\nepoch 56 duration 1.709 train_loss 0.053 val_loss 0.174 val_acc 0.9548\nEpoch: 57, Batch(20/245), Loss: 0.0465\nEpoch: 57, Batch(40/245), Loss: 0.0355\nEpoch: 57, Batch(60/245), Loss: 0.0677\nEpoch: 57, Batch(80/245), Loss: 0.0639\nEpoch: 57, Batch(100/245), Loss: 0.0354\nEpoch: 57, Batch(120/245), Loss: 0.0303\nEpoch: 57, Batch(140/245), Loss: 0.0269\nEpoch: 57, Batch(160/245), Loss: 0.0373\nEpoch: 57, Batch(180/245), Loss: 0.0645\nEpoch: 57, Batch(200/245), Loss: 0.0531\nEpoch: 57, Batch(220/245), Loss: 0.0448\nEpoch: 57, Batch(240/245), Loss: 0.0400\nepoch 57 duration 1.704 train_loss 0.047 val_loss 0.170 val_acc 0.9607\nEpoch: 58, Batch(20/245), Loss: 0.0343\nEpoch: 58, Batch(40/245), Loss: 0.0428\nEpoch: 58, Batch(60/245), Loss: 0.0375\nEpoch: 58, Batch(80/245), Loss: 0.0342\nEpoch: 58, Batch(100/245), Loss: 0.0525\nEpoch: 58, Batch(120/245), Loss: 0.0364\nEpoch: 58, Batch(140/245), Loss: 0.0503\nEpoch: 58, Batch(160/245), Loss: 0.0505\nEpoch: 58, Batch(180/245), Loss: 0.0438\nEpoch: 58, Batch(200/245), Loss: 0.0605\nEpoch: 58, Batch(220/245), Loss: 0.0461\nEpoch: 58, Batch(240/245), Loss: 0.0286\nepoch 58 duration 1.709 train_loss 0.043 val_loss 0.159 val_acc 0.9595\nEpoch: 59, Batch(20/245), Loss: 0.0487\nEpoch: 59, Batch(40/245), Loss: 0.0603\nEpoch: 59, Batch(60/245), Loss: 0.0267\nEpoch: 59, Batch(80/245), Loss: 0.0334\nEpoch: 59, Batch(100/245), Loss: 0.0488\nEpoch: 59, Batch(120/245), Loss: 0.0507\nEpoch: 59, Batch(140/245), Loss: 0.0627\nEpoch: 59, Batch(160/245), Loss: 0.0770\nEpoch: 59, Batch(180/245), Loss: 0.0288\nEpoch: 59, Batch(200/245), Loss: 0.0598\nEpoch: 59, Batch(220/245), Loss: 0.0455\nEpoch: 59, Batch(240/245), Loss: 0.0274\nepoch 59 duration 1.711 train_loss 0.047 val_loss 0.147 val_acc 0.9643\nEpoch: 60, Batch(20/245), Loss: 0.0660\nEpoch: 60, Batch(40/245), Loss: 0.0269\nEpoch: 60, Batch(60/245), Loss: 0.0475\nEpoch: 60, Batch(80/245), Loss: 0.0682\nEpoch: 60, Batch(100/245), Loss: 0.0628\nEpoch: 60, Batch(120/245), Loss: 0.0553\nEpoch: 60, Batch(140/245), Loss: 0.0426\nEpoch: 60, Batch(160/245), Loss: 0.0604\nEpoch: 60, Batch(180/245), Loss: 0.0319\nEpoch: 60, Batch(200/245), Loss: 0.0456\nEpoch: 60, Batch(220/245), Loss: 0.0446\nEpoch: 60, Batch(240/245), Loss: 0.0544\nepoch 60 duration 1.706 train_loss 0.050 val_loss 0.156 val_acc 0.9595\nEpoch: 61, Batch(20/245), Loss: 0.0350\nEpoch: 61, Batch(40/245), Loss: 0.0481\nEpoch: 61, Batch(60/245), Loss: 0.0767\nEpoch: 61, Batch(80/245), Loss: 0.0441\nEpoch: 61, Batch(100/245), Loss: 0.0434\nEpoch: 61, Batch(120/245), Loss: 0.0436\nEpoch: 61, Batch(140/245), Loss: 0.0224\nEpoch: 61, Batch(160/245), Loss: 0.0789\nEpoch: 61, Batch(180/245), Loss: 0.0334\nEpoch: 61, Batch(200/245), Loss: 0.0404\nEpoch: 61, Batch(220/245), Loss: 0.0494\nEpoch: 61, Batch(240/245), Loss: 0.0379\nepoch 61 duration 1.712 train_loss 0.046 val_loss 0.143 val_acc 0.9643\nEpoch: 62, Batch(20/245), Loss: 0.0516\nEpoch: 62, Batch(40/245), Loss: 0.0319\nEpoch: 62, Batch(60/245), Loss: 0.0350\nEpoch: 62, Batch(80/245), Loss: 0.0325\nEpoch: 62, Batch(100/245), Loss: 0.0434\nEpoch: 62, Batch(120/245), Loss: 0.0386\nEpoch: 62, Batch(140/245), Loss: 0.0580\nEpoch: 62, Batch(160/245), Loss: 0.0404\nEpoch: 62, Batch(180/245), Loss: 0.0321\nEpoch: 62, Batch(200/245), Loss: 0.0441\nEpoch: 62, Batch(220/245), Loss: 0.0329\nEpoch: 62, Batch(240/245), Loss: 0.0276\nepoch 62 duration 1.710 train_loss 0.039 val_loss 0.146 val_acc 0.9595\nEpoch: 63, Batch(20/245), Loss: 0.0355\nEpoch: 63, Batch(40/245), Loss: 0.0264\nEpoch: 63, Batch(60/245), Loss: 0.0312\nEpoch: 63, Batch(80/245), Loss: 0.1008\nEpoch: 63, Batch(100/245), Loss: 0.0336\nEpoch: 63, Batch(120/245), Loss: 0.0517\nEpoch: 63, Batch(140/245), Loss: 0.0306\nEpoch: 63, Batch(160/245), Loss: 0.0407\nEpoch: 63, Batch(180/245), Loss: 0.0268\nEpoch: 63, Batch(200/245), Loss: 0.0305\nEpoch: 63, Batch(220/245), Loss: 0.0607\nEpoch: 63, Batch(240/245), Loss: 0.0481\nepoch 63 duration 1.708 train_loss 0.043 val_loss 0.155 val_acc 0.9571\nEpoch: 64, Batch(20/245), Loss: 0.0440\nEpoch: 64, Batch(40/245), Loss: 0.0308\nEpoch: 64, Batch(60/245), Loss: 0.0365\nEpoch: 64, Batch(80/245), Loss: 0.0406\nEpoch: 64, Batch(100/245), Loss: 0.0372\nEpoch: 64, Batch(120/245), Loss: 0.0409\nEpoch: 64, Batch(140/245), Loss: 0.0433\nEpoch: 64, Batch(160/245), Loss: 0.0338\nEpoch: 64, Batch(180/245), Loss: 0.0390\nEpoch: 64, Batch(200/245), Loss: 0.0246\nEpoch: 64, Batch(220/245), Loss: 0.0436\nEpoch: 64, Batch(240/245), Loss: 0.0251\nepoch 64 duration 1.712 train_loss 0.037 val_loss 0.163 val_acc 0.9619\nEpoch: 65, Batch(20/245), Loss: 0.0290\nEpoch: 65, Batch(40/245), Loss: 0.0396\nEpoch: 65, Batch(60/245), Loss: 0.0478\nEpoch: 65, Batch(80/245), Loss: 0.0316\nEpoch: 65, Batch(100/245), Loss: 0.0459\nEpoch: 65, Batch(120/245), Loss: 0.0315\nEpoch: 65, Batch(140/245), Loss: 0.0369\nEpoch: 65, Batch(160/245), Loss: 0.0331\nEpoch: 65, Batch(180/245), Loss: 0.0326\nEpoch: 65, Batch(200/245), Loss: 0.0292\nEpoch: 65, Batch(220/245), Loss: 0.0413\nEpoch: 65, Batch(240/245), Loss: 0.0275\nepoch 65 duration 1.712 train_loss 0.037 val_loss 0.162 val_acc 0.9560\nEpoch: 66, Batch(20/245), Loss: 0.0435\nEpoch: 66, Batch(40/245), Loss: 0.0543\nEpoch: 66, Batch(60/245), Loss: 0.0285\nEpoch: 66, Batch(80/245), Loss: 0.0342\nEpoch: 66, Batch(100/245), Loss: 0.0687\nEpoch: 66, Batch(120/245), Loss: 0.0602\nEpoch: 66, Batch(140/245), Loss: 0.0657\nEpoch: 66, Batch(160/245), Loss: 0.0354\nEpoch: 66, Batch(180/245), Loss: 0.0360\nEpoch: 66, Batch(200/245), Loss: 0.0467\nEpoch: 66, Batch(220/245), Loss: 0.0471\nEpoch: 66, Batch(240/245), Loss: 0.0337\nepoch 66 duration 1.710 train_loss 0.046 val_loss 0.190 val_acc 0.9536\nEpoch: 67, Batch(20/245), Loss: 0.0251\nEpoch: 67, Batch(40/245), Loss: 0.0448\nEpoch: 67, Batch(60/245), Loss: 0.0317\nEpoch: 67, Batch(80/245), Loss: 0.0335\nEpoch: 67, Batch(100/245), Loss: 0.0328\nEpoch: 67, Batch(120/245), Loss: 0.0476\nEpoch: 67, Batch(140/245), Loss: 0.0255\nEpoch: 67, Batch(160/245), Loss: 0.0283\nEpoch: 67, Batch(180/245), Loss: 0.0392\nEpoch: 67, Batch(200/245), Loss: 0.0423\nEpoch: 67, Batch(220/245), Loss: 0.0453\nEpoch: 67, Batch(240/245), Loss: 0.0296\nepoch 67 duration 1.708 train_loss 0.035 val_loss 0.169 val_acc 0.9619\nEpoch: 68, Batch(20/245), Loss: 0.0213\nEpoch: 68, Batch(40/245), Loss: 0.0253\nEpoch: 68, Batch(60/245), Loss: 0.0472\nEpoch: 68, Batch(80/245), Loss: 0.0341\nEpoch: 68, Batch(100/245), Loss: 0.0312\nEpoch: 68, Batch(120/245), Loss: 0.0423\nEpoch: 68, Batch(140/245), Loss: 0.0369\nEpoch: 68, Batch(160/245), Loss: 0.0433\nEpoch: 68, Batch(180/245), Loss: 0.0375\nEpoch: 68, Batch(200/245), Loss: 0.0426\nEpoch: 68, Batch(220/245), Loss: 0.0466\nEpoch: 68, Batch(240/245), Loss: 0.0215\nepoch 68 duration 1.716 train_loss 0.038 val_loss 0.164 val_acc 0.9607\nEpoch: 69, Batch(20/245), Loss: 0.0606\nEpoch: 69, Batch(40/245), Loss: 0.0502\nEpoch: 69, Batch(60/245), Loss: 0.0381\nEpoch: 69, Batch(80/245), Loss: 0.0344\nEpoch: 69, Batch(100/245), Loss: 0.0319\nEpoch: 69, Batch(120/245), Loss: 0.0453\nEpoch: 69, Batch(140/245), Loss: 0.0333\nEpoch: 69, Batch(160/245), Loss: 0.0247\nEpoch: 69, Batch(180/245), Loss: 0.0596\nEpoch: 69, Batch(200/245), Loss: 0.0265\nEpoch: 69, Batch(220/245), Loss: 0.0439\nEpoch: 69, Batch(240/245), Loss: 0.0481\nepoch 69 duration 1.704 train_loss 0.042 val_loss 0.160 val_acc 0.9619\nEpoch: 70, Batch(20/245), Loss: 0.0425\nEpoch: 70, Batch(40/245), Loss: 0.0586\nEpoch: 70, Batch(60/245), Loss: 0.0375\nEpoch: 70, Batch(80/245), Loss: 0.0288\nEpoch: 70, Batch(100/245), Loss: 0.0402\nEpoch: 70, Batch(120/245), Loss: 0.0271\nEpoch: 70, Batch(140/245), Loss: 0.0526\nEpoch: 70, Batch(160/245), Loss: 0.0507\nEpoch: 70, Batch(180/245), Loss: 0.0292\nEpoch: 70, Batch(200/245), Loss: 0.0229\nEpoch: 70, Batch(220/245), Loss: 0.0295\nEpoch: 70, Batch(240/245), Loss: 0.0615\nepoch 70 duration 1.714 train_loss 0.040 val_loss 0.163 val_acc 0.9583\nEpoch: 71, Batch(20/245), Loss: 0.0344\nEpoch: 71, Batch(40/245), Loss: 0.0373\nEpoch: 71, Batch(60/245), Loss: 0.0280\nEpoch: 71, Batch(80/245), Loss: 0.0492\nEpoch: 71, Batch(100/245), Loss: 0.0396\nEpoch: 71, Batch(120/245), Loss: 0.0549\nEpoch: 71, Batch(140/245), Loss: 0.0390\nEpoch: 71, Batch(160/245), Loss: 0.0253\nEpoch: 71, Batch(180/245), Loss: 0.0369\nEpoch: 71, Batch(200/245), Loss: 0.0327\nEpoch: 71, Batch(220/245), Loss: 0.0310\nEpoch: 71, Batch(240/245), Loss: 0.0552\nepoch 71 duration 1.706 train_loss 0.039 val_loss 0.174 val_acc 0.9524\nEpoch: 72, Batch(20/245), Loss: 0.0307\nEpoch: 72, Batch(40/245), Loss: 0.0371\nEpoch: 72, Batch(60/245), Loss: 0.0630\nEpoch: 72, Batch(80/245), Loss: 0.0327\nEpoch: 72, Batch(100/245), Loss: 0.0409\nEpoch: 72, Batch(120/245), Loss: 0.0890\nEpoch: 72, Batch(140/245), Loss: 0.0328\nEpoch: 72, Batch(160/245), Loss: 0.0413\nEpoch: 72, Batch(180/245), Loss: 0.0249\nEpoch: 72, Batch(200/245), Loss: 0.0369\nEpoch: 72, Batch(220/245), Loss: 0.0373\nEpoch: 72, Batch(240/245), Loss: 0.0440\nepoch 72 duration 1.709 train_loss 0.042 val_loss 0.170 val_acc 0.9560\nEpoch: 73, Batch(20/245), Loss: 0.0280\nEpoch: 73, Batch(40/245), Loss: 0.0482\nEpoch: 73, Batch(60/245), Loss: 0.0620\nEpoch: 73, Batch(80/245), Loss: 0.0470\nEpoch: 73, Batch(100/245), Loss: 0.0302\nEpoch: 73, Batch(120/245), Loss: 0.0400\nEpoch: 73, Batch(140/245), Loss: 0.0208\nEpoch: 73, Batch(160/245), Loss: 0.0507\nEpoch: 73, Batch(180/245), Loss: 0.0288\nEpoch: 73, Batch(200/245), Loss: 0.0490\nEpoch: 73, Batch(220/245), Loss: 0.0534\nEpoch: 73, Batch(240/245), Loss: 0.0329\nepoch 73 duration 1.708 train_loss 0.041 val_loss 0.149 val_acc 0.9643\nEpoch: 74, Batch(20/245), Loss: 0.0470\nEpoch: 74, Batch(40/245), Loss: 0.0502\nEpoch: 74, Batch(60/245), Loss: 0.0357\nEpoch: 74, Batch(80/245), Loss: 0.0343\nEpoch: 74, Batch(100/245), Loss: 0.0418\nEpoch: 74, Batch(120/245), Loss: 0.0192\nEpoch: 74, Batch(140/245), Loss: 0.0357\nEpoch: 74, Batch(160/245), Loss: 0.0471\nEpoch: 74, Batch(180/245), Loss: 0.0348\nEpoch: 74, Batch(200/245), Loss: 0.0310\nEpoch: 74, Batch(220/245), Loss: 0.0447\nEpoch: 74, Batch(240/245), Loss: 0.0442\nepoch 74 duration 1.712 train_loss 0.039 val_loss 0.161 val_acc 0.9619\nEpoch: 75, Batch(20/245), Loss: 0.0340\nEpoch: 75, Batch(40/245), Loss: 0.0219\nEpoch: 75, Batch(60/245), Loss: 0.0317\nEpoch: 75, Batch(80/245), Loss: 0.0295\nEpoch: 75, Batch(100/245), Loss: 0.0654\nEpoch: 75, Batch(120/245), Loss: 0.0429\nEpoch: 75, Batch(140/245), Loss: 0.0300\nEpoch: 75, Batch(160/245), Loss: 0.0487\nEpoch: 75, Batch(180/245), Loss: 0.0510\nEpoch: 75, Batch(200/245), Loss: 0.0324\nEpoch: 75, Batch(220/245), Loss: 0.0677\nEpoch: 75, Batch(240/245), Loss: 0.0426\nepoch 75 duration 1.706 train_loss 0.041 val_loss 0.168 val_acc 0.9595\nEpoch: 76, Batch(20/245), Loss: 0.0320\nEpoch: 76, Batch(40/245), Loss: 0.0368\nEpoch: 76, Batch(60/245), Loss: 0.0446\nEpoch: 76, Batch(80/245), Loss: 0.0299\nEpoch: 76, Batch(100/245), Loss: 0.0259\nEpoch: 76, Batch(120/245), Loss: 0.0705\nEpoch: 76, Batch(140/245), Loss: 0.0347\nEpoch: 76, Batch(160/245), Loss: 0.0298\nEpoch: 76, Batch(180/245), Loss: 0.0423\nEpoch: 76, Batch(200/245), Loss: 0.0323\nEpoch: 76, Batch(220/245), Loss: 0.0246\nEpoch: 76, Batch(240/245), Loss: 0.0313\nepoch 76 duration 1.712 train_loss 0.036 val_loss 0.166 val_acc 0.9631\nEpoch: 77, Batch(20/245), Loss: 0.0562\nEpoch: 77, Batch(40/245), Loss: 0.0298\nEpoch: 77, Batch(60/245), Loss: 0.0378\nEpoch: 77, Batch(80/245), Loss: 0.0329\nEpoch: 77, Batch(100/245), Loss: 0.0308\nEpoch: 77, Batch(120/245), Loss: 0.0296\nEpoch: 77, Batch(140/245), Loss: 0.0346\nEpoch: 77, Batch(160/245), Loss: 0.0509\nEpoch: 77, Batch(180/245), Loss: 0.0354\nEpoch: 77, Batch(200/245), Loss: 0.0360\nEpoch: 77, Batch(220/245), Loss: 0.0297\nEpoch: 77, Batch(240/245), Loss: 0.0305\nepoch 77 duration 1.706 train_loss 0.037 val_loss 0.183 val_acc 0.9607\nEpoch: 78, Batch(20/245), Loss: 0.0364\nEpoch: 78, Batch(40/245), Loss: 0.0336\nEpoch: 78, Batch(60/245), Loss: 0.0472\nEpoch: 78, Batch(80/245), Loss: 0.0607\nEpoch: 78, Batch(100/245), Loss: 0.0298\nEpoch: 78, Batch(120/245), Loss: 0.0418\nEpoch: 78, Batch(140/245), Loss: 0.0298\nEpoch: 78, Batch(160/245), Loss: 0.0306\nEpoch: 78, Batch(180/245), Loss: 0.0298\nEpoch: 78, Batch(200/245), Loss: 0.0267\nEpoch: 78, Batch(220/245), Loss: 0.0392\nEpoch: 78, Batch(240/245), Loss: 0.0541\nepoch 78 duration 1.713 train_loss 0.038 val_loss 0.182 val_acc 0.9571\nEpoch: 79, Batch(20/245), Loss: 0.0496\nEpoch: 79, Batch(40/245), Loss: 0.0213\nEpoch: 79, Batch(60/245), Loss: 0.0455\nEpoch: 79, Batch(80/245), Loss: 0.0291\nEpoch: 79, Batch(100/245), Loss: 0.0274\nEpoch: 79, Batch(120/245), Loss: 0.0545\nEpoch: 79, Batch(140/245), Loss: 0.0331\nEpoch: 79, Batch(160/245), Loss: 0.0444\nEpoch: 79, Batch(180/245), Loss: 0.0263\nEpoch: 79, Batch(200/245), Loss: 0.0350\nEpoch: 79, Batch(220/245), Loss: 0.0609\nEpoch: 79, Batch(240/245), Loss: 0.0293\nepoch 79 duration 1.712 train_loss 0.038 val_loss 0.194 val_acc 0.9524\nEpoch: 80, Batch(20/245), Loss: 0.0226\nEpoch: 80, Batch(40/245), Loss: 0.0446\nEpoch: 80, Batch(60/245), Loss: 0.0330\nEpoch: 80, Batch(80/245), Loss: 0.0292\nEpoch: 80, Batch(100/245), Loss: 0.0446\nEpoch: 80, Batch(120/245), Loss: 0.0323\nEpoch: 80, Batch(140/245), Loss: 0.0473\nEpoch: 80, Batch(160/245), Loss: 0.0223\nEpoch: 80, Batch(180/245), Loss: 0.0275\nEpoch: 80, Batch(200/245), Loss: 0.0489\nEpoch: 80, Batch(220/245), Loss: 0.0294\nEpoch: 80, Batch(240/245), Loss: 0.0284\nepoch 80 duration 1.710 train_loss 0.034 val_loss 0.186 val_acc 0.9571\nEpoch: 81, Batch(20/245), Loss: 0.0379\nEpoch: 81, Batch(40/245), Loss: 0.0390\nEpoch: 81, Batch(60/245), Loss: 0.0308\nEpoch: 81, Batch(80/245), Loss: 0.0291\nEpoch: 81, Batch(100/245), Loss: 0.0359\nEpoch: 81, Batch(120/245), Loss: 0.0357\nEpoch: 81, Batch(140/245), Loss: 0.0317\nEpoch: 81, Batch(160/245), Loss: 0.0252\nEpoch: 81, Batch(180/245), Loss: 0.0241\nEpoch: 81, Batch(200/245), Loss: 0.0279\nEpoch: 81, Batch(220/245), Loss: 0.0394\nEpoch: 81, Batch(240/245), Loss: 0.0255\nepoch 81 duration 1.706 train_loss 0.032 val_loss 0.186 val_acc 0.9571\nEpoch: 82, Batch(20/245), Loss: 0.0409\nEpoch: 82, Batch(40/245), Loss: 0.0338\nEpoch: 82, Batch(60/245), Loss: 0.0240\nEpoch: 82, Batch(80/245), Loss: 0.0536\nEpoch: 82, Batch(100/245), Loss: 0.0376\nEpoch: 82, Batch(120/245), Loss: 0.0426\nEpoch: 82, Batch(140/245), Loss: 0.0250\nEpoch: 82, Batch(160/245), Loss: 0.0356\nEpoch: 82, Batch(180/245), Loss: 0.0255\nEpoch: 82, Batch(200/245), Loss: 0.0331\nEpoch: 82, Batch(220/245), Loss: 0.0347\nEpoch: 82, Batch(240/245), Loss: 0.0215\nepoch 82 duration 1.703 train_loss 0.034 val_loss 0.181 val_acc 0.9571\nEpoch: 83, Batch(20/245), Loss: 0.0300\nEpoch: 83, Batch(40/245), Loss: 0.0304\nEpoch: 83, Batch(60/245), Loss: 0.0316\nEpoch: 83, Batch(80/245), Loss: 0.0305\nEpoch: 83, Batch(100/245), Loss: 0.0459\nEpoch: 83, Batch(120/245), Loss: 0.0292\nEpoch: 83, Batch(140/245), Loss: 0.0411\nEpoch: 83, Batch(160/245), Loss: 0.0278\nEpoch: 83, Batch(180/245), Loss: 0.0158\nEpoch: 83, Batch(200/245), Loss: 0.0249\nEpoch: 83, Batch(220/245), Loss: 0.0274\nEpoch: 83, Batch(240/245), Loss: 0.0330\nepoch 83 duration 1.704 train_loss 0.031 val_loss 0.182 val_acc 0.9571\nEpoch: 84, Batch(20/245), Loss: 0.0384\nEpoch: 84, Batch(40/245), Loss: 0.0297\nEpoch: 84, Batch(60/245), Loss: 0.0367\nEpoch: 84, Batch(80/245), Loss: 0.0293\nEpoch: 84, Batch(100/245), Loss: 0.0393\nEpoch: 84, Batch(120/245), Loss: 0.0295\nEpoch: 84, Batch(140/245), Loss: 0.0293\nEpoch: 84, Batch(160/245), Loss: 0.0470\nEpoch: 84, Batch(180/245), Loss: 0.0429\nEpoch: 84, Batch(200/245), Loss: 0.0229\nEpoch: 84, Batch(220/245), Loss: 0.0249\nEpoch: 84, Batch(240/245), Loss: 0.0319\nepoch 84 duration 1.706 train_loss 0.034 val_loss 0.174 val_acc 0.9583\nEpoch: 85, Batch(20/245), Loss: 0.0371\nEpoch: 85, Batch(40/245), Loss: 0.0240\nEpoch: 85, Batch(60/245), Loss: 0.0348\nEpoch: 85, Batch(80/245), Loss: 0.0285\nEpoch: 85, Batch(100/245), Loss: 0.0230\nEpoch: 85, Batch(120/245), Loss: 0.0352\nEpoch: 85, Batch(140/245), Loss: 0.0336\nEpoch: 85, Batch(160/245), Loss: 0.0329\nEpoch: 85, Batch(180/245), Loss: 0.0300\nEpoch: 85, Batch(200/245), Loss: 0.0324\nEpoch: 85, Batch(220/245), Loss: 0.0432\nEpoch: 85, Batch(240/245), Loss: 0.0544\nepoch 85 duration 1.706 train_loss 0.034 val_loss 0.178 val_acc 0.9560\nEpoch: 86, Batch(20/245), Loss: 0.0342\nEpoch: 86, Batch(40/245), Loss: 0.0304\nEpoch: 86, Batch(60/245), Loss: 0.0533\nEpoch: 86, Batch(80/245), Loss: 0.0420\nEpoch: 86, Batch(100/245), Loss: 0.0583\nEpoch: 86, Batch(120/245), Loss: 0.0263\nEpoch: 86, Batch(140/245), Loss: 0.0380\nEpoch: 86, Batch(160/245), Loss: 0.0354\nEpoch: 86, Batch(180/245), Loss: 0.0413\nEpoch: 86, Batch(200/245), Loss: 0.0333\nEpoch: 86, Batch(220/245), Loss: 0.0444\nEpoch: 86, Batch(240/245), Loss: 0.0272\nepoch 86 duration 1.711 train_loss 0.038 val_loss 0.175 val_acc 0.9571\nEpoch: 87, Batch(20/245), Loss: 0.0292\nEpoch: 87, Batch(40/245), Loss: 0.0343\nEpoch: 87, Batch(60/245), Loss: 0.0291\nEpoch: 87, Batch(80/245), Loss: 0.0435\nEpoch: 87, Batch(100/245), Loss: 0.0275\nEpoch: 87, Batch(120/245), Loss: 0.0268\nEpoch: 87, Batch(140/245), Loss: 0.0208\nEpoch: 87, Batch(160/245), Loss: 0.0227\nEpoch: 87, Batch(180/245), Loss: 0.0329\nEpoch: 87, Batch(200/245), Loss: 0.0442\nEpoch: 87, Batch(220/245), Loss: 0.0166\nEpoch: 87, Batch(240/245), Loss: 0.0270\nepoch 87 duration 1.710 train_loss 0.030 val_loss 0.183 val_acc 0.9571\nEpoch: 88, Batch(20/245), Loss: 0.0245\nEpoch: 88, Batch(40/245), Loss: 0.0294\nEpoch: 88, Batch(60/245), Loss: 0.0394\nEpoch: 88, Batch(80/245), Loss: 0.0314\nEpoch: 88, Batch(100/245), Loss: 0.0233\nEpoch: 88, Batch(120/245), Loss: 0.0235\nEpoch: 88, Batch(140/245), Loss: 0.0544\nEpoch: 88, Batch(160/245), Loss: 0.0236\nEpoch: 88, Batch(180/245), Loss: 0.0246\nEpoch: 88, Batch(200/245), Loss: 0.0242\nEpoch: 88, Batch(220/245), Loss: 0.0236\nEpoch: 88, Batch(240/245), Loss: 0.0335\nepoch 88 duration 1.713 train_loss 0.030 val_loss 0.177 val_acc 0.9571\nEpoch: 89, Batch(20/245), Loss: 0.0250\nEpoch: 89, Batch(40/245), Loss: 0.0368\nEpoch: 89, Batch(60/245), Loss: 0.0209\nEpoch: 89, Batch(80/245), Loss: 0.0256\nEpoch: 89, Batch(100/245), Loss: 0.0449\nEpoch: 89, Batch(120/245), Loss: 0.0491\nEpoch: 89, Batch(140/245), Loss: 0.0378\nEpoch: 89, Batch(160/245), Loss: 0.0364\nEpoch: 89, Batch(180/245), Loss: 0.0328\nEpoch: 89, Batch(200/245), Loss: 0.0318\nEpoch: 89, Batch(220/245), Loss: 0.0275\nEpoch: 89, Batch(240/245), Loss: 0.0421\nepoch 89 duration 1.714 train_loss 0.034 val_loss 0.176 val_acc 0.9571\nEpoch: 90, Batch(20/245), Loss: 0.0288\nEpoch: 90, Batch(40/245), Loss: 0.0293\nEpoch: 90, Batch(60/245), Loss: 0.0263\nEpoch: 90, Batch(80/245), Loss: 0.0279\nEpoch: 90, Batch(100/245), Loss: 0.0300\nEpoch: 90, Batch(120/245), Loss: 0.0305\nEpoch: 90, Batch(140/245), Loss: 0.0174\nEpoch: 90, Batch(160/245), Loss: 0.0253\nEpoch: 90, Batch(180/245), Loss: 0.0378\nEpoch: 90, Batch(200/245), Loss: 0.0283\nEpoch: 90, Batch(220/245), Loss: 0.0561\nEpoch: 90, Batch(240/245), Loss: 0.0269\nepoch 90 duration 1.708 train_loss 0.031 val_loss 0.175 val_acc 0.9583\nEpoch: 91, Batch(20/245), Loss: 0.0371\nEpoch: 91, Batch(40/245), Loss: 0.0251\nEpoch: 91, Batch(60/245), Loss: 0.0263\nEpoch: 91, Batch(80/245), Loss: 0.0385\nEpoch: 91, Batch(100/245), Loss: 0.0278\nEpoch: 91, Batch(120/245), Loss: 0.0277\nEpoch: 91, Batch(140/245), Loss: 0.0342\nEpoch: 91, Batch(160/245), Loss: 0.0332\nEpoch: 91, Batch(180/245), Loss: 0.0445\nEpoch: 91, Batch(200/245), Loss: 0.0530\nEpoch: 91, Batch(220/245), Loss: 0.0273\nEpoch: 91, Batch(240/245), Loss: 0.0373\nepoch 91 duration 1.707 train_loss 0.034 val_loss 0.171 val_acc 0.9607\nEpoch: 92, Batch(20/245), Loss: 0.0305\nEpoch: 92, Batch(40/245), Loss: 0.0243\nEpoch: 92, Batch(60/245), Loss: 0.0295\nEpoch: 92, Batch(80/245), Loss: 0.0364\nEpoch: 92, Batch(100/245), Loss: 0.0437\nEpoch: 92, Batch(120/245), Loss: 0.0283\nEpoch: 92, Batch(140/245), Loss: 0.0408\nEpoch: 92, Batch(160/245), Loss: 0.0231\nEpoch: 92, Batch(180/245), Loss: 0.0504\nEpoch: 92, Batch(200/245), Loss: 0.0235\nEpoch: 92, Batch(220/245), Loss: 0.0452\nEpoch: 92, Batch(240/245), Loss: 0.0350\nepoch 92 duration 1.708 train_loss 0.034 val_loss 0.181 val_acc 0.9548\nEpoch: 93, Batch(20/245), Loss: 0.0471\nEpoch: 93, Batch(40/245), Loss: 0.0489\nEpoch: 93, Batch(60/245), Loss: 0.0255\nEpoch: 93, Batch(80/245), Loss: 0.0369\nEpoch: 93, Batch(100/245), Loss: 0.0228\nEpoch: 93, Batch(120/245), Loss: 0.0271\nEpoch: 93, Batch(140/245), Loss: 0.0274\nEpoch: 93, Batch(160/245), Loss: 0.0225\nEpoch: 93, Batch(180/245), Loss: 0.0220\nEpoch: 93, Batch(200/245), Loss: 0.0235\nEpoch: 93, Batch(220/245), Loss: 0.0449\nEpoch: 93, Batch(240/245), Loss: 0.0317\nepoch 93 duration 1.708 train_loss 0.032 val_loss 0.177 val_acc 0.9560\nEpoch: 94, Batch(20/245), Loss: 0.0194\nEpoch: 94, Batch(40/245), Loss: 0.0587\nEpoch: 94, Batch(60/245), Loss: 0.0256\nEpoch: 94, Batch(80/245), Loss: 0.0303\nEpoch: 94, Batch(100/245), Loss: 0.0249\nEpoch: 94, Batch(120/245), Loss: 0.0372\nEpoch: 94, Batch(140/245), Loss: 0.0367\nEpoch: 94, Batch(160/245), Loss: 0.0222\nEpoch: 94, Batch(180/245), Loss: 0.0260\nEpoch: 94, Batch(200/245), Loss: 0.0216\nEpoch: 94, Batch(220/245), Loss: 0.0301\nEpoch: 94, Batch(240/245), Loss: 0.0255\nepoch 94 duration 1.708 train_loss 0.031 val_loss 0.180 val_acc 0.9583\nEpoch: 95, Batch(20/245), Loss: 0.0305\nEpoch: 95, Batch(40/245), Loss: 0.0413\nEpoch: 95, Batch(60/245), Loss: 0.0230\nEpoch: 95, Batch(80/245), Loss: 0.0366\nEpoch: 95, Batch(100/245), Loss: 0.0349\nEpoch: 95, Batch(120/245), Loss: 0.0218\nEpoch: 95, Batch(140/245), Loss: 0.0367\nEpoch: 95, Batch(160/245), Loss: 0.0257\nEpoch: 95, Batch(180/245), Loss: 0.0404\nEpoch: 95, Batch(200/245), Loss: 0.0185\nEpoch: 95, Batch(220/245), Loss: 0.0212\nEpoch: 95, Batch(240/245), Loss: 0.0426\nepoch 95 duration 1.707 train_loss 0.031 val_loss 0.185 val_acc 0.9536\nEpoch: 96, Batch(20/245), Loss: 0.0377\nEpoch: 96, Batch(40/245), Loss: 0.0284\nEpoch: 96, Batch(60/245), Loss: 0.0322\nEpoch: 96, Batch(80/245), Loss: 0.0309\nEpoch: 96, Batch(100/245), Loss: 0.0470\nEpoch: 96, Batch(120/245), Loss: 0.0255\nEpoch: 96, Batch(140/245), Loss: 0.0310\nEpoch: 96, Batch(160/245), Loss: 0.0287\nEpoch: 96, Batch(180/245), Loss: 0.0243\nEpoch: 96, Batch(200/245), Loss: 0.0530\nEpoch: 96, Batch(220/245), Loss: 0.0258\nEpoch: 96, Batch(240/245), Loss: 0.0232\nepoch 96 duration 1.710 train_loss 0.032 val_loss 0.175 val_acc 0.9583\nEpoch: 97, Batch(20/245), Loss: 0.0303\nEpoch: 97, Batch(40/245), Loss: 0.0354\nEpoch: 97, Batch(60/245), Loss: 0.0291\nEpoch: 97, Batch(80/245), Loss: 0.0329\nEpoch: 97, Batch(100/245), Loss: 0.0291\nEpoch: 97, Batch(120/245), Loss: 0.0259\nEpoch: 97, Batch(140/245), Loss: 0.0216\nEpoch: 97, Batch(160/245), Loss: 0.0334\nEpoch: 97, Batch(180/245), Loss: 0.0295\nEpoch: 97, Batch(200/245), Loss: 0.0439\nEpoch: 97, Batch(220/245), Loss: 0.0189\nEpoch: 97, Batch(240/245), Loss: 0.0394\nepoch 97 duration 1.707 train_loss 0.031 val_loss 0.180 val_acc 0.9548\nEpoch: 98, Batch(20/245), Loss: 0.0602\nEpoch: 98, Batch(40/245), Loss: 0.0283\nEpoch: 98, Batch(60/245), Loss: 0.0409\nEpoch: 98, Batch(80/245), Loss: 0.0353\nEpoch: 98, Batch(100/245), Loss: 0.0227\nEpoch: 98, Batch(120/245), Loss: 0.0263\nEpoch: 98, Batch(140/245), Loss: 0.0393\nEpoch: 98, Batch(160/245), Loss: 0.0356\nEpoch: 98, Batch(180/245), Loss: 0.0396\nEpoch: 98, Batch(200/245), Loss: 0.0455\nEpoch: 98, Batch(220/245), Loss: 0.0400\nEpoch: 98, Batch(240/245), Loss: 0.0409\nepoch 98 duration 1.710 train_loss 0.038 val_loss 0.174 val_acc 0.9583\nEpoch: 99, Batch(20/245), Loss: 0.0271\nEpoch: 99, Batch(40/245), Loss: 0.0360\nEpoch: 99, Batch(60/245), Loss: 0.0418\nEpoch: 99, Batch(80/245), Loss: 0.0246\nEpoch: 99, Batch(100/245), Loss: 0.0277\nEpoch: 99, Batch(120/245), Loss: 0.0286\nEpoch: 99, Batch(140/245), Loss: 0.0173\nEpoch: 99, Batch(160/245), Loss: 0.0335\nEpoch: 99, Batch(180/245), Loss: 0.0337\nEpoch: 99, Batch(200/245), Loss: 0.0433\nEpoch: 99, Batch(220/245), Loss: 0.0409\nEpoch: 99, Batch(240/245), Loss: 0.0458\nepoch 99 duration 1.712 train_loss 0.033 val_loss 0.177 val_acc 0.9595\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python test.py --config-file \"../configs/config_full-model_shrec14.yaml\" --gpu 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T03:10:50.708923Z","iopub.execute_input":"2024-12-16T03:10:50.709586Z","iopub.status.idle":"2024-12-16T03:11:13.586247Z","shell.execute_reply.started":"2024-12-16T03:10:50.709553Z","shell.execute_reply":"2024-12-16T03:11:13.585397Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/augmented-e2eET-Skeleton-Based-HGR-Using-Data-Level-Fusion/FPPRC/main/test.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n#model parameters:  4628878\naverage inference time:  18.32238265446254\naccuracy:  0.9642857142857143\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}